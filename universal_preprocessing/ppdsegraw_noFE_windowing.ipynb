{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f4b8d7",
   "metadata": {},
   "source": [
    "> This NB applies windowing to the preprocessed segmented raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e073f3",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be028e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pids = ['P102', 'P103', 'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111', 'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P131', 'P132', 'P004', 'P005', 'P006', 'P008', 'P010', 'P011']\n",
    "all_gesture_types = ['pan', 'delete', 'close', 'select-single', 'rotate', 'zoom-in', 'zoom-out', 'open', 'move', 'duplicate']\n",
    "all_gesture_nums = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbe4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Completed in 437.7451078891754s\n"
     ]
    }
   ],
   "source": [
    "#brc_data_save_path = 'D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\filtered_datasets\\\\'\n",
    "laptop_data_save_path = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\Meta_Gesture_Project'\n",
    "full_filesave_path = laptop_data_save_path+'\\\\ppdsegraw_allEMG.json'\n",
    "\n",
    "print(\"Loading\")\n",
    "start_time = time.time()\n",
    "with open(full_filesave_path, 'r') as f:\n",
    "    loaded_dict = json.load(f)\n",
    "end_time = time.time()\n",
    "print(f\"Completed in {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85b376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momona's code...\n",
    "\n",
    "def resample(data, N):\n",
    "    N_init = len(data)\n",
    "    x = np.linspace(0, N_init-1, N)  # x coordinates at which to evaluate interpolated values\n",
    "    xp = np.linspace(0, N_init-1, N_init)  # x coordinates of data points\n",
    "    return np.interp(x, xp, data)\n",
    "\n",
    "def rectify_EMG(data):\n",
    "    \n",
    "    # 1. high-pass filter @ 40 hz, 4th order butterworth\n",
    "    sos_high = signal.butter(N=4, Wn=40, btype='high', fs=2000, output='sos')\n",
    "    hp_filtered = signal.sosfilt(sos_high, data)\n",
    "    \n",
    "    # 2. demean and rectify\n",
    "    emg_mean = hp_filtered - np.mean(hp_filtered)\n",
    "    rectified = abs(emg_mean)\n",
    "    \n",
    "    # 3. low-pass filter @ 40 Hz, 4th order butterworth\n",
    "    sos_low = signal.butter(N=4,Wn=40, btype='low', fs=2000, output='sos')\n",
    "    lp_filtered = signal.sosfilt(sos_low, rectified)\n",
    "    return lp_filtered\n",
    "\n",
    "def compute_SNR(s,fs=2000):\n",
    "    nperseg = len(s)//4.5\n",
    "    f,Pxx = welch(s,fs=2000,window='hamming',\n",
    "        nperseg=nperseg,noverlap=nperseg//2,\n",
    "        scaling='density',detrend=False)\n",
    "    idx400 = list(abs(f-800)).index(min(abs(f-800)))\n",
    "    N = len(f)-idx400\n",
    "    noise_power = sum(Pxx[idx400:])/N*len(f)\n",
    "    total_power = sum(Pxx)\n",
    "    return 10*np.log10(total_power / noise_power)\n",
    "\n",
    "def bandpass_EMG(data):\n",
    "    sos_high = signal.butter(N=4,Wn=[10,500],btype='bandpass',fs=2000,output='sos')\n",
    "    EMG_filt = signal.sosfilt(sos_high,data)\n",
    "    EMG_filt = EMG_filt - np.mean(EMG_filt)\n",
    "    return EMG_filt\n",
    "\n",
    "def movavg_EMG(data,window=100,overlap=0.5,fs=2000):\n",
    "    # 200 ms window, overlap = 0 is no overlap, overlap=1 is complete overlap \n",
    "    N = int(1*window/1000*fs) # number of datapoints in one window\n",
    "    N_overlap = int(N*overlap)\n",
    "    \n",
    "    movavg_data = []\n",
    "    \n",
    "    ix = N\n",
    "    while ix < len(data):\n",
    "        movavg_data.append(np.mean(data[ix-N:ix]))\n",
    "        ix = ix + (N - N_overlap)\n",
    "    \n",
    "    return np.asarray(movavg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9dd03",
   "metadata": {},
   "source": [
    "## For the non-FE\n",
    "1. rectify the data,\n",
    "2. do MAV or RMS (moving average for now?),\n",
    "3. pick a window size (50-300ms) and overlap (50%), \n",
    "4. use np.interp to resample after\n",
    "\n",
    "> Need to do windowing on the non-FE data since if we have to apply MAV/RMS to the data (to make it less noisy) that would reduce it to one sample per channel. Windowing is how we get multiple samples per channel\n",
    "\n",
    "> Note: preprocessed data file has already been BPF'd, MS'd, and standardized (std=1 for each gesture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13151d",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline for raw (non-handcrafted) EMG data:\n",
    "\n",
    "1. Rectify the signal:\n",
    "   - Take the absolute value of the raw EMG to ensure all values are positive\n",
    "   - This is a standard EMG preprocessing step to reflect signal energy\n",
    "\n",
    "2. Apply temporal feature extraction (e.g., MAV or RMS):\n",
    "   - The signal is segmented into overlapping windows (e.g., 200ms windows with 50% overlap = 100ms step)\n",
    "   - Each window is reduced to a **single scalar per channel** using an aggregation function like MAV (mean absolute value) or RMS\n",
    "   - This turns a long sequence of raw EMG into a **low-resolution sequence of extracted features**, one per window\n",
    "\n",
    "3. Resample the feature sequence:\n",
    "   - After windowing, each gesture trial has a different number of windows (due to gesture duration)\n",
    "   - We use resampling (e.g., `scipy.signal.resample`) to interpolate this variable-length sequence to a fixed length (e.g., 64 time steps)\n",
    "   - This ensures all samples have the same shape `[64, num_channels]` and can be fed into CNN/LSTM models as batched input\n",
    "\n",
    "# Why this is necessary:\n",
    "- Without windowing + feature aggregation, rectifying alone would reduce each gesture to a single averaged vector (i.e., 1 sample per channel) â€” losing temporal structure\n",
    "- Windowing preserves **time-local structure** while smoothing out noise via MAV/RMS\n",
    "- Resampling standardizes the **temporal resolution** across gestures with different durations\n",
    "\n",
    "# Note:\n",
    "- Input EMG data is already:\n",
    "   - Bandpass filtered (BPF)\n",
    "   - Mean subtracted (MS)\n",
    "   - Standardized per gesture (std = 1). This was done gesture-wide to account for some muscles being bigger/stronger than others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing_emg(emg_data, window_size=200, step_size=100, feature=\"MAV\"):\n",
    "    \"\"\"\n",
    "    Applies windowing and extracts MAV or RMS features from EMG data.\n",
    "\n",
    "    Parameters:\n",
    "        emg_data (numpy array): Shape (num_samples, 16), where 16 is the number of channels.\n",
    "        window_size (int): Number of samples per window (default 200 for 100 ms at 2000 Hz).\n",
    "        step_size (int): Step size for sliding window (default 100 for 50% overlap).\n",
    "        feature (str): Feature type, either \"MAV\" or \"RMS\".\n",
    "\n",
    "    Returns:\n",
    "        feature_matrix (numpy array): Shape (num_windows, 16), where each row is a feature vector.\n",
    "    \"\"\"\n",
    "    # NOTE: num_samples here refers to how many scalar values are present, this is more along the lines of sequence length actually\n",
    "    num_samples, num_channels = emg_data.shape\n",
    "    num_windows = int((num_samples - window_size) // step_size + 1)  # Total windows\n",
    "    # NOTE: variablity in samples (num_samples-->num_windows) leads to different number of windows, but this gets standardized to 64 in resampling\n",
    "    feature_matrix = np.zeros((num_windows, num_channels))  # Initialize output matrix\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start = i * step_size\n",
    "        end = start + window_size\n",
    "        window = emg_data[start:end, :]  # Extract window for all channels\n",
    "\n",
    "        if feature == \"MAV\":\n",
    "            feature_matrix[i, :] = np.mean(np.abs(window), axis=0)\n",
    "        elif feature == \"RMS\":\n",
    "            feature_matrix[i, :] = np.sqrt(np.mean(window**2, axis=0))\n",
    "        else:\n",
    "            raise ValueError(\"Feature must be 'MAV' or 'RMS'\")\n",
    "\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40578a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rect_windowing_resampling(nested_dict, num_resampled_rows=64, fs=2000, window_size_ms=200, window_step_size_ms=100, aggregation_func=\"MAV\", step_size_points=None):\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    result_data = []\n",
    "\n",
    "    # Track shape stats for final print\n",
    "    debug_shapes = {}\n",
    "\n",
    "    # Iterate over all participants, gesture IDs, and gesture numbers\n",
    "    for pid, gestures in nested_dict.items():\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for gesture_num, modality_data in gesture_data.items():\n",
    "                # Initialize dict to store the result for the current gesture\n",
    "                result_dict = {\n",
    "                    'Participant': pid,\n",
    "                    'Gesture_ID': gesture_name,\n",
    "                    'Gesture_Num': gesture_num\n",
    "                }\n",
    "                \n",
    "                # Apply the feature engineering functions to each EMG channel\n",
    "                for modality_str, single_gesture_data in modality_data.items():\n",
    "                    # single_gesture_data is a list with 16 lists as elements\n",
    "                    gesture_npy = np.array(single_gesture_data).T\n",
    "                    debug_shapes['raw_input_shape'] = gesture_npy.shape\n",
    "                    # Rectify the data\n",
    "                    rect_channel_data = np.abs(gesture_npy)\n",
    "                    debug_shapes['rectified_shape'] = rect_channel_data.shape\n",
    "                    # Divide by 1000 to get out of ms\n",
    "                    window_size_num_points = int(fs * window_size_ms // 1000)\n",
    "                    debug_shapes['window_size_num_points'] = window_size_num_points\n",
    "                    # Doing this so I can easily set the step size to 1 point, without having to do the math/rounding\n",
    "                    if step_size_points is None:\n",
    "                        window_step_size_num_points = int(fs * window_step_size_ms // 1000)\n",
    "                    else:\n",
    "                        window_step_size_num_points = step_size_points\n",
    "                    debug_shapes['window_step_size_num_points'] = window_step_size_num_points\n",
    "                    windows_matrix = windowing_emg(rect_channel_data, window_size=window_size_num_points, step_size=window_step_size_num_points, feature=aggregation_func)\n",
    "                    debug_shapes['windows_matrix'] = windows_matrix.shape\n",
    "\n",
    "                    # Resample the gesture after to get 64 rows\n",
    "                    resampled_windows_matrix = np.array([resample(windows_matrix[:, col], num_resampled_rows) for col in range(windows_matrix.shape[1])])\n",
    "                    debug_shapes['resampled_matrix'] = resampled_windows_matrix.shape\n",
    "\n",
    "                    result_dict.update({\n",
    "                        f'windowed_ts_data': resampled_windows_matrix\n",
    "                    })\n",
    "                # Append the result for the current gesture to the result list\n",
    "                result_data.append(result_dict)\n",
    "\n",
    "    # Print out debug info from the *last* sample\n",
    "    print(\"\\n--- DEBUG SHAPES (from last gesture processed) ---\")\n",
    "    for k, v in debug_shapes.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    # Convert the result list to a DataFrame (everything should have the same lengths now)\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87361727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sliding_window_segmentation(nested_dict, fs=2000, window_size_ms=200, step_size_ms=100, step_size_num_points=None):\n",
    "    \"\"\"\n",
    "    Implements the 'sliding window cropping' approach described in the augmentation paper.\n",
    "    Each gesture is split into multiple fixed-length overlapping windows (segments),\n",
    "    and each segment becomes its own training sample.\n",
    "\n",
    "    This differs from the original function, which compresses each gesture into a single\n",
    "    fixed-size time-series feature matrix using feature extraction + resampling.\n",
    "\n",
    "    This function does not perform MAV/RMS feature extraction or resampling.\n",
    "    Each segment retains its raw (or rectified) EMG values.\n",
    "    \"\"\"\n",
    "\n",
    "    result_data = []\n",
    "    debug_shapes = {}\n",
    "\n",
    "    # Convert ms to number of sample points\n",
    "    window_size_pts = int(fs * window_size_ms / 1000)\n",
    "    debug_shapes['window_size_pts'] = window_size_pts\n",
    "    if step_size_num_points is None:\n",
    "        step_size_pts = int(fs * step_size_ms / 1000)\n",
    "    else:\n",
    "        step_size_pts = step_size_num_points\n",
    "    debug_shapes['step_size_pts'] = step_size_pts\n",
    "\n",
    "    for pid, gestures in nested_dict.items():\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for gesture_num, modality_data in gesture_data.items():\n",
    "                for modality_str, single_gesture_data in modality_data.items():\n",
    "                    # Convert to NumPy and transpose to shape [T, C] where T = time, C = channels\n",
    "                    gesture_npy = np.array(single_gesture_data).T\n",
    "                    debug_shapes['raw_input_shape'] = gesture_npy.shape\n",
    "\n",
    "                    # Rectify the signal (absolute value)\n",
    "                    rect_channel_data = np.abs(gesture_npy)\n",
    "                    debug_shapes['rectified_shape'] = rect_channel_data.shape\n",
    "\n",
    "                    total_samples = rect_channel_data.shape[0]\n",
    "                    num_channels = rect_channel_data.shape[1]\n",
    "\n",
    "                    # Slide a window over the gesture trial\n",
    "                    for start_idx in range(0, total_samples - window_size_pts + 1, step_size_pts):\n",
    "                        end_idx = start_idx + window_size_pts\n",
    "                        segment = rect_channel_data[start_idx:end_idx, :]  # Shape: [window_size_pts, num_channels]\n",
    "\n",
    "                        debug_shapes['segment_shape'] = segment.shape\n",
    "\n",
    "                        result_data.append({\n",
    "                            'Participant': pid,\n",
    "                            'Gesture_ID': gesture_name,\n",
    "                            'Gesture_Num': gesture_num,\n",
    "                            'windowed_ts_data': segment\n",
    "                        })\n",
    "\n",
    "    # Print debug shapes from last segment processed\n",
    "    print(\"\\n--- DEBUG SHAPES (from last segment processed) ---\")\n",
    "    for k, v in debug_shapes.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    return pd.DataFrame(result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e45568",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m windowed_df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rect_windowing_resampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_resampled_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_step_size_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMAV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(windowed_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m, in \u001b[0;36mapply_rect_windowing_resampling\u001b[1;34m(nested_dict, num_resampled_rows, fs, window_size_ms, window_step_size_ms, aggregation_func, step_size_points)\u001b[0m\n\u001b[0;32m     35\u001b[0m     window_step_size_num_points \u001b[38;5;241m=\u001b[39m step_size_points\n\u001b[0;32m     36\u001b[0m debug_shapes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_step_size_num_points\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m window_step_size_num_points\n\u001b[1;32m---> 37\u001b[0m windows_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mwindowing_emg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrect_channel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size_num_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_step_size_num_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m debug_shapes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindows_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m windows_matrix\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Resample the gesture after to get 64 rows\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mwindowing_emg\u001b[1;34m(emg_data, window_size, step_size, feature)\u001b[0m\n\u001b[0;32m     23\u001b[0m window \u001b[38;5;241m=\u001b[39m emg_data[start:end, :]  \u001b[38;5;66;03m# Extract window for all channels\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     feature_matrix[i, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m feature \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     28\u001b[0m     feature_matrix[i, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean(window\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "windowed_df = apply_rect_windowing_resampling(loaded_dict, num_resampled_rows=64, fs=2000, window_size_ms=100, window_step_size_ms=50, aggregation_func=\"MAV\")\n",
    "\n",
    "print(f\"Completed in {(time.time() - start_time):.2f}s\\n\")\n",
    "\n",
    "print(windowed_df.shape)\n",
    "windowed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(windowed_df.iloc[0, 3].shape)\n",
    "windowed_df.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "windowed_df2 = apply_sliding_window_segmentation(loaded_dict, window_size_ms=100, step_size_num_points=10)\n",
    "\n",
    "print(f\"Completed in {(time.time() - start_time):.2f}s\\n\")\n",
    "\n",
    "print(windowed_df2.shape)\n",
    "windowed_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(windowed_df2.iloc[0, 3].shape)\n",
    "windowed_df2.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41843931",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "noFE_filesave_path = laptop_data_save_path+'\\\\noFE_windowed_segmentation_10steps_segraw_allEMG.pkl'\n",
    "windowed_df2.to_pickle(noFE_filesave_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ec2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to be like 500GB or something.......\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "windowed_df3 = apply_sliding_window_segmentation(loaded_dict, window_size_ms=100, step_size_num_points=1)\n",
    "\n",
    "print(f\"Completed in {(time.time() - start_time):.2f}s\\n\")\n",
    "\n",
    "print(windowed_df3.shape)  # (21217483, 4)\n",
    "windowed_df3.head()\n",
    "\n",
    "noFE_filesave_path = laptop_data_save_path+'\\\\noFE_windowed_segmentation_1step_segraw_allEMG.pkl'\n",
    "windowed_df3.to_pickle(noFE_filesave_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9de0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4df537",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "windowed_df4 = apply_sliding_window_segmentation(loaded_dict, window_size_ms=100, step_size_ms=100)\n",
    "\n",
    "print(f\"Completed in {(time.time() - start_time):.2f}s\\n\")\n",
    "\n",
    "print(windowed_df4.shape)  # (107444, 4)\n",
    "windowed_df4.head()\n",
    "\n",
    "noFE_filesave_path = laptop_data_save_path+'\\\\noFE_windowed_segmentation_allsteps_segraw_allEMG.pkl'\n",
    "windowed_df4.to_pickle(noFE_filesave_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb086df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noFE_filesave_path = laptop_data_save_path+'\\\\noFE_windowed_segraw_allEMG.pkl'\n",
    "\n",
    "windowed_df.to_pickle(noFE_filesave_path)#, index=False, header=True)  # , sep='\\t', encoding='utf-8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_windowed_df = pd.read_pickle(noFE_filesave_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f179d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded_windowed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_windowed_df.shape)\n",
    "loaded_windowed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_windowed_df.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(loaded_windowed_df.iloc[0, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eb0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
