{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f4b8d7",
   "metadata": {},
   "source": [
    "> This NB applies the moments feature engineerings to the new windowed h5 files and saves them as more h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "\n",
    "from segraw_featureengr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a03c1",
   "metadata": {},
   "source": [
    "## Khushaba Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de405d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_array_to_unit_std(arr):\n",
    "    \"\"\"\n",
    "    Normalizes a 1D numpy array so that its sample standard deviation equals 1.\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): 1D array of floats\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized array with std = 1\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    std = np.std(arr, ddof=1)  # Sample standard deviation\n",
    "    if std > 0:\n",
    "        return arr / std\n",
    "    else:\n",
    "        print(arr)\n",
    "        raise ValueError(\"Standard deviation is zero; cannot normalize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f25dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_spectral_features(segment, fs=2000, normalize_feature_vector=True):\n",
    "    \"\"\"\n",
    "    Compute all features for a segment: shape [window_len, n_channels]\n",
    "    Returns a flat array of all features for all channels, with shape [n_channels * n_features,]\n",
    "    \"\"\"\n",
    "    n_channels = segment.shape[1]\n",
    "    feature_vec = []\n",
    "    for ch in range(n_channels):\n",
    "        ch_data = segment[:, ch]\n",
    "        zero_log, m0 = zero_order(ch_data, fs)\n",
    "        second_log, m2 = second_order(ch_data, m0, fs)\n",
    "        fourth_log, m4 = fourth_order(ch_data, m0, fs)\n",
    "        spars = sparsity(m0, m2, m4)\n",
    "        irreg = irregularity_factor(ch_data, m0, m2, m4, fs)\n",
    "        feature_vec.extend([zero_log, second_log, fourth_log, spars, irreg])\n",
    "    feature_arr = np.array(feature_vec, dtype=np.float32)\n",
    "    if normalize_feature_vector:\n",
    "        return normalize_array_to_unit_std(feature_arr)\n",
    "    else:\n",
    "        return feature_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c73c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features_and_save(in_h5_path, out_h5_path, fs=2000, batch_size=10000):\n",
    "    with h5py.File(in_h5_path, 'r') as fin:\n",
    "        n_samples = fin['features'].shape[0]\n",
    "        window_len = fin['features'].shape[1]\n",
    "        n_channels = fin['features'].shape[2]\n",
    "        n_features_per_channel = 5  # zero, second, fourth, sparsity, irregularity\n",
    "        total_features = n_channels * n_features_per_channel\n",
    "\n",
    "        with h5py.File(out_h5_path, 'w') as fout:\n",
    "            features_ds = fout.create_dataset('features', shape=(n_samples, total_features), dtype='float32')\n",
    "            # Copy metadata\n",
    "            for k in ['participant', 'gesture_id', 'gesture_num']:\n",
    "                fout.create_dataset(k, data=fin[k])\n",
    "\n",
    "            for batch_start in range(0, n_samples, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, n_samples)\n",
    "                batch_segments = fin['features'][batch_start:batch_end]  # shape [batch, window_len, n_channels]\n",
    "                feats_batch = np.zeros((batch_end - batch_start, total_features), dtype=np.float32)\n",
    "                for i, segment in enumerate(batch_segments):\n",
    "                    feats_batch[i] = extract_spectral_features(segment, fs=fs)\n",
    "                features_ds[batch_start:batch_end] = feats_batch\n",
    "                print(f\"Processed {batch_end} / {n_samples} segments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613eacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_box_path = \"C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\\\Meta_Gesture_Project\\\\windowed_data_augmentation\\\\\"\n",
    "max_batch_size = 30_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000 / 104244 segments\n",
      "Processed 60000 / 104244 segments\n",
      "Processed 90000 / 104244 segments\n",
      "Processed 104244 / 104244 segments\n"
     ]
    }
   ],
   "source": [
    "#NoFE_windowed_window200ms_step100ms --> Took 8 mins\n",
    "\n",
    "process_features_and_save(\n",
    "    in_h5_path=base_box_path+\"NoFE_windowed_window200ms_step100ms.h5\",\n",
    "    out_h5_path=base_box_path+\"moments_windowed_window200ms_step100ms.h5\",\n",
    "    fs=2000,            \n",
    "    batch_size=max_batch_size     # Tune based on RAM/speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000 / 498872 segments\n",
      "Processed 60000 / 498872 segments\n",
      "Processed 90000 / 498872 segments\n",
      "Processed 120000 / 498872 segments\n",
      "Processed 150000 / 498872 segments\n",
      "Processed 180000 / 498872 segments\n",
      "Processed 210000 / 498872 segments\n",
      "Processed 240000 / 498872 segments\n",
      "Processed 270000 / 498872 segments\n",
      "Processed 300000 / 498872 segments\n",
      "Processed 330000 / 498872 segments\n",
      "Processed 360000 / 498872 segments\n",
      "Processed 390000 / 498872 segments\n",
      "Processed 420000 / 498872 segments\n",
      "Processed 450000 / 498872 segments\n",
      "Processed 480000 / 498872 segments\n",
      "Processed 498872 / 498872 segments\n"
     ]
    }
   ],
   "source": [
    "#NoFE_windowed_window300ms_step20ms --> Took 64 mins\n",
    "\n",
    "process_features_and_save(\n",
    "    in_h5_path=base_box_path+\"NoFE_windowed_window300ms_step20ms.h5\",\n",
    "    out_h5_path=base_box_path+\"moments_windowed_window300ms_step20ms.h5\",\n",
    "    fs=2000,            \n",
    "    batch_size=max_batch_size     # Tune based on RAM/speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ed40dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "print(\"Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b685ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Processed 30000 / 997448 segments\n",
      "Processed 60000 / 997448 segments\n",
      "Processed 90000 / 997448 segments\n",
      "Processed 120000 / 997448 segments\n",
      "Processed 150000 / 997448 segments\n",
      "Processed 180000 / 997448 segments\n",
      "Processed 210000 / 997448 segments\n",
      "Processed 240000 / 997448 segments\n",
      "Processed 270000 / 997448 segments\n",
      "Processed 300000 / 997448 segments\n",
      "Processed 330000 / 997448 segments\n",
      "Processed 360000 / 997448 segments\n",
      "Processed 390000 / 997448 segments\n",
      "Processed 420000 / 997448 segments\n",
      "Processed 450000 / 997448 segments\n",
      "Processed 480000 / 997448 segments\n",
      "Processed 510000 / 997448 segments\n",
      "Processed 540000 / 997448 segments\n",
      "Processed 570000 / 997448 segments\n",
      "Processed 600000 / 997448 segments\n",
      "Processed 630000 / 997448 segments\n",
      "Processed 660000 / 997448 segments\n",
      "Processed 690000 / 997448 segments\n",
      "Processed 720000 / 997448 segments\n",
      "Processed 750000 / 997448 segments\n",
      "Processed 780000 / 997448 segments\n",
      "Processed 810000 / 997448 segments\n",
      "Processed 840000 / 997448 segments\n",
      "Processed 870000 / 997448 segments\n",
      "Processed 900000 / 997448 segments\n",
      "Processed 930000 / 997448 segments\n",
      "Processed 960000 / 997448 segments\n",
      "Processed 990000 / 997448 segments\n",
      "Processed 997448 / 997448 segments\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#NoFE_windowed_window300ms_step10ms\n",
    "## This one ran for 37 minutes but never printed anything... I'm assuming something broke? This is the big one FWIW. Maybe it's still loading the data in? Idk\n",
    "\n",
    "print(\"Started\")\n",
    "\n",
    "process_features_and_save(\n",
    "    in_h5_path=base_box_path+\"NoFE_windowed_window300ms_step10ms.h5\",\n",
    "    out_h5_path=base_box_path+\"moments_windowed_window300ms_step10ms.h5\",\n",
    "    fs=2000,            \n",
    "    batch_size=max_batch_size     # Tune based on RAM/speed\n",
    ")\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51257da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Processed 30000 / 514872 segments\n",
      "Processed 60000 / 514872 segments\n",
      "Processed 90000 / 514872 segments\n",
      "Processed 120000 / 514872 segments\n",
      "Processed 150000 / 514872 segments\n",
      "Processed 180000 / 514872 segments\n",
      "Processed 210000 / 514872 segments\n",
      "Processed 240000 / 514872 segments\n",
      "Processed 270000 / 514872 segments\n",
      "Processed 300000 / 514872 segments\n",
      "Processed 330000 / 514872 segments\n",
      "Processed 360000 / 514872 segments\n",
      "Processed 390000 / 514872 segments\n",
      "Processed 420000 / 514872 segments\n",
      "Processed 450000 / 514872 segments\n",
      "Processed 480000 / 514872 segments\n",
      "Processed 510000 / 514872 segments\n",
      "Processed 514872 / 514872 segments\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#NoFE_windowed_window200ms_step20ms\n",
    "\n",
    "print(\"Started\")\n",
    "\n",
    "process_features_and_save(\n",
    "    in_h5_path=base_box_path+\"NoFE_windowed_window200ms_step20ms.h5\",\n",
    "    out_h5_path=base_box_path+\"moments_windowed_window200ms_step20ms.h5\",\n",
    "    fs=2000,            \n",
    "    batch_size=max_batch_size     # Tune based on RAM/speed\n",
    ")\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdcef2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
