{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f4b8d7",
   "metadata": {},
   "source": [
    "> This NB applies windowing to the preprocessed segmented raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e073f3",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be028e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pids = ['P102', 'P103', 'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111', 'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P131', 'P132', 'P004', 'P005', 'P006', 'P008', 'P010', 'P011']\n",
    "all_gesture_types = ['pan', 'delete', 'close', 'select-single', 'rotate', 'zoom-in', 'zoom-out', 'open', 'move', 'duplicate']\n",
    "all_gesture_nums = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fbe4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Completed in 469.9323408603668s\n"
     ]
    }
   ],
   "source": [
    "# This takes 8 minutes to run...\n",
    "\n",
    "laptop_data_save_path = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\Meta_Gesture_Project'\n",
    "full_filesave_path = laptop_data_save_path+'\\\\ppdsegraw_allEMG.json'\n",
    "\n",
    "print(\"Loading\")\n",
    "start_time = time.time()\n",
    "with open(full_filesave_path, 'r') as f:\n",
    "    loaded_dict = json.load(f)\n",
    "end_time = time.time()\n",
    "print(f\"Completed in {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4126731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b862703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['P102', 'P103', 'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111', 'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P131', 'P132', 'P004', 'P005', 'P006', 'P008', 'P010', 'P011'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0381f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33e599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pan', 'duplicate', 'zoom-out', 'zoom-in', 'move', 'rotate', 'select-single', 'delete', 'close', 'open'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict['P102'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68bf16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6380"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_dict['P102']['pan']['1']['EMG'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85b376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momona's code...\n",
    "\n",
    "def resample(data, N):\n",
    "    N_init = len(data)\n",
    "    x = np.linspace(0, N_init-1, N)  # x coordinates at which to evaluate interpolated values\n",
    "    xp = np.linspace(0, N_init-1, N_init)  # x coordinates of data points\n",
    "    return np.interp(x, xp, data)\n",
    "\n",
    "def rectify_EMG(data):\n",
    "    \n",
    "    # 1. high-pass filter @ 40 hz, 4th order butterworth\n",
    "    sos_high = signal.butter(N=4, Wn=40, btype='high', fs=2000, output='sos')\n",
    "    hp_filtered = signal.sosfilt(sos_high, data)\n",
    "    \n",
    "    # 2. demean and rectify\n",
    "    emg_mean = hp_filtered - np.mean(hp_filtered)\n",
    "    rectified = abs(emg_mean)\n",
    "    \n",
    "    # 3. low-pass filter @ 40 Hz, 4th order butterworth\n",
    "    sos_low = signal.butter(N=4,Wn=40, btype='low', fs=2000, output='sos')\n",
    "    lp_filtered = signal.sosfilt(sos_low, rectified)\n",
    "    return lp_filtered\n",
    "\n",
    "def compute_SNR(s,fs=2000):\n",
    "    nperseg = len(s)//4.5\n",
    "    f,Pxx = welch(s,fs=2000,window='hamming',\n",
    "        nperseg=nperseg,noverlap=nperseg//2,\n",
    "        scaling='density',detrend=False)\n",
    "    idx400 = list(abs(f-800)).index(min(abs(f-800)))\n",
    "    N = len(f)-idx400\n",
    "    noise_power = sum(Pxx[idx400:])/N*len(f)\n",
    "    total_power = sum(Pxx)\n",
    "    return 10*np.log10(total_power / noise_power)\n",
    "\n",
    "def bandpass_EMG(data):\n",
    "    sos_high = signal.butter(N=4,Wn=[10,500],btype='bandpass',fs=2000,output='sos')\n",
    "    EMG_filt = signal.sosfilt(sos_high,data)\n",
    "    EMG_filt = EMG_filt - np.mean(EMG_filt)\n",
    "    return EMG_filt\n",
    "\n",
    "def movavg_EMG(data,window=100,overlap=0.5,fs=2000):\n",
    "    # 200 ms window, overlap = 0 is no overlap, overlap=1 is complete overlap \n",
    "    N = int(1*window/1000*fs) # number of datapoints in one window\n",
    "    N_overlap = int(N*overlap)\n",
    "    \n",
    "    movavg_data = []\n",
    "    \n",
    "    ix = N\n",
    "    while ix < len(data):\n",
    "        movavg_data.append(np.mean(data[ix-N:ix]))\n",
    "        ix = ix + (N - N_overlap)\n",
    "    \n",
    "    return np.asarray(movavg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9dd03",
   "metadata": {},
   "source": [
    "## For the non-FE\n",
    "1. rectify the data,\n",
    "2. do MAV or RMS (moving average for now?),\n",
    "3. pick a window size (50-300ms) and overlap (50%), \n",
    "4. use np.interp to resample after\n",
    "\n",
    "> Need to do windowing on the non-FE data since if we have to apply MAV/RMS to the data (to make it less noisy) that would reduce it to one sample per channel. Windowing is how we get multiple samples per channel\n",
    "\n",
    "> Note: preprocessed data file has already been BPF'd, MS'd, and standardized (std=1 for each gesture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13151d",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline for raw (non-handcrafted) EMG data:\n",
    "\n",
    "1. Rectify the signal:\n",
    "   - Take the absolute value of the raw EMG to ensure all values are positive\n",
    "   - This is a standard EMG preprocessing step to reflect signal energy\n",
    "\n",
    "2. Apply temporal feature extraction (e.g., MAV or RMS):\n",
    "   - The signal is segmented into overlapping windows (e.g., 200ms windows with 50% overlap = 100ms step)\n",
    "   - Each window is reduced to a **single scalar per channel** using an aggregation function like MAV (mean absolute value) or RMS\n",
    "   - This turns a long sequence of raw EMG into a **low-resolution sequence of extracted features**, one per window\n",
    "\n",
    "3. Resample the feature sequence:\n",
    "   - After windowing, each gesture trial has a different number of windows (due to gesture duration)\n",
    "   - We use resampling (e.g., `scipy.signal.resample`) to interpolate this variable-length sequence to a fixed length (e.g., 64 time steps)\n",
    "   - This ensures all samples have the same shape `[64, num_channels]` and can be fed into CNN/LSTM models as batched input\n",
    "\n",
    "# Why this is necessary:\n",
    "- Without windowing + feature aggregation, rectifying alone would reduce each gesture to a single averaged vector (i.e., 1 sample per channel) — losing temporal structure\n",
    "- Windowing preserves **time-local structure** while smoothing out noise via MAV/RMS\n",
    "- Resampling standardizes the **temporal resolution** across gestures with different durations\n",
    "\n",
    "# Note:\n",
    "- Input EMG data is already:\n",
    "   - Bandpass filtered (BPF)\n",
    "   - Mean subtracted (MS)\n",
    "   - Standardized per gesture (std = 1). This was done gesture-wide to account for some muscles being bigger/stronger than others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing_emg(emg_data, window_size=200, step_size=100, feature=\"MAV\"):\n",
    "    \"\"\"\n",
    "    Applies windowing and extracts MAV or RMS features from EMG data.\n",
    "\n",
    "    Parameters:\n",
    "        emg_data (numpy array): Shape (num_samples, 16), where 16 is the number of channels.\n",
    "        window_size (int): Number of samples per window (default 200 for 100 ms at 2000 Hz).\n",
    "        step_size (int): Step size for sliding window (default 100 for 50% overlap).\n",
    "        feature (str): Feature type, either \"MAV\" or \"RMS\".\n",
    "\n",
    "    Returns:\n",
    "        feature_matrix (numpy array): Shape (num_windows, 16), where each row is a feature vector.\n",
    "    \"\"\"\n",
    "    # NOTE: num_samples here refers to how many scalar values are present, this is more along the lines of sequence length actually\n",
    "    num_samples, num_channels = emg_data.shape\n",
    "    num_windows = int((num_samples - window_size) // step_size + 1)  # Total windows\n",
    "    # NOTE: variablity in samples (num_samples-->num_windows) leads to different number of windows, but this gets standardized to 64 in resampling\n",
    "    feature_matrix = np.zeros((num_windows, num_channels))  # Initialize output matrix\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start = i * step_size\n",
    "        end = start + window_size\n",
    "        window = emg_data[start:end, :]  # Extract window for all channels\n",
    "\n",
    "        if feature == \"MAV\":\n",
    "            feature_matrix[i, :] = np.mean(np.abs(window), axis=0)\n",
    "        elif feature == \"RMS\":\n",
    "            feature_matrix[i, :] = np.sqrt(np.mean(window**2, axis=0))\n",
    "        else:\n",
    "            raise ValueError(\"Feature must be 'MAV' or 'RMS'\")\n",
    "\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40578a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rect_windowing_resampling(nested_dict, num_resampled_rows=64, fs=2000, window_size_ms=200, window_step_size_ms=100, aggregation_func=\"MAV\", step_size_points=None):\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    result_data = []\n",
    "\n",
    "    # Track shape stats for final print\n",
    "    debug_shapes = {}\n",
    "\n",
    "    # Iterate over all participants, gesture IDs, and gesture numbers\n",
    "    for pid, gestures in nested_dict.items():\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for gesture_num, modality_data in gesture_data.items():\n",
    "                # Initialize dict to store the result for the current gesture\n",
    "                result_dict = {\n",
    "                    'Participant': pid,\n",
    "                    'Gesture_ID': gesture_name,\n",
    "                    'Gesture_Num': gesture_num\n",
    "                }\n",
    "                \n",
    "                # Apply the feature engineering functions to each EMG channel\n",
    "                for modality_str, single_gesture_data in modality_data.items():\n",
    "                    # single_gesture_data is a list with 16 lists as elements\n",
    "                    gesture_npy = np.array(single_gesture_data).T\n",
    "                    debug_shapes['raw_input_shape'] = gesture_npy.shape\n",
    "                    # Rectify the data\n",
    "                    rect_channel_data = np.abs(gesture_npy)\n",
    "                    debug_shapes['rectified_shape'] = rect_channel_data.shape\n",
    "                    # Divide by 1000 to get out of ms\n",
    "                    window_size_num_points = int(fs * window_size_ms // 1000)\n",
    "                    debug_shapes['window_size_num_points'] = window_size_num_points\n",
    "                    # Doing this so I can easily set the step size to 1 point, without having to do the math/rounding\n",
    "                    if step_size_points is None:\n",
    "                        window_step_size_num_points = int(fs * window_step_size_ms // 1000)\n",
    "                    else:\n",
    "                        window_step_size_num_points = step_size_points\n",
    "                    debug_shapes['window_step_size_num_points'] = window_step_size_num_points\n",
    "                    windows_matrix = windowing_emg(rect_channel_data, window_size=window_size_num_points, step_size=window_step_size_num_points, feature=aggregation_func)\n",
    "                    debug_shapes['windows_matrix'] = windows_matrix.shape\n",
    "\n",
    "                    # Resample the gesture after to get 64 rows\n",
    "                    resampled_windows_matrix = np.array([resample(windows_matrix[:, col], num_resampled_rows) for col in range(windows_matrix.shape[1])])\n",
    "                    debug_shapes['resampled_matrix'] = resampled_windows_matrix.shape\n",
    "\n",
    "                    result_dict.update({\n",
    "                        f'windowed_ts_data': resampled_windows_matrix\n",
    "                    })\n",
    "                # Append the result for the current gesture to the result list\n",
    "                result_data.append(result_dict)\n",
    "\n",
    "    # Print out debug info from the *last* sample\n",
    "    print(\"\\n--- DEBUG SHAPES (from last gesture processed) ---\")\n",
    "    for k, v in debug_shapes.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    # Convert the result list to a DataFrame (everything should have the same lengths now)\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87361727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sliding_window_segmentation(nested_dict, fs=2000, window_size_ms=200, step_size_ms=100, step_size_num_points=None):\n",
    "    \"\"\"\n",
    "    Implements the 'sliding window cropping' approach described in the augmentation paper.\n",
    "    Each gesture is split into multiple fixed-length overlapping windows (segments),\n",
    "    and each segment becomes its own training sample.\n",
    "\n",
    "    This differs from the original function, which compresses each gesture into a single\n",
    "    fixed-size time-series feature matrix using feature extraction + resampling.\n",
    "\n",
    "    This function does not perform MAV/RMS feature extraction or resampling.\n",
    "    Each segment retains its raw (or rectified) EMG values.\n",
    "    \"\"\"\n",
    "\n",
    "    result_data = []\n",
    "    debug_shapes = {}\n",
    "\n",
    "    # Convert ms to number of sample points\n",
    "    window_size_pts = int(fs * window_size_ms / 1000)\n",
    "    debug_shapes['window_size_pts'] = window_size_pts\n",
    "    if step_size_num_points is None:\n",
    "        step_size_pts = int(fs * step_size_ms / 1000)\n",
    "    else:\n",
    "        step_size_pts = step_size_num_points\n",
    "    debug_shapes['step_size_pts'] = step_size_pts\n",
    "\n",
    "    for pid, gestures in nested_dict.items():\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for gesture_num, modality_data in gesture_data.items():\n",
    "                for modality_str, single_gesture_data in modality_data.items():\n",
    "                    # Convert to NumPy and transpose to shape [T, C] where T = time, C = channels\n",
    "                    gesture_npy = np.array(single_gesture_data).T\n",
    "                    debug_shapes['raw_input_shape'] = gesture_npy.shape\n",
    "\n",
    "                    # Rectify the signal (absolute value)\n",
    "                    rect_channel_data = np.abs(gesture_npy)\n",
    "                    debug_shapes['rectified_shape'] = rect_channel_data.shape\n",
    "\n",
    "                    total_samples = rect_channel_data.shape[0]\n",
    "                    num_channels = rect_channel_data.shape[1]\n",
    "\n",
    "                    # Slide a window over the gesture trial\n",
    "                    for start_idx in range(0, total_samples - window_size_pts + 1, step_size_pts):\n",
    "                        end_idx = start_idx + window_size_pts\n",
    "                        segment = rect_channel_data[start_idx:end_idx, :]  # Shape: [window_size_pts, num_channels]\n",
    "\n",
    "                        debug_shapes['segment_shape'] = segment.shape\n",
    "\n",
    "                        result_data.append({\n",
    "                            'Participant': pid,\n",
    "                            'Gesture_ID': gesture_name,\n",
    "                            'Gesture_Num': gesture_num,\n",
    "                            'windowed_ts_data': segment\n",
    "                        })\n",
    "\n",
    "    # Print debug shapes from last segment processed\n",
    "    print(\"\\n--- DEBUG SHAPES (from last segment processed) ---\")\n",
    "    for k, v in debug_shapes.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    return pd.DataFrame(result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adddf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segments_to_hdf5_batched(\n",
    "    nested_dict, h5_path, fs=2000, window_size_ms=200, step_size_ms=100, step_size_num_points=None, batch_size=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies sliding window and saves the results to HDF5 file at h5_path, batching writes for speed.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "\n",
    "    print(\"Counting total number of segments...\")\n",
    "    total_segments = 0\n",
    "    window_size_pts = int(fs * window_size_ms / 1000)\n",
    "    if step_size_num_points is None:\n",
    "        step_size_pts = int(fs * step_size_ms / 1000)\n",
    "    else:\n",
    "        step_size_pts = step_size_num_points\n",
    "\n",
    "    num_channels = None\n",
    "    for pid, gestures in nested_dict.items():\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for gesture_num, modality_data in gesture_data.items():\n",
    "                for modality_str, single_gesture_data in modality_data.items():\n",
    "                    gesture_npy = np.array(single_gesture_data).T\n",
    "                    total_samples = gesture_npy.shape[0]\n",
    "                    if num_channels is None:\n",
    "                        num_channels = gesture_npy.shape[1]\n",
    "                    n_segments = max(0, (total_samples - window_size_pts) // step_size_pts + 1)\n",
    "                    total_segments += n_segments\n",
    "\n",
    "    print(f\"Total number of segments: {total_segments}, Num channels: {num_channels}\")\n",
    "\n",
    "    # Create datasets\n",
    "    with h5py.File(h5_path, 'w') as f:\n",
    "        X = f.create_dataset(\n",
    "            'features',\n",
    "            shape=(total_segments, window_size_pts, num_channels),\n",
    "            dtype='float32',\n",
    "            compression=\"gzip\",\n",
    "            #compression=None,  # Setting to None makes it faster... idk if this is important or not at this stage...\n",
    "            chunks=(batch_size, window_size_pts, num_channels)\n",
    "        )\n",
    "        pid_arr      = f.create_dataset('participant', shape=(total_segments,), dtype='S32')\n",
    "        gesture_id   = f.create_dataset('gesture_id', shape=(total_segments,), dtype='S32')\n",
    "        gesture_num_ds  = f.create_dataset('gesture_num', shape=(total_segments,), dtype='i4')\n",
    "\n",
    "        seg_idx = 0\n",
    "        batch_segments = []\n",
    "        batch_pids = []\n",
    "        batch_gesture_ids = []\n",
    "        batch_gesture_nums = []\n",
    "\n",
    "        def flush_batch():\n",
    "            nonlocal seg_idx\n",
    "            n = len(batch_segments)\n",
    "            if n == 0:\n",
    "                return\n",
    "            X[seg_idx:seg_idx+n, :, :] = np.stack(batch_segments)\n",
    "            pid_arr[seg_idx:seg_idx+n] = batch_pids\n",
    "            gesture_id[seg_idx:seg_idx+n] = batch_gesture_ids\n",
    "            gesture_num_ds[seg_idx:seg_idx+n] = batch_gesture_nums\n",
    "            seg_idx += n\n",
    "            batch_segments.clear()\n",
    "            batch_pids.clear()\n",
    "            batch_gesture_ids.clear()\n",
    "            batch_gesture_nums.clear()\n",
    "\n",
    "        for pid, gestures in nested_dict.items():\n",
    "            for gesture_name, gesture_data in gestures.items():\n",
    "                for gesture_num, modality_data in gesture_data.items():\n",
    "                    for modality_str, single_gesture_data in modality_data.items():\n",
    "                        gesture_npy = np.array(single_gesture_data).T\n",
    "                        rect_channel_data = np.abs(gesture_npy)\n",
    "                        total_samples = rect_channel_data.shape[0]\n",
    "                        for start_idx in range(0, total_samples - window_size_pts + 1, step_size_pts):\n",
    "                            end_idx = start_idx + window_size_pts\n",
    "                            segment = rect_channel_data[start_idx:end_idx, :]  # shape: [window_size_pts, num_channels]\n",
    "                            batch_segments.append(segment)\n",
    "                            batch_pids.append(str(pid).encode('utf-8'))\n",
    "                            batch_gesture_ids.append(str(gesture_name).encode('utf-8'))\n",
    "                            batch_gesture_nums.append(int(gesture_num))\n",
    "                            if len(batch_segments) == batch_size:\n",
    "                                flush_batch()\n",
    "\n",
    "        # Write any leftovers\n",
    "        flush_batch()\n",
    "\n",
    "        print(f\"Wrote {seg_idx} segments to {h5_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ec7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_h5_file(h5_path, num_samples=5):\n",
    "    \"\"\"\n",
    "    Print out the structure, shapes, dtypes, and a few example values from an HDF5 file.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        print(f\"\\n{'='*35}\")\n",
    "        print(f\"HDF5 FILE: {h5_path}\")\n",
    "        print(f\"{'='*35}\")\n",
    "        for key in f.keys():\n",
    "            data = f[key]\n",
    "            print(f\"Dataset '{key}':\")\n",
    "            print(f\"  shape: {data.shape}\")\n",
    "            print(f\"  dtype: {data.dtype}\")\n",
    "            #if data.shape[0] > 0:\n",
    "            #    print(f\"  First {num_samples} values: {data[:num_samples]}\")\n",
    "            print(\"-\"*30)\n",
    "\n",
    "        # Example: check that all datasets have the same first dimension\n",
    "        lengths = [f[key].shape[0] for key in f.keys()]\n",
    "        print(\"All datasets first-dimension lengths:\", lengths)\n",
    "        if len(set(lengths)) == 1:\n",
    "            print(\"✅ All datasets aligned in sample count!\")\n",
    "        else:\n",
    "            print(\"❌ MISMATCHED lengths!\")\n",
    "\n",
    "        # Optionally: check contents of one segment\n",
    "        if \"features\" in f.keys() and f[\"features\"].shape[0] > 0:\n",
    "            sample_feat = f[\"features\"][0]\n",
    "            print(f\"\\nSample features[0] shape: {sample_feat.shape}\")\n",
    "            print(\"Sample features[0] values:\\n\", sample_feat[:4, :4])  # Show top left corner\n",
    "\n",
    "        print(f\"{'='*35}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c4907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total number of segments...\n",
      "Total number of segments: 514872, Num channels: 16\n",
      "Wrote 514872 segments to NoFE_windowed_window200ms_step50ms.h5\n"
     ]
    }
   ],
   "source": [
    "# Set parameters and call the function!\n",
    "window_size_ms = 200      # Window size in ms\n",
    "step_size_ms = 20         # Step size in ms\n",
    "batch_size = 10000        # Tune based on your RAM\n",
    "\n",
    "save_segments_to_hdf5_batched(\n",
    "    loaded_dict,\n",
    "    h5_path=\"NoFE_windowed_window200ms_step20ms.h5\",\n",
    "    fs=2000,\n",
    "    window_size_ms=window_size_ms,\n",
    "    step_size_ms=step_size_ms,\n",
    "    step_size_num_points=None,  # or set this for points-based stride\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04858757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "HDF5 FILE: NoFE_windowed_window200ms_step20ms.h5\n",
      "===================================\n",
      "Dataset 'features':\n",
      "  shape: (514872, 400, 16)\n",
      "  dtype: float32\n",
      "------------------------------\n",
      "Dataset 'gesture_id':\n",
      "  shape: (514872,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "Dataset 'gesture_num':\n",
      "  shape: (514872,)\n",
      "  dtype: int32\n",
      "------------------------------\n",
      "Dataset 'participant':\n",
      "  shape: (514872,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "All datasets first-dimension lengths: [514872, 514872, 514872, 514872]\n",
      "✅ All datasets aligned in sample count!\n",
      "\n",
      "Sample features[0] shape: (400, 16)\n",
      "Sample features[0] values:\n",
      " [[0.03515185 0.02158573 0.00215493 0.02088776]\n",
      " [0.18355413 0.11790513 0.0108623  0.11257195]\n",
      " [0.40104574 0.27782953 0.02398603 0.26320285]\n",
      " [0.46639353 0.3787155  0.02814692 0.3591374 ]]\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_h5_file(\"NoFE_windowed_window200ms_step20ms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e0015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db1eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total number of segments...\n",
      "Total number of segments: 104244, Num channels: 16\n",
      "Wrote 104244 segments to NoFE_windowed_window200ms_step100ms.h5\n"
     ]
    }
   ],
   "source": [
    "# Set parameters and call the function!\n",
    "window_size_ms = 200      # Window size in ms\n",
    "step_size_ms = 100         # Step size in ms\n",
    "batch_size = 10000        # Tune based on your RAM\n",
    "\n",
    "save_segments_to_hdf5_batched(\n",
    "    loaded_dict,\n",
    "    h5_path=\"NoFE_windowed_window200ms_step100ms.h5\",\n",
    "    fs=2000,\n",
    "    window_size_ms=window_size_ms,\n",
    "    step_size_ms=step_size_ms,\n",
    "    step_size_num_points=None,  # or set this for points-based stride\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012ecb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "HDF5 FILE: NoFE_windowed_window200ms_step100ms.h5\n",
      "===================================\n",
      "Dataset 'features':\n",
      "  shape: (104244, 400, 16)\n",
      "  dtype: float32\n",
      "------------------------------\n",
      "Dataset 'gesture_id':\n",
      "  shape: (104244,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "Dataset 'gesture_num':\n",
      "  shape: (104244,)\n",
      "  dtype: int32\n",
      "------------------------------\n",
      "Dataset 'participant':\n",
      "  shape: (104244,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "All datasets first-dimension lengths: [104244, 104244, 104244, 104244]\n",
      "✅ All datasets aligned in sample count!\n",
      "\n",
      "Sample features[0] shape: (400, 16)\n",
      "Sample features[0] values:\n",
      " [[0.03515185 0.02158573 0.00215493 0.02088776]\n",
      " [0.18355413 0.11790513 0.0108623  0.11257195]\n",
      " [0.40104574 0.27782953 0.02398603 0.26320285]\n",
      " [0.46639353 0.3787155  0.02814692 0.3591374 ]]\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_h5_file(\"NoFE_windowed_window200ms_step100ms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c97a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a80c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total number of segments...\n",
      "Total number of segments: 498872, Num channels: 16\n",
      "Wrote 498872 segments to NoFE_windowed_window300ms_step20ms.h5\n"
     ]
    }
   ],
   "source": [
    "# This took 17 minutes for some reason...\n",
    "\n",
    "# Set parameters and call the function!\n",
    "window_size_ms = 300      # Window size in ms\n",
    "step_size_ms = 20         # Step size in ms\n",
    "batch_size = 10000        # Tune based on your RAM\n",
    "\n",
    "save_segments_to_hdf5_batched(\n",
    "    loaded_dict,\n",
    "    h5_path=\"NoFE_windowed_window300ms_step20ms.h5\",\n",
    "    fs=2000,\n",
    "    window_size_ms=window_size_ms,\n",
    "    step_size_ms=step_size_ms,\n",
    "    step_size_num_points=None,  # or set this for points-based stride\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e4dd419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "HDF5 FILE: NoFE_windowed_window300ms_step20ms.h5\n",
      "===================================\n",
      "Dataset 'features':\n",
      "  shape: (498872, 600, 16)\n",
      "  dtype: float32\n",
      "------------------------------\n",
      "Dataset 'gesture_id':\n",
      "  shape: (498872,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "Dataset 'gesture_num':\n",
      "  shape: (498872,)\n",
      "  dtype: int32\n",
      "------------------------------\n",
      "Dataset 'participant':\n",
      "  shape: (498872,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "All datasets first-dimension lengths: [498872, 498872, 498872, 498872]\n",
      "✅ All datasets aligned in sample count!\n",
      "\n",
      "Sample features[0] shape: (600, 16)\n",
      "Sample features[0] values:\n",
      " [[0.03515185 0.02158573 0.00215493 0.02088776]\n",
      " [0.18355413 0.11790513 0.0108623  0.11257195]\n",
      " [0.40104574 0.27782953 0.02398603 0.26320285]\n",
      " [0.46639353 0.3787155  0.02814692 0.3591374 ]]\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_h5_file(\"NoFE_windowed_window300ms_step20ms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf796bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total number of segments...\n",
      "Total number of segments: 997448, Num channels: 16\n",
      "Wrote 997448 segments to NoFE_windowed_window300ms_step10ms.h5\n"
     ]
    }
   ],
   "source": [
    "# This took 36 minutes to run\n",
    "\n",
    "# Set parameters and call the function!\n",
    "window_size_ms = 300      # Window size in ms\n",
    "step_size_ms = 10         # Step size in ms\n",
    "batch_size = 10000        # Tune based on your RAM\n",
    "\n",
    "save_segments_to_hdf5_batched(\n",
    "    loaded_dict,\n",
    "    h5_path=\"NoFE_windowed_window300ms_step10ms.h5\",\n",
    "    fs=2000,\n",
    "    window_size_ms=window_size_ms,\n",
    "    step_size_ms=step_size_ms,\n",
    "    step_size_num_points=None,  # or set this for points-based stride\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10bf082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "HDF5 FILE: NoFE_windowed_window300ms_step10ms.h5\n",
      "===================================\n",
      "Dataset 'features':\n",
      "  shape: (997448, 600, 16)\n",
      "  dtype: float32\n",
      "------------------------------\n",
      "Dataset 'gesture_id':\n",
      "  shape: (997448,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "Dataset 'gesture_num':\n",
      "  shape: (997448,)\n",
      "  dtype: int32\n",
      "------------------------------\n",
      "Dataset 'participant':\n",
      "  shape: (997448,)\n",
      "  dtype: |S32\n",
      "------------------------------\n",
      "All datasets first-dimension lengths: [997448, 997448, 997448, 997448]\n",
      "✅ All datasets aligned in sample count!\n",
      "\n",
      "Sample features[0] shape: (600, 16)\n",
      "Sample features[0] values:\n",
      " [[0.03515185 0.02158573 0.00215493 0.02088776]\n",
      " [0.18355413 0.11790513 0.0108623  0.11257195]\n",
      " [0.40104574 0.27782953 0.02398603 0.26320285]\n",
      " [0.46639353 0.3787155  0.02814692 0.3591374 ]]\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_h5_file(\"NoFE_windowed_window300ms_step10ms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eb0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
