{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b33801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909e5d57",
   "metadata": {},
   "source": [
    "What exactly is the gating mechanism here?\n",
    "\n",
    "The gate is a single linear layer that sees the concatenation of the backbone embedding h ∈ R^emb_dim, and a user embedding u ∈ R^user_dim (from an embedding table during pretrain, or your PEFT/override at test).\n",
    "\n",
    "g = Linear([h ; u]) ∈ R^E          # raw gate logits for E experts <br>\n",
    "w = softmax(g)                      # probs over experts <br>\n",
    "w = topk_mask(w, k=TOP_K)           # keep only top-k, renormalize <br>\n",
    "\n",
    "Non-selected experts (masked by top-k) do not receive gradient on that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81c2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "Global seed set to 17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from MOE_model_classes import *\n",
    "from MOE_quick_cls_heads import *\n",
    "from MOE_training import *\n",
    "from diagnostics import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# Add the parent directory folder to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from configs.hyperparam_tuned_configs import *\n",
    "from utils.DNN_FT_funcs import *\n",
    "#load_expdef_gestures, make_data_split, process_split\n",
    "from utils.gesture_dataset_classes import *\n",
    "#from utils.viz_utils.quick_analysis_plots import *\n",
    "#from utils.full_study_funcs import * \n",
    "\n",
    "from utils.global_seed import set_seed\n",
    "set_seed()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1d86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 17\n"
     ]
    }
   ],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93cb632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================== MODEL CONFIG ===========================\n",
    "basicMOE_config = {\n",
    "    # ----- Identity / IO -----\n",
    "    \"model_str\": \"MoEClassifier\",          # class name\n",
    "    \"backbone_type\": \"TinyBackbone\",\n",
    "    \"feature_engr\": \"moments\",             # keep if your loaders rely on this\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # PRETRAINING\n",
    "    \"num_epochs\": 300,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 7e-05,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"lr_scheduler_patience\": 7,\n",
    "    \"lr_scheduler_factor\": 0.2,\n",
    "    \"earlystopping_patience\": 10,\n",
    "    \"earlystopping_min_delta\": 0.005,\n",
    "    \"num_train_gesture_trials\": NUM_TRAIN_GESTURES,\n",
    "    \"num_pretrain_users\": 24,\n",
    "\n",
    "    # ----- Data shape (maps to TinyBackbone) -----\n",
    "    \"num_channels\": 16,                    # -> TinyBackbone.in_ch\n",
    "    \"sequence_length\": 5,                  # -> TinyBackbone.seq_len\n",
    "    \"time_steps\": 1,                       # legacy (kept for compatibility)\n",
    "\n",
    "    # ----- Embeddings / dims -----\n",
    "    \"emb_dim\": 64,                    # -> TinyBackbone.emb_dim and Expert/keys\n",
    "    \"user_emb_dim\": 16,                  # -> user embedding dim\n",
    "\n",
    "    # ----- Task -----\n",
    "    \"num_classes\": 10,\n",
    "\n",
    "    # ----- MoE layout -----\n",
    "    \"num_experts\": 6,            # -> number of Expert heads\n",
    "    \"top_k\": 2,                        # -> gating sparsity (None for dense)\n",
    "    \"use_user_table\": True,      # -> whether to learn user embedding table\n",
    "    \"gate_type\": \"user_aware\",             # (\"user_aware\" | \"feature_only\")\n",
    "    \"gate_dense_before_topk\": True,        # softmax then mask (matches code)\n",
    "\n",
    "    # ----- Expert head (maps to Expert) -----\n",
    "    \"expert_hidden\": 64,              # Expert.fc1 hidden = emb_dim\n",
    "    \"expert_dropout\": 0.10,           # -> Expert.drop p\n",
    "    \"expert_norm\": \"layernorm\",            # fixed in code; keeping for clarity\n",
    "\n",
    "    # ----- Prototype / keys -----\n",
    "    \"expert_keys_init_std\": 0.1,           # -> nn.Parameter N(0, std^2)\n",
    "\n",
    "    # ----- Regularization / losses -----\n",
    "    \"label_smooth\": 0.05,\n",
    "    \"gate_balance_coef\": 0.05,\n",
    "\n",
    "    # ----- Training / logging -----\n",
    "    \"seed\": 17,\n",
    "    \"print_every\": 50,\n",
    "    \"user_split_json_filepath\": \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\fixed_user_splits\\\\24_8_user_splits_RS17.json\",\n",
    "    \"results_save_dir\": f\"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\results\\\\MOE\\\\{timestamp}_MOE\",\n",
    "    \"models_save_dir\": f\"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\models\\\\MOE\\\\{timestamp}_MOE\",\n",
    "\n",
    "    # ----- FINETUNING / PEFT / adaptation -----\n",
    "    \"finetune_strategy\": \"linear_probing\",\n",
    "    \"use_dropout_during_peft\": False,\n",
    "    \"ft_learning_rate\": 0.001,\n",
    "    \"ft_batch_size\": 10,\n",
    "    \"num_ft_epochs\": 100,\n",
    "    \"ft_weight_decay\": 1e-3,\n",
    "    \"ft_lr_scheduler_patience\": 7,\n",
    "    \"ft_lr_scheduler_factor\": 0.25,\n",
    "    \"ft_earlystopping_patience\": 10,\n",
    "    \"ft_earlystopping_min_delta\": 0.003,\n",
    "    \"num_ft_gesture_trials\": NUM_FT_GESTURES,\n",
    "    \"num_testft_users\": 8,\n",
    "    \"save_ft_models\": False,\n",
    "    \"reset_ft_layers\": False,\n",
    "\n",
    "    # MISC\n",
    "    \"cluster_iter_str\": 'Iter18',\n",
    "    \"use_earlystopping\": True,\n",
    "    \"verbose\": False,\n",
    "    \"log_each_pid_results\": False, \n",
    "\n",
    "\n",
    "    # ===== Legacy CNN/LSTM keys kept for compatibility (NOT IMPLEMENTED) =====\n",
    "    # \"use_batch_norm\": False,             # NOT IMPLEMENTED in TinyBackbone/Expert\n",
    "    # \"conv_layers\": [[32, 3, 1]],         # NOT IMPLEMENTED\n",
    "    # \"fc_layers\": [256],                  # NOT IMPLEMENTED (backbone uses fixed MLP)\n",
    "    # \"fc_dropout\": 0.3,                   # NOT IMPLEMENTED\n",
    "    # \"cnn_dropout\": 0.0,                  # NOT IMPLEMENTED\n",
    "    # \"dense_cnnlstm_dropout\": 0.1,        # NOT IMPLEMENTED\n",
    "    # \"use_dense_cnn_lstm\": True,          # NOT IMPLEMENTED\n",
    "    # \"use_layerwise_maxpool\": False,      # NOT IMPLEMENTED\n",
    "    # \"pooling_layers\": [False],           # NOT IMPLEMENTED\n",
    "    # \"lstm_num_layers\": 0,                # NOT IMPLEMENTED\n",
    "    # \"lstm_hidden_size\": 0,               # NOT IMPLEMENTED\n",
    "    # \"lstm_dropout\": 0.0,                 # NOT IMPLEMENTED\n",
    "    # \"padding\": 0,                        # NOT USED\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7925a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_STR = basicMOE_config[\"model_str\"]\n",
    "MY_CONFIG = copy.deepcopy(basicMOE_config)\n",
    "\n",
    "DEVICE = basicMOE_config[\"device\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd7f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in data, with moments feature engineering!\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FOR_SPLITS = basicMOE_config\n",
    "\n",
    "# This is super repeated, consolidate and remove the unnecessary stuff from the other cells\n",
    "moments_expdef_df = load_expdef_gestures(feateng_method=\"moments\")\n",
    "moments_data_splits = make_data_split(moments_expdef_df, CONFIG_FOR_SPLITS, split_index=None)  \n",
    "test_participants = list(np.unique(moments_data_splits['novel_subject_test_dict']['participant_ids']))\n",
    "all_participants = np.unique(moments_data_splits['pretrain_dict']['participant_ids'] + moments_data_splits['pretrain_subject_test_dict']['participant_ids'] + test_participants)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_participants)\n",
    "train_df = process_split(moments_data_splits, 'pretrain_dict', label_encoder)\n",
    "intra_test_df = process_split(moments_data_splits, 'pretrain_subject_test_dict', label_encoder)\n",
    "moments_data_dfs_dict = {'pretrain_df':train_df, 'pretrain_subject_test_df':intra_test_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952000a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in data, with moments feature engineering!\n",
      "(3200, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "      <th>Gesture_Encoded</th>\n",
       "      <th>Cluster_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.1964091032249778, -0.3520191722373255, -0....</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0360169757802307, -0.5029028515788563, -0....</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.1001860983981701, -0.46837367292591, -0.56...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.060986345793039, -0.480572887435198, -0.66...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.0315063363429103, -0.5235507309772758, -0....</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID  Gesture_Num   \n",
       "0        P102        pan            1  \\\n",
       "1        P102        pan            2   \n",
       "2        P102        pan            3   \n",
       "3        P102        pan            4   \n",
       "4        P102        pan            5   \n",
       "\n",
       "                                             feature  Gesture_Encoded   \n",
       "0  [-0.1964091032249778, -0.3520191722373255, -0....                5  \\\n",
       "1  [-0.0360169757802307, -0.5029028515788563, -0....                5   \n",
       "2  [-0.1001860983981701, -0.46837367292591, -0.56...                5   \n",
       "3  [-0.060986345793039, -0.480572887435198, -0.66...                5   \n",
       "4  [-0.0315063363429103, -0.5235507309772758, -0....                5   \n",
       "\n",
       "   Cluster_ID  \n",
       "0           6  \n",
       "1           6  \n",
       "2           6  \n",
       "3           6  \n",
       "4           6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expdef_df = load_expdef_gestures(feateng_method=MY_CONFIG[\"feature_engr\"])\n",
    "\n",
    "print(expdef_df.shape)\n",
    "expdef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1234e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = make_data_split(expdef_df, MY_CONFIG, split_index=None)  \n",
    "# json splits are taken care of here!\n",
    "# So did I even need to load it earlier? ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6be7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pretrain_dict', 'pretrain_subject_test_dict', 'novel_trainFT_dict', 'novel_subject_test_dict', 'novel_val_trainFT_dict', 'novel_subject_val_test_dict'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c47522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bcc034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[map] 24 pretrain users -> contiguous [0..23]\n"
     ]
    }
   ],
   "source": [
    "# If these are arrays/lists (not sets), do this:\n",
    "train_ids = np.asarray(convert_participant_ids(data_splits['pretrain_dict']['participant_ids']))\n",
    "val_ids   = np.asarray(convert_participant_ids(data_splits['pretrain_subject_test_dict']['participant_ids']))\n",
    "\n",
    "# Unique union (no double counting)\n",
    "all_pretrain_ids = np.unique(np.concatenate([train_ids, val_ids]))\n",
    "# Ensure sorted & int type (nice-to-have)\n",
    "all_pretrain_ids = np.sort(all_pretrain_ids.astype(np.int64))\n",
    "\n",
    "# Build maps\n",
    "id2idx = {int(u): i for i, u in enumerate(all_pretrain_ids)}\n",
    "idx2id = all_pretrain_ids.tolist()\n",
    "\n",
    "num_train_users = len(idx2id)\n",
    "print(f\"[map] {num_train_users} pretrain users -> contiguous [0..{num_train_users-1}]\")\n",
    "\n",
    "encoded_train_ids = [id2idx[my_id] for my_id in train_ids]\n",
    "encoded_val_ids = [id2idx[my_id] for my_id in val_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0f371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_tensor_dataset(data_splits['pretrain_dict']['feature'], data_splits['pretrain_dict']['labels'], MY_CONFIG, participant_ids=encoded_train_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=MY_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "val_dataset = make_tensor_dataset(data_splits['pretrain_subject_test_dict']['feature'], data_splits['pretrain_subject_test_dict']['labels'], MY_CONFIG, participant_ids=encoded_val_ids)\n",
    "val_loader = DataLoader(val_dataset, batch_size=MY_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31fb3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_users = len(set(data_splits['pretrain_dict']['participant_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947511a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MoEClassifier(MY_CONFIG).to(MY_CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e8c790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this matches the embedding size you passed to MoEClassifier:\n",
    "#print(\"num_train_users =\", num_train_users)\n",
    "#\n",
    "#debug_one_batch(model, train_loader, device=DEVICE,\n",
    "#                num_classes=NUM_CLASSES, num_users_train=num_train_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d28ce",
   "metadata": {},
   "source": [
    "### Pretrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af0edbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch] avg_usage=[0.32899999618530273, 0.2770000100135803, 0.024000000208616257, 0.0010000000474974513, 0.06599999964237213, 0.30300000309944153]  entropy=1.357 (target≈1.792)  KL=1.099\n",
      "[epoch 001] train: loss=2.3813 acc=0.105 | val: loss=2.3236 acc=0.115\n",
      "[epoch] avg_usage=[0.36000001430511475, 0.2669999897480011, 0.020999999716877937, 0.0010000000474974513, 0.052000001072883606, 0.29899999499320984]  entropy=1.321 (target≈1.792)  KL=1.151\n",
      "[epoch 002] train: loss=2.3265 acc=0.137 | val: loss=2.2809 acc=0.133\n",
      "[epoch] avg_usage=[0.38100001215934753, 0.25999999046325684, 0.01899999938905239, 0.0010000000474974513, 0.04100000113248825, 0.296999990940094]  entropy=1.295 (target≈1.792)  KL=1.079\n",
      "[epoch 003] train: loss=2.2876 acc=0.161 | val: loss=2.2470 acc=0.152\n",
      "[epoch] avg_usage=[0.3889999985694885, 0.25699999928474426, 0.019999999552965164, 0.003000000026077032, 0.03400000184774399, 0.296999990940094]  entropy=1.288 (target≈1.792)  KL=0.980\n",
      "[epoch 004] train: loss=2.2472 acc=0.186 | val: loss=2.2174 acc=0.196\n",
      "[epoch] avg_usage=[0.38100001215934753, 0.2590000033378601, 0.019999999552965164, 0.004000000189989805, 0.032999999821186066, 0.3019999861717224]  entropy=1.295 (target≈1.792)  KL=0.918\n",
      "[epoch 005] train: loss=2.2218 acc=0.209 | val: loss=2.1876 acc=0.219\n",
      "[epoch] avg_usage=[0.3880000114440918, 0.24899999797344208, 0.023000000044703484, 0.003000000026077032, 0.032999999821186066, 0.30399999022483826]  entropy=1.294 (target≈1.792)  KL=0.943\n",
      "[epoch 006] train: loss=2.1885 acc=0.228 | val: loss=2.1629 acc=0.240\n",
      "[epoch] avg_usage=[0.3840000033378601, 0.23999999463558197, 0.02500000037252903, 0.009999999776482582, 0.03400000184774399, 0.3089999854564667]  entropy=1.323 (target≈1.792)  KL=0.759\n",
      "[epoch 007] train: loss=2.1592 acc=0.246 | val: loss=2.1332 acc=0.258\n",
      "[epoch] avg_usage=[0.38999998569488525, 0.23100000619888306, 0.027000000700354576, 0.014000000432133675, 0.03500000014901161, 0.30300000309944153]  entropy=1.343 (target≈1.792)  KL=0.678\n",
      "[epoch 008] train: loss=2.1262 acc=0.268 | val: loss=2.1123 acc=0.271\n",
      "[epoch] avg_usage=[0.3889999985694885, 0.23000000417232513, 0.029999999329447746, 0.01600000075995922, 0.03500000014901161, 0.3009999990463257]  entropy=1.353 (target≈1.792)  KL=0.649\n",
      "[epoch 009] train: loss=2.0988 acc=0.279 | val: loss=2.0858 acc=0.287\n",
      "[epoch] avg_usage=[0.375, 0.22100000083446503, 0.03400000184774399, 0.019999999552965164, 0.035999998450279236, 0.3140000104904175]  entropy=1.377 (target≈1.792)  KL=0.586\n",
      "[epoch 010] train: loss=2.0730 acc=0.295 | val: loss=2.0630 acc=0.306\n",
      "[epoch] avg_usage=[0.367000013589859, 0.22200000286102295, 0.039000000804662704, 0.023000000044703484, 0.03500000014901161, 0.3140000104904175]  entropy=1.396 (target≈1.792)  KL=0.549\n",
      "[epoch 011] train: loss=2.0463 acc=0.316 | val: loss=2.0337 acc=0.308\n",
      "[epoch] avg_usage=[0.36500000953674316, 0.21799999475479126, 0.04100000113248825, 0.024000000208616257, 0.03799999877810478, 0.3149999976158142]  entropy=1.406 (target≈1.792)  KL=0.526\n",
      "[epoch 012] train: loss=2.0214 acc=0.334 | val: loss=2.0080 acc=0.323\n",
      "[epoch] avg_usage=[0.3610000014305115, 0.210999995470047, 0.04600000008940697, 0.02500000037252903, 0.03799999877810478, 0.3199999928474426]  entropy=1.416 (target≈1.792)  KL=0.506\n",
      "[epoch 013] train: loss=1.9994 acc=0.343 | val: loss=1.9805 acc=0.331\n",
      "[epoch] avg_usage=[0.35899999737739563, 0.20100000500679016, 0.05000000074505806, 0.027000000700354576, 0.039000000804662704, 0.3240000009536743]  entropy=1.430 (target≈1.792)  KL=0.476\n",
      "[epoch 014] train: loss=1.9757 acc=0.354 | val: loss=1.9499 acc=0.348\n",
      "[epoch] avg_usage=[0.36000001430511475, 0.1979999989271164, 0.050999999046325684, 0.028999999165534973, 0.041999999433755875, 0.32100000977516174]  entropy=1.438 (target≈1.792)  KL=0.458\n",
      "[epoch 015] train: loss=1.9494 acc=0.370 | val: loss=1.9270 acc=0.354\n",
      "[epoch] avg_usage=[0.3529999852180481, 0.1899999976158142, 0.05299999937415123, 0.03200000151991844, 0.04100000113248825, 0.3310000002384186]  entropy=1.445 (target≈1.792)  KL=0.440\n",
      "[epoch 016] train: loss=1.9240 acc=0.396 | val: loss=1.9023 acc=0.375\n",
      "[epoch] avg_usage=[0.35600000619888306, 0.17800000309944153, 0.061000000685453415, 0.032999999821186066, 0.0430000014603138, 0.33000001311302185]  entropy=1.458 (target≈1.792)  KL=0.415\n",
      "[epoch 017] train: loss=1.8951 acc=0.396 | val: loss=1.8829 acc=0.383\n",
      "[epoch] avg_usage=[0.3529999852180481, 0.17800000309944153, 0.05999999865889549, 0.032999999821186066, 0.04500000178813934, 0.3310000002384186]  entropy=1.462 (target≈1.792)  KL=0.408\n",
      "[epoch 018] train: loss=1.8819 acc=0.412 | val: loss=1.8622 acc=0.408\n",
      "[epoch] avg_usage=[0.3490000069141388, 0.1720000058412552, 0.06499999761581421, 0.03500000014901161, 0.04600000008940697, 0.33399999141693115]  entropy=1.472 (target≈1.792)  KL=0.390\n",
      "[epoch 019] train: loss=1.8502 acc=0.428 | val: loss=1.8431 acc=0.431\n",
      "[epoch] avg_usage=[0.3440000116825104, 0.15600000321865082, 0.07800000160932541, 0.035999998450279236, 0.04600000008940697, 0.33899998664855957]  entropy=1.485 (target≈1.792)  KL=0.367\n",
      "[epoch 020] train: loss=1.8294 acc=0.446 | val: loss=1.8250 acc=0.435\n",
      "[epoch] avg_usage=[0.3409999907016754, 0.1459999978542328, 0.08399999886751175, 0.03700000047683716, 0.04600000008940697, 0.34599998593330383]  entropy=1.487 (target≈1.792)  KL=0.360\n",
      "[epoch 021] train: loss=1.8124 acc=0.457 | val: loss=1.7969 acc=0.435\n",
      "[epoch] avg_usage=[0.33500000834465027, 0.14900000393390656, 0.08399999886751175, 0.035999998450279236, 0.04800000041723251, 0.34700000286102295]  entropy=1.493 (target≈1.792)  KL=0.353\n",
      "[epoch 022] train: loss=1.7835 acc=0.460 | val: loss=1.7788 acc=0.440\n",
      "[epoch] avg_usage=[0.335999995470047, 0.14300000667572021, 0.0860000029206276, 0.035999998450279236, 0.05000000074505806, 0.3499999940395355]  entropy=1.492 (target≈1.792)  KL=0.352\n",
      "[epoch 023] train: loss=1.7662 acc=0.467 | val: loss=1.7520 acc=0.452\n",
      "[epoch] avg_usage=[0.33899998664855957, 0.14100000262260437, 0.08699999749660492, 0.03700000047683716, 0.050999999046325684, 0.3449999988079071]  entropy=1.496 (target≈1.792)  KL=0.345\n",
      "[epoch 024] train: loss=1.7458 acc=0.484 | val: loss=1.7301 acc=0.456\n",
      "[epoch] avg_usage=[0.3319999873638153, 0.13300000131130219, 0.09300000220537186, 0.03700000047683716, 0.04899999871850014, 0.35600000619888306]  entropy=1.493 (target≈1.792)  KL=0.348\n",
      "[epoch 025] train: loss=1.7225 acc=0.492 | val: loss=1.7072 acc=0.456\n",
      "[epoch] avg_usage=[0.3370000123977661, 0.13199999928474426, 0.08799999952316284, 0.03799999877810478, 0.050999999046325684, 0.35499998927116394]  entropy=1.490 (target≈1.792)  KL=0.348\n",
      "[epoch 026] train: loss=1.7010 acc=0.492 | val: loss=1.6865 acc=0.450\n",
      "[epoch] avg_usage=[0.34599998593330383, 0.12999999523162842, 0.09000000357627869, 0.03799999877810478, 0.05000000074505806, 0.3449999988079071]  entropy=1.491 (target≈1.792)  KL=0.347\n",
      "[epoch 027] train: loss=1.6812 acc=0.513 | val: loss=1.6666 acc=0.473\n",
      "[epoch] avg_usage=[0.3310000002384186, 0.1289999932050705, 0.09399999678134918, 0.03999999910593033, 0.04899999871850014, 0.35600000619888306]  entropy=1.497 (target≈1.792)  KL=0.338\n",
      "[epoch 028] train: loss=1.6615 acc=0.520 | val: loss=1.6401 acc=0.481\n",
      "[epoch] avg_usage=[0.33500000834465027, 0.12700000405311584, 0.09600000083446503, 0.03799999877810478, 0.05000000074505806, 0.35499998927116394]  entropy=1.494 (target≈1.792)  KL=0.344\n",
      "[epoch 029] train: loss=1.6437 acc=0.519 | val: loss=1.6262 acc=0.485\n",
      "[epoch] avg_usage=[0.3370000123977661, 0.12600000202655792, 0.09300000220537186, 0.03999999910593033, 0.052000001072883606, 0.3529999852180481]  entropy=1.498 (target≈1.792)  KL=0.334\n",
      "[epoch 030] train: loss=1.6255 acc=0.529 | val: loss=1.6115 acc=0.487\n",
      "[epoch] avg_usage=[0.33899998664855957, 0.12600000202655792, 0.0949999988079071, 0.03999999910593033, 0.052000001072883606, 0.3490000069141388]  entropy=1.500 (target≈1.792)  KL=0.333\n",
      "[epoch 031] train: loss=1.6046 acc=0.547 | val: loss=1.5823 acc=0.506\n",
      "[epoch] avg_usage=[0.335999995470047, 0.12399999797344208, 0.09200000017881393, 0.04100000113248825, 0.05299999937415123, 0.35499998927116394]  entropy=1.497 (target≈1.792)  KL=0.334\n",
      "[epoch 032] train: loss=1.5837 acc=0.549 | val: loss=1.5661 acc=0.504\n",
      "[epoch] avg_usage=[0.3449999988079071, 0.12399999797344208, 0.09799999743700027, 0.03999999910593033, 0.050999999046325684, 0.34200000762939453]  entropy=1.501 (target≈1.792)  KL=0.333\n",
      "[epoch 033] train: loss=1.5664 acc=0.560 | val: loss=1.5519 acc=0.510\n",
      "[epoch] avg_usage=[0.3310000002384186, 0.12300000339746475, 0.09700000286102295, 0.03999999910593033, 0.05400000140070915, 0.35499998927116394]  entropy=1.504 (target≈1.792)  KL=0.327\n",
      "[epoch 034] train: loss=1.5483 acc=0.574 | val: loss=1.5309 acc=0.531\n",
      "[epoch] avg_usage=[0.3370000123977661, 0.12099999934434891, 0.0949999988079071, 0.04100000113248825, 0.054999999701976776, 0.3499999940395355]  entropy=1.505 (target≈1.792)  KL=0.322\n",
      "[epoch 035] train: loss=1.5229 acc=0.583 | val: loss=1.5117 acc=0.531\n",
      "[epoch] avg_usage=[0.335999995470047, 0.12200000137090683, 0.0989999994635582, 0.04100000113248825, 0.0560000017285347, 0.34599998593330383]  entropy=1.511 (target≈1.792)  KL=0.317\n",
      "[epoch 036] train: loss=1.5012 acc=0.595 | val: loss=1.4928 acc=0.542\n",
      "[epoch] avg_usage=[0.3330000042915344, 0.12600000202655792, 0.0949999988079071, 0.041999999433755875, 0.0560000017285347, 0.3479999899864197]  entropy=1.513 (target≈1.792)  KL=0.313\n",
      "[epoch 037] train: loss=1.4865 acc=0.607 | val: loss=1.4815 acc=0.544\n",
      "[epoch] avg_usage=[0.33399999141693115, 0.12099999934434891, 0.10100000351667404, 0.04100000113248825, 0.057999998331069946, 0.34599998593330383]  entropy=1.516 (target≈1.792)  KL=0.309\n",
      "[epoch 038] train: loss=1.4694 acc=0.621 | val: loss=1.4629 acc=0.542\n",
      "[epoch] avg_usage=[0.335999995470047, 0.12200000137090683, 0.0989999994635582, 0.041999999433755875, 0.05900000035762787, 0.34200000762939453]  entropy=1.519 (target≈1.792)  KL=0.305\n",
      "[epoch 039] train: loss=1.4522 acc=0.617 | val: loss=1.4478 acc=0.552\n",
      "[epoch] avg_usage=[0.33000001311302185, 0.125, 0.09700000286102295, 0.0430000014603138, 0.05900000035762787, 0.3449999988079071]  entropy=1.523 (target≈1.792)  KL=0.300\n",
      "[epoch 040] train: loss=1.4300 acc=0.631 | val: loss=1.4292 acc=0.575\n",
      "[epoch] avg_usage=[0.33000001311302185, 0.12300000339746475, 0.10000000149011612, 0.0430000014603138, 0.06199999898672104, 0.34200000762939453]  entropy=1.529 (target≈1.792)  KL=0.293\n",
      "[epoch 041] train: loss=1.4205 acc=0.639 | val: loss=1.4138 acc=0.581\n",
      "[epoch] avg_usage=[0.328000009059906, 0.12399999797344208, 0.10000000149011612, 0.0430000014603138, 0.06199999898672104, 0.34299999475479126]  entropy=1.529 (target≈1.792)  KL=0.292\n",
      "[epoch 042] train: loss=1.3992 acc=0.646 | val: loss=1.3943 acc=0.573\n",
      "[epoch] avg_usage=[0.3230000138282776, 0.12399999797344208, 0.10300000011920929, 0.041999999433755875, 0.06499999761581421, 0.3440000116825104]  entropy=1.534 (target≈1.792)  KL=0.288\n",
      "[epoch 043] train: loss=1.3779 acc=0.660 | val: loss=1.3742 acc=0.585\n",
      "[epoch] avg_usage=[0.3319999873638153, 0.12200000137090683, 0.10000000149011612, 0.04399999976158142, 0.06400000303983688, 0.33799999952316284]  entropy=1.533 (target≈1.792)  KL=0.287\n",
      "[epoch 044] train: loss=1.3670 acc=0.648 | val: loss=1.3570 acc=0.602\n",
      "[epoch] avg_usage=[0.32100000977516174, 0.12300000339746475, 0.0989999994635582, 0.04399999976158142, 0.06599999964237213, 0.34700000286102295]  entropy=1.536 (target≈1.792)  KL=0.282\n",
      "[epoch 045] train: loss=1.3487 acc=0.661 | val: loss=1.3400 acc=0.606\n",
      "[epoch] avg_usage=[0.32100000977516174, 0.12600000202655792, 0.10300000011920929, 0.04500000178813934, 0.06499999761581421, 0.3400000035762787]  entropy=1.543 (target≈1.792)  KL=0.276\n",
      "[epoch 046] train: loss=1.3348 acc=0.666 | val: loss=1.3244 acc=0.608\n",
      "[epoch] avg_usage=[0.3230000138282776, 0.12700000405311584, 0.10000000149011612, 0.04399999976158142, 0.06599999964237213, 0.3409999907016754]  entropy=1.539 (target≈1.792)  KL=0.280\n",
      "[epoch 047] train: loss=1.3145 acc=0.679 | val: loss=1.3083 acc=0.610\n",
      "[epoch] avg_usage=[0.3190000057220459, 0.125, 0.10100000351667404, 0.04399999976158142, 0.06800000369548798, 0.34299999475479126]  entropy=1.542 (target≈1.792)  KL=0.276\n",
      "[epoch 048] train: loss=1.2992 acc=0.688 | val: loss=1.2948 acc=0.619\n",
      "[epoch] avg_usage=[0.3179999887943268, 0.12600000202655792, 0.10300000011920929, 0.04500000178813934, 0.06700000166893005, 0.34200000762939453]  entropy=1.546 (target≈1.792)  KL=0.272\n",
      "[epoch 049] train: loss=1.2854 acc=0.682 | val: loss=1.2767 acc=0.629\n",
      "[epoch] avg_usage=[0.3199999928474426, 0.12700000405311584, 0.10199999809265137, 0.04500000178813934, 0.06700000166893005, 0.33899998664855957]  entropy=1.547 (target≈1.792)  KL=0.271\n",
      "[epoch 050] train: loss=1.2639 acc=0.703 | val: loss=1.2643 acc=0.644\n",
      "[epoch] avg_usage=[0.3140000104904175, 0.1289999932050705, 0.10000000149011612, 0.04500000178813934, 0.0689999982714653, 0.34299999475479126]  entropy=1.549 (target≈1.792)  KL=0.268\n",
      "[epoch 051] train: loss=1.2567 acc=0.697 | val: loss=1.2505 acc=0.648\n",
      "[epoch] avg_usage=[0.3190000057220459, 0.12999999523162842, 0.10100000351667404, 0.04500000178813934, 0.06800000369548798, 0.335999995470047]  entropy=1.552 (target≈1.792)  KL=0.265\n",
      "[epoch 052] train: loss=1.2362 acc=0.712 | val: loss=1.2360 acc=0.642\n",
      "[epoch] avg_usage=[0.3089999854564667, 0.1289999932050705, 0.10100000351667404, 0.04500000178813934, 0.07199999690055847, 0.3440000116825104]  entropy=1.555 (target≈1.792)  KL=0.261\n",
      "[epoch 053] train: loss=1.2216 acc=0.707 | val: loss=1.2197 acc=0.675\n",
      "[epoch] avg_usage=[0.31700000166893005, 0.12999999523162842, 0.10100000351667404, 0.04600000008940697, 0.0689999982714653, 0.335999995470047]  entropy=1.555 (target≈1.792)  KL=0.260\n",
      "[epoch 054] train: loss=1.2031 acc=0.720 | val: loss=1.2110 acc=0.667\n",
      "[epoch] avg_usage=[0.3160000145435333, 0.1289999932050705, 0.10199999809265137, 0.04600000008940697, 0.07000000029802322, 0.3370000123977661]  entropy=1.555 (target≈1.792)  KL=0.260\n",
      "[epoch 055] train: loss=1.1957 acc=0.738 | val: loss=1.1947 acc=0.669\n",
      "[epoch] avg_usage=[0.30799999833106995, 0.12999999523162842, 0.10100000351667404, 0.04699999839067459, 0.07199999690055847, 0.3409999907016754]  entropy=1.561 (target≈1.792)  KL=0.252\n",
      "[epoch 056] train: loss=1.1751 acc=0.732 | val: loss=1.1789 acc=0.677\n",
      "[epoch] avg_usage=[0.30799999833106995, 0.12999999523162842, 0.10000000149011612, 0.04699999839067459, 0.0729999989271164, 0.3409999907016754]  entropy=1.561 (target≈1.792)  KL=0.252\n",
      "[epoch 057] train: loss=1.1630 acc=0.736 | val: loss=1.1676 acc=0.675\n",
      "[epoch] avg_usage=[0.30799999833106995, 0.13199999928474426, 0.10199999809265137, 0.04500000178813934, 0.07199999690055847, 0.3400000035762787]  entropy=1.560 (target≈1.792)  KL=0.256\n",
      "[epoch 058] train: loss=1.1520 acc=0.745 | val: loss=1.1498 acc=0.683\n",
      "[epoch] avg_usage=[0.3089999854564667, 0.1289999932050705, 0.10199999809265137, 0.04699999839067459, 0.07400000095367432, 0.33799999952316284]  entropy=1.564 (target≈1.792)  KL=0.249\n",
      "[epoch 059] train: loss=1.1330 acc=0.753 | val: loss=1.1435 acc=0.690\n",
      "[epoch] avg_usage=[0.3089999854564667, 0.13199999928474426, 0.10300000011920929, 0.04699999839067459, 0.07500000298023224, 0.33399999141693115]  entropy=1.569 (target≈1.792)  KL=0.244\n",
      "[epoch 060] train: loss=1.1185 acc=0.753 | val: loss=1.1226 acc=0.702\n",
      "[epoch] avg_usage=[0.3100000023841858, 0.13300000131130219, 0.10100000351667404, 0.04800000041723251, 0.07500000298023224, 0.33399999141693115]  entropy=1.568 (target≈1.792)  KL=0.244\n",
      "[epoch 061] train: loss=1.1086 acc=0.755 | val: loss=1.1112 acc=0.706\n",
      "[epoch] avg_usage=[0.3019999861717224, 0.13199999928474426, 0.10300000011920929, 0.04699999839067459, 0.07599999755620956, 0.3400000035762787]  entropy=1.569 (target≈1.792)  KL=0.244\n",
      "[epoch 062] train: loss=1.0897 acc=0.768 | val: loss=1.0996 acc=0.702\n",
      "[epoch] avg_usage=[0.30799999833106995, 0.1340000033378601, 0.10000000149011612, 0.04800000041723251, 0.07599999755620956, 0.3330000042915344]  entropy=1.571 (target≈1.792)  KL=0.241\n",
      "[epoch 063] train: loss=1.0806 acc=0.762 | val: loss=1.0896 acc=0.708\n",
      "[epoch] avg_usage=[0.2980000078678131, 0.13500000536441803, 0.10100000351667404, 0.04699999839067459, 0.07800000160932541, 0.3409999907016754]  entropy=1.573 (target≈1.792)  KL=0.239\n",
      "[epoch 064] train: loss=1.0785 acc=0.766 | val: loss=1.0727 acc=0.719\n",
      "[epoch] avg_usage=[0.30399999022483826, 0.13600000739097595, 0.10100000351667404, 0.04699999839067459, 0.07800000160932541, 0.3330000042915344]  entropy=1.575 (target≈1.792)  KL=0.237\n",
      "[epoch 065] train: loss=1.0523 acc=0.781 | val: loss=1.0625 acc=0.710\n",
      "[epoch] avg_usage=[0.3009999990463257, 0.13500000536441803, 0.10000000149011612, 0.04800000041723251, 0.07900000363588333, 0.33799999952316284]  entropy=1.573 (target≈1.792)  KL=0.238\n",
      "[epoch 066] train: loss=1.0494 acc=0.776 | val: loss=1.0511 acc=0.721\n",
      "[epoch] avg_usage=[0.30000001192092896, 0.13899999856948853, 0.10000000149011612, 0.04800000041723251, 0.07800000160932541, 0.335999995470047]  entropy=1.576 (target≈1.792)  KL=0.236\n",
      "[epoch 067] train: loss=1.0272 acc=0.794 | val: loss=1.0383 acc=0.721\n",
      "[epoch] avg_usage=[0.3059999942779541, 0.13699999451637268, 0.10100000351667404, 0.04800000041723251, 0.07800000160932541, 0.33000001311302185]  entropy=1.577 (target≈1.792)  KL=0.235\n",
      "[epoch 068] train: loss=1.0237 acc=0.792 | val: loss=1.0290 acc=0.721\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.13899999856948853, 0.0989999994635582, 0.04899999871850014, 0.07999999821186066, 0.3370000123977661]  entropy=1.580 (target≈1.792)  KL=0.231\n",
      "[epoch 069] train: loss=1.0065 acc=0.799 | val: loss=1.0166 acc=0.727\n",
      "[epoch] avg_usage=[0.3019999861717224, 0.13899999856948853, 0.0989999994635582, 0.04899999871850014, 0.07800000160932541, 0.3319999873638153]  entropy=1.579 (target≈1.792)  KL=0.232\n",
      "[epoch 070] train: loss=1.0032 acc=0.801 | val: loss=1.0032 acc=0.740\n",
      "[epoch] avg_usage=[0.29499998688697815, 0.13899999856948853, 0.0989999994635582, 0.04800000041723251, 0.07999999821186066, 0.33899998664855957]  entropy=1.578 (target≈1.792)  KL=0.233\n",
      "[epoch 071] train: loss=0.9817 acc=0.815 | val: loss=0.9901 acc=0.744\n",
      "[epoch] avg_usage=[0.3009999990463257, 0.14000000059604645, 0.10000000149011612, 0.04899999871850014, 0.07999999821186066, 0.33000001311302185]  entropy=1.582 (target≈1.792)  KL=0.229\n",
      "[epoch 072] train: loss=0.9755 acc=0.813 | val: loss=0.9851 acc=0.738\n",
      "[epoch] avg_usage=[0.2980000078678131, 0.14100000262260437, 0.09799999743700027, 0.04800000041723251, 0.08100000023841858, 0.33500000834465027]  entropy=1.580 (target≈1.792)  KL=0.231\n",
      "[epoch 073] train: loss=0.9582 acc=0.821 | val: loss=0.9679 acc=0.760\n",
      "[epoch] avg_usage=[0.30000001192092896, 0.14000000059604645, 0.10000000149011612, 0.04899999871850014, 0.07999999821186066, 0.3310000002384186]  entropy=1.583 (target≈1.792)  KL=0.227\n",
      "[epoch 074] train: loss=0.9517 acc=0.810 | val: loss=0.9580 acc=0.754\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.14100000262260437, 0.0989999994635582, 0.04800000041723251, 0.08100000023841858, 0.33500000834465027]  entropy=1.581 (target≈1.792)  KL=0.230\n",
      "[epoch 075] train: loss=0.9386 acc=0.822 | val: loss=0.9488 acc=0.748\n",
      "[epoch] avg_usage=[0.29499998688697815, 0.14100000262260437, 0.10000000149011612, 0.04899999871850014, 0.0820000022649765, 0.3330000042915344]  entropy=1.587 (target≈1.792)  KL=0.224\n",
      "[epoch 076] train: loss=0.9290 acc=0.829 | val: loss=0.9391 acc=0.758\n",
      "[epoch] avg_usage=[0.2980000078678131, 0.14300000667572021, 0.0989999994635582, 0.04800000041723251, 0.08100000023841858, 0.3310000002384186]  entropy=1.583 (target≈1.792)  KL=0.229\n",
      "[epoch 077] train: loss=0.9150 acc=0.830 | val: loss=0.9306 acc=0.765\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.14300000667572021, 0.09799999743700027, 0.04899999871850014, 0.0820000022649765, 0.3319999873638153]  entropy=1.584 (target≈1.792)  KL=0.226\n",
      "[epoch 078] train: loss=0.9075 acc=0.834 | val: loss=0.9205 acc=0.758\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.14399999380111694, 0.0989999994635582, 0.04800000041723251, 0.08299999684095383, 0.3310000002384186]  entropy=1.585 (target≈1.792)  KL=0.227\n",
      "[epoch 079] train: loss=0.8950 acc=0.839 | val: loss=0.9135 acc=0.767\n",
      "[epoch] avg_usage=[0.2919999957084656, 0.14300000667572021, 0.09799999743700027, 0.04899999871850014, 0.08399999886751175, 0.33500000834465027]  entropy=1.587 (target≈1.792)  KL=0.223\n",
      "[epoch 080] train: loss=0.8837 acc=0.847 | val: loss=0.9020 acc=0.767\n",
      "[epoch] avg_usage=[0.2980000078678131, 0.14300000667572021, 0.10000000149011612, 0.04899999871850014, 0.0820000022649765, 0.3269999921321869]  entropy=1.588 (target≈1.792)  KL=0.224\n",
      "[epoch 081] train: loss=0.8748 acc=0.849 | val: loss=0.8921 acc=0.773\n",
      "[epoch] avg_usage=[0.296999990940094, 0.14499999582767487, 0.09700000286102295, 0.04800000041723251, 0.0820000022649765, 0.3310000002384186]  entropy=1.584 (target≈1.792)  KL=0.228\n",
      "[epoch 082] train: loss=0.8730 acc=0.842 | val: loss=0.8848 acc=0.767\n",
      "[epoch] avg_usage=[0.2939999997615814, 0.14300000667572021, 0.09700000286102295, 0.04800000041723251, 0.08399999886751175, 0.33399999141693115]  entropy=1.584 (target≈1.792)  KL=0.227\n",
      "[epoch 083] train: loss=0.8583 acc=0.859 | val: loss=0.8777 acc=0.781\n",
      "[epoch] avg_usage=[0.2939999997615814, 0.14499999582767487, 0.09799999743700027, 0.04800000041723251, 0.08399999886751175, 0.3319999873638153]  entropy=1.587 (target≈1.792)  KL=0.224\n",
      "[epoch 084] train: loss=0.8489 acc=0.847 | val: loss=0.8684 acc=0.779\n",
      "[epoch] avg_usage=[0.2939999997615814, 0.14499999582767487, 0.09700000286102295, 0.04899999871850014, 0.08299999684095383, 0.3319999873638153]  entropy=1.587 (target≈1.792)  KL=0.224\n",
      "[epoch 085] train: loss=0.8393 acc=0.860 | val: loss=0.8638 acc=0.767\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.14399999380111694, 0.09799999743700027, 0.04800000041723251, 0.0820000022649765, 0.3310000002384186]  entropy=1.585 (target≈1.792)  KL=0.227\n",
      "[epoch 086] train: loss=0.8324 acc=0.858 | val: loss=0.8524 acc=0.783\n",
      "[epoch] avg_usage=[0.29100000858306885, 0.1469999998807907, 0.09700000286102295, 0.04800000041723251, 0.08399999886751175, 0.3319999873638153]  entropy=1.588 (target≈1.792)  KL=0.224\n",
      "[epoch 087] train: loss=0.8192 acc=0.861 | val: loss=0.8457 acc=0.783\n",
      "[epoch] avg_usage=[0.2939999997615814, 0.14399999380111694, 0.0989999994635582, 0.04800000041723251, 0.08500000089406967, 0.33000001311302185]  entropy=1.588 (target≈1.792)  KL=0.224\n",
      "[epoch 088] train: loss=0.8165 acc=0.863 | val: loss=0.8403 acc=0.781\n",
      "[epoch] avg_usage=[0.29600000381469727, 0.14800000190734863, 0.09700000286102295, 0.04899999871850014, 0.08399999886751175, 0.32600000500679016]  entropy=1.590 (target≈1.792)  KL=0.221\n",
      "[epoch 089] train: loss=0.8099 acc=0.872 | val: loss=0.8292 acc=0.787\n",
      "[epoch] avg_usage=[0.29100000858306885, 0.14499999582767487, 0.09799999743700027, 0.04800000041723251, 0.08500000089406967, 0.3330000042915344]  entropy=1.588 (target≈1.792)  KL=0.223\n",
      "[epoch 090] train: loss=0.7990 acc=0.874 | val: loss=0.8211 acc=0.785\n",
      "[epoch] avg_usage=[0.2939999997615814, 0.1469999998807907, 0.09700000286102295, 0.04899999871850014, 0.08299999684095383, 0.33000001311302185]  entropy=1.589 (target≈1.792)  KL=0.222\n",
      "[epoch 091] train: loss=0.7915 acc=0.874 | val: loss=0.8151 acc=0.783\n",
      "[epoch] avg_usage=[0.2930000126361847, 0.1459999978542328, 0.09799999743700027, 0.04899999871850014, 0.0860000029206276, 0.32899999618530273]  entropy=1.592 (target≈1.792)  KL=0.219\n",
      "[epoch 092] train: loss=0.7803 acc=0.877 | val: loss=0.8081 acc=0.785\n",
      "[epoch] avg_usage=[0.29499998688697815, 0.1469999998807907, 0.09799999743700027, 0.04800000041723251, 0.08500000089406967, 0.3269999921321869]  entropy=1.590 (target≈1.792)  KL=0.222\n",
      "[epoch 093] train: loss=0.7710 acc=0.881 | val: loss=0.7986 acc=0.796\n",
      "[epoch] avg_usage=[0.29100000858306885, 0.14800000190734863, 0.09799999743700027, 0.04899999871850014, 0.08399999886751175, 0.33000001311302185]  entropy=1.590 (target≈1.792)  KL=0.221\n",
      "[epoch 094] train: loss=0.7669 acc=0.880 | val: loss=0.7931 acc=0.787\n",
      "[epoch] avg_usage=[0.2919999957084656, 0.14900000393390656, 0.09600000083446503, 0.04899999871850014, 0.08500000089406967, 0.32899999618530273]  entropy=1.591 (target≈1.792)  KL=0.221\n",
      "[epoch 095] train: loss=0.7621 acc=0.883 | val: loss=0.7881 acc=0.790\n",
      "[epoch] avg_usage=[0.29499998688697815, 0.1459999978542328, 0.09799999743700027, 0.04899999871850014, 0.0860000029206276, 0.32600000500679016]  entropy=1.592 (target≈1.792)  KL=0.219\n",
      "[epoch 096] train: loss=0.7519 acc=0.888 | val: loss=0.7815 acc=0.800\n",
      "[epoch] avg_usage=[0.289000004529953, 0.14900000393390656, 0.09700000286102295, 0.04800000041723251, 0.08699999749660492, 0.3310000002384186]  entropy=1.592 (target≈1.792)  KL=0.220\n",
      "[epoch 097] train: loss=0.7442 acc=0.891 | val: loss=0.7769 acc=0.794\n",
      "[epoch] avg_usage=[0.29100000858306885, 0.1469999998807907, 0.09799999743700027, 0.04800000041723251, 0.08699999749660492, 0.32899999618530273]  entropy=1.594 (target≈1.792)  KL=0.218\n",
      "[epoch 098] train: loss=0.7344 acc=0.894 | val: loss=0.7660 acc=0.798\n",
      "[epoch] avg_usage=[0.2919999957084656, 0.14800000190734863, 0.09799999743700027, 0.04800000041723251, 0.08699999749660492, 0.3269999921321869]  entropy=1.593 (target≈1.792)  KL=0.218\n",
      "[epoch 099] train: loss=0.7284 acc=0.894 | val: loss=0.7626 acc=0.798\n",
      "[epoch] avg_usage=[0.29100000858306885, 0.14800000190734863, 0.09700000286102295, 0.04800000041723251, 0.08699999749660492, 0.32899999618530273]  entropy=1.592 (target≈1.792)  KL=0.219\n",
      "[epoch 100] train: loss=0.7214 acc=0.895 | val: loss=0.7590 acc=0.792\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14900000393390656, 0.09799999743700027, 0.04899999871850014, 0.08900000154972076, 0.33000001311302185]  entropy=1.596 (target≈1.792)  KL=0.214\n",
      "[epoch 101] train: loss=0.7173 acc=0.896 | val: loss=0.7525 acc=0.796\n",
      "[epoch] avg_usage=[0.28999999165534973, 0.14800000190734863, 0.09700000286102295, 0.04899999871850014, 0.08699999749660492, 0.33000001311302185]  entropy=1.593 (target≈1.792)  KL=0.218\n",
      "[epoch 102] train: loss=0.7168 acc=0.896 | val: loss=0.7457 acc=0.798\n",
      "[epoch] avg_usage=[0.28999999165534973, 0.14900000393390656, 0.09700000286102295, 0.04899999871850014, 0.08799999952316284, 0.328000009059906]  entropy=1.595 (target≈1.792)  KL=0.216\n",
      "[epoch 103] train: loss=0.7097 acc=0.898 | val: loss=0.7401 acc=0.802\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.14900000393390656, 0.09799999743700027, 0.04899999871850014, 0.08799999952316284, 0.32899999618530273]  entropy=1.597 (target≈1.792)  KL=0.214\n",
      "[epoch 104] train: loss=0.6956 acc=0.906 | val: loss=0.7314 acc=0.802\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.14800000190734863, 0.09700000286102295, 0.04899999871850014, 0.08799999952316284, 0.3319999873638153]  entropy=1.595 (target≈1.792)  KL=0.215\n",
      "[epoch 105] train: loss=0.6952 acc=0.904 | val: loss=0.7266 acc=0.796\n",
      "[epoch] avg_usage=[0.2879999876022339, 0.14800000190734863, 0.09700000286102295, 0.04899999871850014, 0.08699999749660492, 0.33000001311302185]  entropy=1.594 (target≈1.792)  KL=0.216\n",
      "[epoch 106] train: loss=0.6829 acc=0.911 | val: loss=0.7220 acc=0.796\n",
      "[epoch] avg_usage=[0.289000004529953, 0.14900000393390656, 0.09700000286102295, 0.05000000074505806, 0.08799999952316284, 0.32899999618530273]  entropy=1.596 (target≈1.792)  KL=0.214\n",
      "[epoch 107] train: loss=0.6778 acc=0.909 | val: loss=0.7156 acc=0.802\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.14900000393390656, 0.0989999994635582, 0.04899999871850014, 0.08799999952316284, 0.328000009059906]  entropy=1.599 (target≈1.792)  KL=0.212\n",
      "[epoch 108] train: loss=0.6714 acc=0.912 | val: loss=0.7124 acc=0.806\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14900000393390656, 0.09799999743700027, 0.05000000074505806, 0.08699999749660492, 0.33000001311302185]  entropy=1.597 (target≈1.792)  KL=0.213\n",
      "[epoch 109] train: loss=0.6678 acc=0.908 | val: loss=0.7067 acc=0.802\n",
      "[epoch] avg_usage=[0.2879999876022339, 0.1469999998807907, 0.09799999743700027, 0.04899999871850014, 0.09000000357627869, 0.3269999921321869]  entropy=1.599 (target≈1.792)  KL=0.211\n",
      "[epoch 110] train: loss=0.6607 acc=0.912 | val: loss=0.7008 acc=0.800\n",
      "[epoch] avg_usage=[0.289000004529953, 0.14900000393390656, 0.09799999743700027, 0.05000000074505806, 0.08799999952316284, 0.3269999921321869]  entropy=1.598 (target≈1.792)  KL=0.212\n",
      "[epoch 111] train: loss=0.6536 acc=0.915 | val: loss=0.6926 acc=0.808\n",
      "[epoch] avg_usage=[0.289000004529953, 0.14900000393390656, 0.09700000286102295, 0.04899999871850014, 0.09000000357627869, 0.32499998807907104]  entropy=1.600 (target≈1.792)  KL=0.210\n",
      "[epoch 112] train: loss=0.6457 acc=0.917 | val: loss=0.6894 acc=0.808\n",
      "[epoch] avg_usage=[0.2879999876022339, 0.15000000596046448, 0.0989999994635582, 0.05000000074505806, 0.08900000154972076, 0.32499998807907104]  entropy=1.601 (target≈1.792)  KL=0.209\n",
      "[epoch 113] train: loss=0.6483 acc=0.919 | val: loss=0.6853 acc=0.815\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.14900000393390656, 0.0989999994635582, 0.05000000074505806, 0.08900000154972076, 0.32899999618530273]  entropy=1.600 (target≈1.792)  KL=0.210\n",
      "[epoch 114] train: loss=0.6431 acc=0.917 | val: loss=0.6772 acc=0.815\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.14900000393390656, 0.09600000083446503, 0.05000000074505806, 0.08900000154972076, 0.328000009059906]  entropy=1.599 (target≈1.792)  KL=0.211\n",
      "[epoch 115] train: loss=0.6309 acc=0.917 | val: loss=0.6767 acc=0.812\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14800000190734863, 0.09799999743700027, 0.05000000074505806, 0.09099999815225601, 0.32600000500679016]  entropy=1.603 (target≈1.792)  KL=0.207\n",
      "[epoch 116] train: loss=0.6310 acc=0.922 | val: loss=0.6673 acc=0.823\n",
      "[epoch] avg_usage=[0.2840000092983246, 0.14800000190734863, 0.09799999743700027, 0.05000000074505806, 0.09000000357627869, 0.3310000002384186]  entropy=1.600 (target≈1.792)  KL=0.209\n",
      "[epoch 117] train: loss=0.6234 acc=0.926 | val: loss=0.6643 acc=0.823\n",
      "[epoch] avg_usage=[0.28999999165534973, 0.15000000596046448, 0.09700000286102295, 0.05000000074505806, 0.08799999952316284, 0.3240000009536743]  entropy=1.599 (target≈1.792)  KL=0.211\n",
      "[epoch 118] train: loss=0.6201 acc=0.926 | val: loss=0.6589 acc=0.829\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.14900000393390656, 0.0989999994635582, 0.05000000074505806, 0.09000000357627869, 0.3310000002384186]  entropy=1.602 (target≈1.792)  KL=0.207\n",
      "[epoch 119] train: loss=0.6169 acc=0.930 | val: loss=0.6540 acc=0.829\n",
      "[epoch] avg_usage=[0.2879999876022339, 0.14900000393390656, 0.09799999743700027, 0.050999999046325684, 0.09000000357627869, 0.32499998807907104]  entropy=1.602 (target≈1.792)  KL=0.207\n",
      "[epoch 120] train: loss=0.6066 acc=0.932 | val: loss=0.6504 acc=0.829\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14800000190734863, 0.09799999743700027, 0.05000000074505806, 0.09000000357627869, 0.328000009059906]  entropy=1.601 (target≈1.792)  KL=0.209\n",
      "[epoch 121] train: loss=0.6070 acc=0.929 | val: loss=0.6487 acc=0.831\n",
      "[epoch] avg_usage=[0.2879999876022339, 0.14800000190734863, 0.09799999743700027, 0.05000000074505806, 0.09099999815225601, 0.3240000009536743]  entropy=1.603 (target≈1.792)  KL=0.206\n",
      "[epoch 122] train: loss=0.6001 acc=0.934 | val: loss=0.6444 acc=0.829\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.14900000393390656, 0.09799999743700027, 0.05000000074505806, 0.09200000017881393, 0.3269999921321869]  entropy=1.604 (target≈1.792)  KL=0.205\n",
      "[epoch 123] train: loss=0.5920 acc=0.939 | val: loss=0.6390 acc=0.835\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14800000190734863, 0.09799999743700027, 0.05000000074505806, 0.09099999815225601, 0.3269999921321869]  entropy=1.603 (target≈1.792)  KL=0.206\n",
      "[epoch 124] train: loss=0.5896 acc=0.933 | val: loss=0.6336 acc=0.831\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.15000000596046448, 0.0989999994635582, 0.05000000074505806, 0.09099999815225601, 0.3230000138282776]  entropy=1.604 (target≈1.792)  KL=0.205\n",
      "[epoch 125] train: loss=0.5878 acc=0.934 | val: loss=0.6370 acc=0.829\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.15000000596046448, 0.09799999743700027, 0.05000000074505806, 0.09099999815225601, 0.32600000500679016]  entropy=1.604 (target≈1.792)  KL=0.205\n",
      "[epoch 126] train: loss=0.5826 acc=0.939 | val: loss=0.6241 acc=0.829\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.09799999743700027, 0.050999999046325684, 0.09200000017881393, 0.328000009059906]  entropy=1.605 (target≈1.792)  KL=0.203\n",
      "[epoch 127] train: loss=0.5830 acc=0.938 | val: loss=0.6257 acc=0.831\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.1509999930858612, 0.0989999994635582, 0.05000000074505806, 0.09099999815225601, 0.3230000138282776]  entropy=1.605 (target≈1.792)  KL=0.204\n",
      "[epoch 128] train: loss=0.5669 acc=0.942 | val: loss=0.6196 acc=0.833\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.14900000393390656, 0.09799999743700027, 0.050999999046325684, 0.09200000017881393, 0.328000009059906]  entropy=1.605 (target≈1.792)  KL=0.203\n",
      "[epoch 129] train: loss=0.5723 acc=0.939 | val: loss=0.6126 acc=0.833\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.15000000596046448, 0.0989999994635582, 0.05000000074505806, 0.09099999815225601, 0.3240000009536743]  entropy=1.605 (target≈1.792)  KL=0.204\n",
      "[epoch 130] train: loss=0.5656 acc=0.939 | val: loss=0.6084 acc=0.840\n",
      "[epoch] avg_usage=[0.2840000092983246, 0.15000000596046448, 0.09799999743700027, 0.05000000074505806, 0.09200000017881393, 0.32600000500679016]  entropy=1.605 (target≈1.792)  KL=0.204\n",
      "[epoch 131] train: loss=0.5602 acc=0.945 | val: loss=0.6061 acc=0.840\n",
      "[epoch] avg_usage=[0.2840000092983246, 0.14900000393390656, 0.0989999994635582, 0.050999999046325684, 0.09300000220537186, 0.32499998807907104]  entropy=1.607 (target≈1.792)  KL=0.201\n",
      "[epoch 132] train: loss=0.5565 acc=0.947 | val: loss=0.6037 acc=0.838\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.0989999994635582, 0.050999999046325684, 0.09200000017881393, 0.3269999921321869]  entropy=1.606 (target≈1.792)  KL=0.203\n",
      "[epoch 133] train: loss=0.5589 acc=0.946 | val: loss=0.5985 acc=0.844\n",
      "[epoch] avg_usage=[0.2840000092983246, 0.1509999930858612, 0.0989999994635582, 0.05000000074505806, 0.09300000220537186, 0.3230000138282776]  entropy=1.607 (target≈1.792)  KL=0.202\n",
      "[epoch 134] train: loss=0.5458 acc=0.953 | val: loss=0.5957 acc=0.848\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.14900000393390656, 0.0989999994635582, 0.050999999046325684, 0.09399999678134918, 0.3240000009536743]  entropy=1.609 (target≈1.792)  KL=0.200\n",
      "[epoch 135] train: loss=0.5449 acc=0.954 | val: loss=0.5944 acc=0.846\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.14900000393390656, 0.0989999994635582, 0.050999999046325684, 0.09300000220537186, 0.32499998807907104]  entropy=1.608 (target≈1.792)  KL=0.200\n",
      "[epoch 136] train: loss=0.5419 acc=0.951 | val: loss=0.5915 acc=0.850\n",
      "[epoch] avg_usage=[0.28700000047683716, 0.14900000393390656, 0.10000000149011612, 0.050999999046325684, 0.09300000220537186, 0.3199999928474426]  entropy=1.608 (target≈1.792)  KL=0.200\n",
      "[epoch 137] train: loss=0.5405 acc=0.956 | val: loss=0.5832 acc=0.850\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.15000000596046448, 0.09799999743700027, 0.050999999046325684, 0.09300000220537186, 0.32499998807907104]  entropy=1.608 (target≈1.792)  KL=0.201\n",
      "[epoch 138] train: loss=0.5362 acc=0.953 | val: loss=0.5769 acc=0.842\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.050999999046325684, 0.09300000220537186, 0.32600000500679016]  entropy=1.610 (target≈1.792)  KL=0.198\n",
      "[epoch 139] train: loss=0.5359 acc=0.952 | val: loss=0.5797 acc=0.842\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.15000000596046448, 0.0989999994635582, 0.050999999046325684, 0.09300000220537186, 0.3230000138282776]  entropy=1.609 (target≈1.792)  KL=0.199\n",
      "[epoch 140] train: loss=0.5317 acc=0.952 | val: loss=0.5723 acc=0.850\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14900000393390656, 0.0989999994635582, 0.050999999046325684, 0.09300000220537186, 0.32100000977516174]  entropy=1.609 (target≈1.792)  KL=0.200\n",
      "[epoch 141] train: loss=0.5277 acc=0.952 | val: loss=0.5692 acc=0.852\n",
      "[epoch] avg_usage=[0.2840000092983246, 0.1509999930858612, 0.10000000149011612, 0.050999999046325684, 0.09300000220537186, 0.32199999690055847]  entropy=1.610 (target≈1.792)  KL=0.198\n",
      "[epoch 142] train: loss=0.5286 acc=0.951 | val: loss=0.5668 acc=0.846\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.052000001072883606, 0.09300000220537186, 0.32499998807907104]  entropy=1.611 (target≈1.792)  KL=0.197\n",
      "[epoch 143] train: loss=0.5221 acc=0.952 | val: loss=0.5629 acc=0.848\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.15000000596046448, 0.0989999994635582, 0.050999999046325684, 0.09399999678134918, 0.3230000138282776]  entropy=1.610 (target≈1.792)  KL=0.198\n",
      "[epoch 144] train: loss=0.5187 acc=0.959 | val: loss=0.5669 acc=0.856\n",
      "[epoch] avg_usage=[0.2849999964237213, 0.15000000596046448, 0.0989999994635582, 0.052000001072883606, 0.09300000220537186, 0.32100000977516174]  entropy=1.610 (target≈1.792)  KL=0.198\n",
      "[epoch 145] train: loss=0.5157 acc=0.959 | val: loss=0.5577 acc=0.852\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.1509999930858612, 0.0989999994635582, 0.050999999046325684, 0.09399999678134918, 0.3230000138282776]  entropy=1.611 (target≈1.792)  KL=0.198\n",
      "[epoch 146] train: loss=0.5141 acc=0.954 | val: loss=0.5548 acc=0.854\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.1509999930858612, 0.10000000149011612, 0.050999999046325684, 0.09300000220537186, 0.32199999690055847]  entropy=1.611 (target≈1.792)  KL=0.197\n",
      "[epoch 147] train: loss=0.5030 acc=0.963 | val: loss=0.5562 acc=0.850\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.14900000393390656, 0.10100000351667404, 0.052000001072883606, 0.0949999988079071, 0.32100000977516174]  entropy=1.614 (target≈1.792)  KL=0.194\n",
      "[epoch 148] train: loss=0.5059 acc=0.958 | val: loss=0.5477 acc=0.858\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.15000000596046448, 0.09799999743700027, 0.052000001072883606, 0.0949999988079071, 0.3190000057220459]  entropy=1.612 (target≈1.792)  KL=0.196\n",
      "[epoch 149] train: loss=0.5028 acc=0.960 | val: loss=0.5468 acc=0.863\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10100000351667404, 0.052000001072883606, 0.0949999988079071, 0.3240000009536743]  entropy=1.614 (target≈1.792)  KL=0.194\n",
      "[epoch 150] train: loss=0.4998 acc=0.966 | val: loss=0.5429 acc=0.854\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14900000393390656, 0.10000000149011612, 0.052000001072883606, 0.0949999988079071, 0.3179999887943268]  entropy=1.613 (target≈1.792)  KL=0.195\n",
      "[epoch 151] train: loss=0.4995 acc=0.960 | val: loss=0.5363 acc=0.858\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.14900000393390656, 0.10000000149011612, 0.052000001072883606, 0.0949999988079071, 0.3230000138282776]  entropy=1.613 (target≈1.792)  KL=0.194\n",
      "[epoch 152] train: loss=0.4950 acc=0.966 | val: loss=0.5355 acc=0.856\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.1509999930858612, 0.10100000351667404, 0.052000001072883606, 0.0949999988079071, 0.3179999887943268]  entropy=1.615 (target≈1.792)  KL=0.193\n",
      "[epoch 153] train: loss=0.4931 acc=0.963 | val: loss=0.5365 acc=0.860\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10100000351667404, 0.052000001072883606, 0.0949999988079071, 0.3240000009536743]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 154] train: loss=0.4888 acc=0.970 | val: loss=0.5299 acc=0.865\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10100000351667404, 0.052000001072883606, 0.0949999988079071, 0.32100000977516174]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 155] train: loss=0.4893 acc=0.965 | val: loss=0.5333 acc=0.863\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.05299999937415123, 0.09600000083446503, 0.32100000977516174]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 156] train: loss=0.4850 acc=0.964 | val: loss=0.5327 acc=0.860\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.052000001072883606, 0.09700000286102295, 0.3199999928474426]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 157] train: loss=0.4785 acc=0.968 | val: loss=0.5240 acc=0.863\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.14900000393390656, 0.10000000149011612, 0.052000001072883606, 0.09700000286102295, 0.3199999928474426]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 158] train: loss=0.4759 acc=0.968 | val: loss=0.5176 acc=0.860\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05400000140070915, 0.0949999988079071, 0.32199999690055847]  entropy=1.617 (target≈1.792)  KL=0.189\n",
      "[epoch 159] train: loss=0.4789 acc=0.965 | val: loss=0.5178 acc=0.869\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.0989999994635582, 0.05400000140070915, 0.09700000286102295, 0.3190000057220459]  entropy=1.617 (target≈1.792)  KL=0.189\n",
      "[epoch 160] train: loss=0.4744 acc=0.969 | val: loss=0.5257 acc=0.867\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.1509999930858612, 0.0989999994635582, 0.05299999937415123, 0.09600000083446503, 0.3179999887943268]  entropy=1.616 (target≈1.792)  KL=0.191\n",
      "[epoch 161] train: loss=0.4716 acc=0.972 | val: loss=0.5202 acc=0.867\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10100000351667404, 0.05299999937415123, 0.09600000083446503, 0.32100000977516174]  entropy=1.618 (target≈1.792)  KL=0.189\n",
      "[epoch 162] train: loss=0.4686 acc=0.970 | val: loss=0.5119 acc=0.865\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.05299999937415123, 0.09600000083446503, 0.3190000057220459]  entropy=1.617 (target≈1.792)  KL=0.189\n",
      "[epoch 163] train: loss=0.4662 acc=0.974 | val: loss=0.5121 acc=0.869\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.1509999930858612, 0.10000000149011612, 0.05299999937415123, 0.09700000286102295, 0.3190000057220459]  entropy=1.619 (target≈1.792)  KL=0.187\n",
      "[epoch 164] train: loss=0.4629 acc=0.973 | val: loss=0.5069 acc=0.867\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.05299999937415123, 0.09700000286102295, 0.3190000057220459]  entropy=1.619 (target≈1.792)  KL=0.187\n",
      "[epoch 165] train: loss=0.4638 acc=0.971 | val: loss=0.5099 acc=0.865\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.10000000149011612, 0.05400000140070915, 0.09700000286102295, 0.3179999887943268]  entropy=1.619 (target≈1.792)  KL=0.186\n",
      "[epoch 166] train: loss=0.4584 acc=0.977 | val: loss=0.5059 acc=0.865\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.14900000393390656, 0.10000000149011612, 0.05400000140070915, 0.09700000286102295, 0.3179999887943268]  entropy=1.620 (target≈1.792)  KL=0.186\n",
      "[epoch 167] train: loss=0.4582 acc=0.972 | val: loss=0.5016 acc=0.865\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05400000140070915, 0.09799999743700027, 0.3190000057220459]  entropy=1.621 (target≈1.792)  KL=0.185\n",
      "[epoch 168] train: loss=0.4573 acc=0.974 | val: loss=0.4983 acc=0.873\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10000000149011612, 0.05400000140070915, 0.09799999743700027, 0.32199999690055847]  entropy=1.619 (target≈1.792)  KL=0.186\n",
      "[epoch 169] train: loss=0.4535 acc=0.973 | val: loss=0.4921 acc=0.865\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.05400000140070915, 0.09700000286102295, 0.31700000166893005]  entropy=1.621 (target≈1.792)  KL=0.185\n",
      "[epoch 170] train: loss=0.4443 acc=0.978 | val: loss=0.4929 acc=0.873\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.14900000393390656, 0.10000000149011612, 0.05400000140070915, 0.09799999743700027, 0.3199999928474426]  entropy=1.620 (target≈1.792)  KL=0.185\n",
      "[epoch 171] train: loss=0.4455 acc=0.978 | val: loss=0.4922 acc=0.867\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.0989999994635582, 0.054999999701976776, 0.09700000286102295, 0.3179999887943268]  entropy=1.622 (target≈1.792)  KL=0.183\n",
      "[epoch 172] train: loss=0.4474 acc=0.978 | val: loss=0.4868 acc=0.867\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.14900000393390656, 0.0989999994635582, 0.054999999701976776, 0.09799999743700027, 0.32199999690055847]  entropy=1.620 (target≈1.792)  KL=0.184\n",
      "[epoch 173] train: loss=0.4483 acc=0.981 | val: loss=0.4899 acc=0.863\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.14900000393390656, 0.0989999994635582, 0.0560000017285347, 0.09799999743700027, 0.31700000166893005]  entropy=1.621 (target≈1.792)  KL=0.183\n",
      "[epoch 174] train: loss=0.4470 acc=0.977 | val: loss=0.4862 acc=0.873\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.1469999998807907, 0.10100000351667404, 0.054999999701976776, 0.09799999743700027, 0.3179999887943268]  entropy=1.622 (target≈1.792)  KL=0.183\n",
      "[epoch 175] train: loss=0.4433 acc=0.978 | val: loss=0.4855 acc=0.873\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.14800000190734863, 0.10100000351667404, 0.054999999701976776, 0.09799999743700027, 0.32199999690055847]  entropy=1.622 (target≈1.792)  KL=0.182\n",
      "[epoch 176] train: loss=0.4413 acc=0.978 | val: loss=0.4869 acc=0.871\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.054999999701976776, 0.09799999743700027, 0.3149999976158142]  entropy=1.624 (target≈1.792)  KL=0.181\n",
      "[epoch 177] train: loss=0.4366 acc=0.980 | val: loss=0.4767 acc=0.877\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.1509999930858612, 0.10000000149011612, 0.054999999701976776, 0.09799999743700027, 0.3160000145435333]  entropy=1.623 (target≈1.792)  KL=0.181\n",
      "[epoch 178] train: loss=0.4380 acc=0.978 | val: loss=0.4754 acc=0.871\n",
      "[epoch] avg_usage=[0.28600001335144043, 0.14800000190734863, 0.10100000351667404, 0.054999999701976776, 0.09700000286102295, 0.31299999356269836]  entropy=1.622 (target≈1.792)  KL=0.183\n",
      "[epoch 179] train: loss=0.4352 acc=0.980 | val: loss=0.4744 acc=0.875\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10000000149011612, 0.054999999701976776, 0.0989999994635582, 0.3190000057220459]  entropy=1.623 (target≈1.792)  KL=0.182\n",
      "[epoch 180] train: loss=0.4331 acc=0.980 | val: loss=0.4789 acc=0.879\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.09799999743700027, 0.3179999887943268]  entropy=1.623 (target≈1.792)  KL=0.181\n",
      "[epoch 181] train: loss=0.4312 acc=0.981 | val: loss=0.4738 acc=0.875\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10100000351667404, 0.054999999701976776, 0.0989999994635582, 0.31700000166893005]  entropy=1.626 (target≈1.792)  KL=0.179\n",
      "[epoch 182] train: loss=0.4217 acc=0.982 | val: loss=0.4705 acc=0.885\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.09799999743700027, 0.31700000166893005]  entropy=1.624 (target≈1.792)  KL=0.180\n",
      "[epoch 183] train: loss=0.4232 acc=0.985 | val: loss=0.4668 acc=0.881\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.0989999994635582, 0.3149999976158142]  entropy=1.626 (target≈1.792)  KL=0.179\n",
      "[epoch 184] train: loss=0.4272 acc=0.982 | val: loss=0.4634 acc=0.881\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.14900000393390656, 0.10000000149011612, 0.0560000017285347, 0.10000000149011612, 0.3140000104904175]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 185] train: loss=0.4238 acc=0.983 | val: loss=0.4721 acc=0.875\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.14900000393390656, 0.10000000149011612, 0.05700000002980232, 0.09799999743700027, 0.31700000166893005]  entropy=1.625 (target≈1.792)  KL=0.178\n",
      "[epoch 186] train: loss=0.4220 acc=0.983 | val: loss=0.4624 acc=0.881\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.0989999994635582, 0.3149999976158142]  entropy=1.625 (target≈1.792)  KL=0.180\n",
      "[epoch 187] train: loss=0.4200 acc=0.986 | val: loss=0.4649 acc=0.877\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.1509999930858612, 0.0989999994635582, 0.0560000017285347, 0.0989999994635582, 0.3140000104904175]  entropy=1.625 (target≈1.792)  KL=0.180\n",
      "[epoch 188] train: loss=0.4228 acc=0.981 | val: loss=0.4664 acc=0.875\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.10000000149011612, 0.31700000166893005]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 189] train: loss=0.4168 acc=0.985 | val: loss=0.4633 acc=0.879\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.10000000149011612, 0.3140000104904175]  entropy=1.627 (target≈1.792)  KL=0.178\n",
      "[epoch 190] train: loss=0.4142 acc=0.985 | val: loss=0.4586 acc=0.877\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.14900000393390656, 0.10000000149011612, 0.0560000017285347, 0.10000000149011612, 0.3140000104904175]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 191] train: loss=0.4158 acc=0.984 | val: loss=0.4548 acc=0.877\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.0989999994635582, 0.0560000017285347, 0.10000000149011612, 0.31299999356269836]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 192] train: loss=0.4163 acc=0.983 | val: loss=0.4521 acc=0.877\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10000000149011612, 0.0560000017285347, 0.0989999994635582, 0.3160000145435333]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 193] train: loss=0.4144 acc=0.985 | val: loss=0.4567 acc=0.879\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10100000351667404, 0.0560000017285347, 0.10000000149011612, 0.31299999356269836]  entropy=1.628 (target≈1.792)  KL=0.176\n",
      "[epoch 194] train: loss=0.4137 acc=0.985 | val: loss=0.4547 acc=0.885\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.14900000393390656, 0.10000000149011612, 0.0560000017285347, 0.10000000149011612, 0.3149999976158142]  entropy=1.626 (target≈1.792)  KL=0.178\n",
      "[epoch 195] train: loss=0.4075 acc=0.986 | val: loss=0.4526 acc=0.885\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.14900000393390656, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.31700000166893005]  entropy=1.627 (target≈1.792)  KL=0.177\n",
      "[epoch 196] train: loss=0.4080 acc=0.985 | val: loss=0.4504 acc=0.881\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.15000000596046448, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.31200000643730164]  entropy=1.627 (target≈1.792)  KL=0.177\n",
      "[epoch 197] train: loss=0.4068 acc=0.989 | val: loss=0.4451 acc=0.877\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10000000149011612, 0.05700000002980232, 0.10100000351667404, 0.3149999976158142]  entropy=1.628 (target≈1.792)  KL=0.175\n",
      "[epoch 198] train: loss=0.4068 acc=0.983 | val: loss=0.4425 acc=0.890\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.31200000643730164]  entropy=1.628 (target≈1.792)  KL=0.176\n",
      "[epoch 199] train: loss=0.4066 acc=0.986 | val: loss=0.4443 acc=0.881\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.14900000393390656, 0.10100000351667404, 0.05700000002980232, 0.10000000149011612, 0.3160000145435333]  entropy=1.629 (target≈1.792)  KL=0.174\n",
      "[epoch 200] train: loss=0.4038 acc=0.986 | val: loss=0.4431 acc=0.885\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10100000351667404, 0.05700000002980232, 0.10000000149011612, 0.3140000104904175]  entropy=1.629 (target≈1.792)  KL=0.175\n",
      "[epoch 201] train: loss=0.4027 acc=0.988 | val: loss=0.4395 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.3149999976158142]  entropy=1.628 (target≈1.792)  KL=0.176\n",
      "[epoch 202] train: loss=0.4014 acc=0.982 | val: loss=0.4400 acc=0.892\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.14900000393390656, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.3140000104904175]  entropy=1.628 (target≈1.792)  KL=0.175\n",
      "[epoch 203] train: loss=0.4009 acc=0.989 | val: loss=0.4395 acc=0.885\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.0989999994635582, 0.057999998331069946, 0.10100000351667404, 0.31299999356269836]  entropy=1.630 (target≈1.792)  KL=0.173\n",
      "[epoch 204] train: loss=0.4026 acc=0.986 | val: loss=0.4397 acc=0.877\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.14900000393390656, 0.10100000351667404, 0.05700000002980232, 0.10199999809265137, 0.3140000104904175]  entropy=1.630 (target≈1.792)  KL=0.173\n",
      "[epoch 205] train: loss=0.4010 acc=0.985 | val: loss=0.4382 acc=0.883\n",
      "[epoch] avg_usage=[0.2750000059604645, 0.15000000596046448, 0.10100000351667404, 0.05700000002980232, 0.10100000351667404, 0.3160000145435333]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 206] train: loss=0.3984 acc=0.988 | val: loss=0.4339 acc=0.883\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10100000351667404, 0.31200000643730164]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 207] train: loss=0.3967 acc=0.986 | val: loss=0.4358 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10199999809265137, 0.3109999895095825]  entropy=1.632 (target≈1.792)  KL=0.171\n",
      "[epoch 208] train: loss=0.3979 acc=0.988 | val: loss=0.4344 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05700000002980232, 0.10000000149011612, 0.31299999356269836]  entropy=1.630 (target≈1.792)  KL=0.174\n",
      "[epoch 209] train: loss=0.3922 acc=0.991 | val: loss=0.4314 acc=0.892\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10100000351667404, 0.31299999356269836]  entropy=1.630 (target≈1.792)  KL=0.173\n",
      "[epoch 210] train: loss=0.3921 acc=0.988 | val: loss=0.4248 acc=0.879\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.14900000393390656, 0.0989999994635582, 0.05700000002980232, 0.10100000351667404, 0.3149999976158142]  entropy=1.629 (target≈1.792)  KL=0.174\n",
      "[epoch 211] train: loss=0.3951 acc=0.987 | val: loss=0.4253 acc=0.890\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10000000149011612, 0.3109999895095825]  entropy=1.630 (target≈1.792)  KL=0.173\n",
      "[epoch 212] train: loss=0.3918 acc=0.991 | val: loss=0.4253 acc=0.885\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.0989999994635582, 0.057999998331069946, 0.10000000149011612, 0.31200000643730164]  entropy=1.630 (target≈1.792)  KL=0.173\n",
      "[epoch 213] train: loss=0.3890 acc=0.992 | val: loss=0.4240 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.0989999994635582, 0.057999998331069946, 0.10100000351667404, 0.3140000104904175]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 214] train: loss=0.3927 acc=0.988 | val: loss=0.4302 acc=0.892\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10100000351667404, 0.057999998331069946, 0.10000000149011612, 0.31299999356269836]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 215] train: loss=0.3890 acc=0.989 | val: loss=0.4260 acc=0.885\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.1509999930858612, 0.10000000149011612, 0.057999998331069946, 0.10000000149011612, 0.3089999854564667]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 216] train: loss=0.3886 acc=0.991 | val: loss=0.4260 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10199999809265137, 0.3109999895095825]  entropy=1.633 (target≈1.792)  KL=0.170\n",
      "[epoch 217] train: loss=0.3879 acc=0.989 | val: loss=0.4179 acc=0.879\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.1509999930858612, 0.10000000149011612, 0.057999998331069946, 0.10100000351667404, 0.3109999895095825]  entropy=1.632 (target≈1.792)  KL=0.171\n",
      "[epoch 218] train: loss=0.3906 acc=0.989 | val: loss=0.4165 acc=0.892\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05900000035762787, 0.10100000351667404, 0.31200000643730164]  entropy=1.632 (target≈1.792)  KL=0.170\n",
      "[epoch 219] train: loss=0.3874 acc=0.990 | val: loss=0.4183 acc=0.883\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10000000149011612, 0.3109999895095825]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 220] train: loss=0.3825 acc=0.993 | val: loss=0.4149 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.0989999994635582, 0.057999998331069946, 0.10100000351667404, 0.31299999356269836]  entropy=1.631 (target≈1.792)  KL=0.172\n",
      "[epoch 221] train: loss=0.3801 acc=0.990 | val: loss=0.4173 acc=0.887\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15000000596046448, 0.10000000149011612, 0.057999998331069946, 0.10100000351667404, 0.3109999895095825]  entropy=1.632 (target≈1.792)  KL=0.171\n",
      "[epoch 222] train: loss=0.3810 acc=0.992 | val: loss=0.4146 acc=0.887\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.0989999994635582, 0.05900000035762787, 0.10199999809265137, 0.31299999356269836]  entropy=1.632 (target≈1.792)  KL=0.170\n",
      "[epoch 223] train: loss=0.3842 acc=0.991 | val: loss=0.4186 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.057999998331069946, 0.10199999809265137, 0.3109999895095825]  entropy=1.633 (target≈1.792)  KL=0.169\n",
      "[epoch 224] train: loss=0.3785 acc=0.992 | val: loss=0.4185 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15000000596046448, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.3100000023841858]  entropy=1.634 (target≈1.792)  KL=0.168\n",
      "[epoch 225] train: loss=0.3786 acc=0.993 | val: loss=0.4128 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.14900000393390656, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.31200000643730164]  entropy=1.633 (target≈1.792)  KL=0.170\n",
      "[epoch 226] train: loss=0.3770 acc=0.994 | val: loss=0.4117 acc=0.887\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.31200000643730164]  entropy=1.634 (target≈1.792)  KL=0.168\n",
      "[epoch 227] train: loss=0.3794 acc=0.992 | val: loss=0.4227 acc=0.885\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.14900000393390656, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.31299999356269836]  entropy=1.633 (target≈1.792)  KL=0.169\n",
      "[epoch 228] train: loss=0.3821 acc=0.992 | val: loss=0.4193 acc=0.883\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.3109999895095825]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 229] train: loss=0.3736 acc=0.994 | val: loss=0.4118 acc=0.881\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10000000149011612, 0.05900000035762787, 0.10300000011920929, 0.3089999854564667]  entropy=1.636 (target≈1.792)  KL=0.166\n",
      "[epoch 230] train: loss=0.3741 acc=0.991 | val: loss=0.4102 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.1509999930858612, 0.10000000149011612, 0.05900000035762787, 0.10199999809265137, 0.3089999854564667]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 231] train: loss=0.3748 acc=0.993 | val: loss=0.4101 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.0989999994635582, 0.05900000035762787, 0.10199999809265137, 0.31299999356269836]  entropy=1.633 (target≈1.792)  KL=0.169\n",
      "[epoch 232] train: loss=0.3718 acc=0.993 | val: loss=0.4118 acc=0.892\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.14900000393390656, 0.10000000149011612, 0.05999999865889549, 0.10300000011920929, 0.3100000023841858]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 233] train: loss=0.3715 acc=0.992 | val: loss=0.4124 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.0989999994635582, 0.05900000035762787, 0.10199999809265137, 0.3100000023841858]  entropy=1.634 (target≈1.792)  KL=0.168\n",
      "[epoch 234] train: loss=0.3734 acc=0.991 | val: loss=0.4058 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.0989999994635582, 0.05900000035762787, 0.10300000011920929, 0.3109999895095825]  entropy=1.634 (target≈1.792)  KL=0.168\n",
      "[epoch 235] train: loss=0.3710 acc=0.992 | val: loss=0.4093 acc=0.892\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10300000011920929, 0.30799999833106995]  entropy=1.636 (target≈1.792)  KL=0.166\n",
      "[epoch 236] train: loss=0.3758 acc=0.993 | val: loss=0.4066 acc=0.885\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15000000596046448, 0.10100000351667404, 0.05900000035762787, 0.10199999809265137, 0.3109999895095825]  entropy=1.636 (target≈1.792)  KL=0.166\n",
      "[epoch 237] train: loss=0.3723 acc=0.993 | val: loss=0.4039 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15000000596046448, 0.10000000149011612, 0.05999999865889549, 0.10199999809265137, 0.3109999895095825]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 238] train: loss=0.3677 acc=0.994 | val: loss=0.4043 acc=0.892\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15000000596046448, 0.10000000149011612, 0.05999999865889549, 0.10300000011920929, 0.3109999895095825]  entropy=1.636 (target≈1.792)  KL=0.166\n",
      "[epoch 239] train: loss=0.3684 acc=0.992 | val: loss=0.4058 acc=0.890\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10300000011920929, 0.30799999833106995]  entropy=1.638 (target≈1.792)  KL=0.164\n",
      "[epoch 240] train: loss=0.3687 acc=0.992 | val: loss=0.4019 acc=0.890\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15000000596046448, 0.10100000351667404, 0.05999999865889549, 0.10300000011920929, 0.3100000023841858]  entropy=1.638 (target≈1.792)  KL=0.164\n",
      "[epoch 241] train: loss=0.3683 acc=0.993 | val: loss=0.4019 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.0989999994635582, 0.05999999865889549, 0.10300000011920929, 0.30799999833106995]  entropy=1.637 (target≈1.792)  KL=0.164\n",
      "[epoch 242] train: loss=0.3673 acc=0.994 | val: loss=0.4003 acc=0.894\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10400000214576721, 0.3089999854564667]  entropy=1.638 (target≈1.792)  KL=0.164\n",
      "[epoch 243] train: loss=0.3660 acc=0.995 | val: loss=0.4085 acc=0.887\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.1509999930858612, 0.10100000351667404, 0.05999999865889549, 0.10300000011920929, 0.3089999854564667]  entropy=1.638 (target≈1.792)  KL=0.163\n",
      "[epoch 244] train: loss=0.3661 acc=0.996 | val: loss=0.3990 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15000000596046448, 0.10000000149011612, 0.05999999865889549, 0.10199999809265137, 0.3100000023841858]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 245] train: loss=0.3650 acc=0.993 | val: loss=0.3990 acc=0.890\n",
      "[epoch] avg_usage=[0.28200000524520874, 0.1509999930858612, 0.0989999994635582, 0.05999999865889549, 0.10199999809265137, 0.3059999942779541]  entropy=1.635 (target≈1.792)  KL=0.167\n",
      "[epoch 246] train: loss=0.3642 acc=0.995 | val: loss=0.3991 acc=0.883\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.0989999994635582, 0.05999999865889549, 0.10400000214576721, 0.3089999854564667]  entropy=1.637 (target≈1.792)  KL=0.165\n",
      "[epoch 247] train: loss=0.3624 acc=0.996 | val: loss=0.3969 acc=0.883\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10400000214576721, 0.3059999942779541]  entropy=1.639 (target≈1.792)  KL=0.163\n",
      "[epoch 248] train: loss=0.3646 acc=0.995 | val: loss=0.3966 acc=0.890\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.1509999930858612, 0.0989999994635582, 0.05999999865889549, 0.10400000214576721, 0.3070000112056732]  entropy=1.639 (target≈1.792)  KL=0.163\n",
      "[epoch 249] train: loss=0.3629 acc=0.994 | val: loss=0.3940 acc=0.885\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.0989999994635582, 0.05999999865889549, 0.10400000214576721, 0.3059999942779541]  entropy=1.638 (target≈1.792)  KL=0.164\n",
      "[epoch 250] train: loss=0.3611 acc=0.995 | val: loss=0.3947 acc=0.890\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10400000214576721, 0.3059999942779541]  entropy=1.639 (target≈1.792)  KL=0.163\n",
      "[epoch 251] train: loss=0.3621 acc=0.995 | val: loss=0.3955 acc=0.896\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.061000000685453415, 0.10400000214576721, 0.3059999942779541]  entropy=1.640 (target≈1.792)  KL=0.162\n",
      "[epoch 252] train: loss=0.3623 acc=0.995 | val: loss=0.3971 acc=0.890\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.0989999994635582, 0.05999999865889549, 0.10400000214576721, 0.3050000071525574]  entropy=1.639 (target≈1.792)  KL=0.163\n",
      "[epoch 253] train: loss=0.3601 acc=0.995 | val: loss=0.3980 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15199999511241913, 0.10000000149011612, 0.05999999865889549, 0.10400000214576721, 0.3070000112056732]  entropy=1.639 (target≈1.792)  KL=0.163\n",
      "[epoch 254] train: loss=0.3614 acc=0.994 | val: loss=0.3934 acc=0.896\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15199999511241913, 0.10000000149011612, 0.05999999865889549, 0.10199999809265137, 0.3050000071525574]  entropy=1.637 (target≈1.792)  KL=0.164\n",
      "[epoch 255] train: loss=0.3632 acc=0.994 | val: loss=0.4011 acc=0.896\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.061000000685453415, 0.10400000214576721, 0.3070000112056732]  entropy=1.640 (target≈1.792)  KL=0.162\n",
      "[epoch 256] train: loss=0.3563 acc=0.996 | val: loss=0.3960 acc=0.892\n",
      "[epoch] avg_usage=[0.2809999883174896, 0.15199999511241913, 0.0989999994635582, 0.05999999865889549, 0.10300000011920929, 0.3050000071525574]  entropy=1.638 (target≈1.792)  KL=0.164\n",
      "[epoch 257] train: loss=0.3609 acc=0.995 | val: loss=0.3978 acc=0.890\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15199999511241913, 0.10000000149011612, 0.061000000685453415, 0.10400000214576721, 0.30300000309944153]  entropy=1.641 (target≈1.792)  KL=0.161\n",
      "[epoch 258] train: loss=0.3585 acc=0.996 | val: loss=0.3960 acc=0.894\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.061000000685453415, 0.10400000214576721, 0.30399999022483826]  entropy=1.641 (target≈1.792)  KL=0.161\n",
      "[epoch 259] train: loss=0.3561 acc=0.995 | val: loss=0.3998 acc=0.890\n",
      "[epoch] avg_usage=[0.2750000059604645, 0.1509999930858612, 0.10000000149011612, 0.05999999865889549, 0.10499999672174454, 0.30799999833106995]  entropy=1.640 (target≈1.792)  KL=0.162\n",
      "[epoch 260] train: loss=0.3565 acc=0.997 | val: loss=0.3949 acc=0.890\n",
      "[epoch] avg_usage=[0.2750000059604645, 0.15199999511241913, 0.0989999994635582, 0.061000000685453415, 0.10599999874830246, 0.3070000112056732]  entropy=1.641 (target≈1.792)  KL=0.160\n",
      "[epoch 261] train: loss=0.3574 acc=0.996 | val: loss=0.3957 acc=0.894\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.10000000149011612, 0.061000000685453415, 0.10499999672174454, 0.30300000309944153]  entropy=1.642 (target≈1.792)  KL=0.160\n",
      "[epoch 262] train: loss=0.3551 acc=0.996 | val: loss=0.3908 acc=0.892\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.1509999930858612, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.3070000112056732]  entropy=1.641 (target≈1.792)  KL=0.159\n",
      "[epoch 263] train: loss=0.3537 acc=0.998 | val: loss=0.3914 acc=0.898\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.1509999930858612, 0.0989999994635582, 0.061000000685453415, 0.10499999672174454, 0.3059999942779541]  entropy=1.641 (target≈1.792)  KL=0.159\n",
      "[epoch 264] train: loss=0.3565 acc=0.995 | val: loss=0.3881 acc=0.887\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.3019999861717224]  entropy=1.642 (target≈1.792)  KL=0.159\n",
      "[epoch 265] train: loss=0.3593 acc=0.994 | val: loss=0.3869 acc=0.896\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10599999874830246, 0.30399999022483826]  entropy=1.644 (target≈1.792)  KL=0.157\n",
      "[epoch 266] train: loss=0.3549 acc=0.996 | val: loss=0.3894 acc=0.894\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.30399999022483826]  entropy=1.643 (target≈1.792)  KL=0.157\n",
      "[epoch 267] train: loss=0.3538 acc=0.996 | val: loss=0.3865 acc=0.900\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.3050000071525574]  entropy=1.643 (target≈1.792)  KL=0.157\n",
      "[epoch 268] train: loss=0.3519 acc=0.995 | val: loss=0.3910 acc=0.890\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10599999874830246, 0.3009999990463257]  entropy=1.643 (target≈1.792)  KL=0.158\n",
      "[epoch 269] train: loss=0.3552 acc=0.995 | val: loss=0.3851 acc=0.894\n",
      "[epoch] avg_usage=[0.27300000190734863, 0.1509999930858612, 0.10000000149011612, 0.06199999898672104, 0.10599999874830246, 0.30799999833106995]  entropy=1.644 (target≈1.792)  KL=0.157\n",
      "[epoch 270] train: loss=0.3544 acc=0.996 | val: loss=0.3893 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15199999511241913, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.30300000309944153]  entropy=1.644 (target≈1.792)  KL=0.157\n",
      "[epoch 271] train: loss=0.3523 acc=0.996 | val: loss=0.3892 acc=0.892\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.0989999994635582, 0.06199999898672104, 0.10599999874830246, 0.3019999861717224]  entropy=1.643 (target≈1.792)  KL=0.157\n",
      "[epoch 272] train: loss=0.3517 acc=0.996 | val: loss=0.3888 acc=0.887\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15299999713897705, 0.0989999994635582, 0.06300000101327896, 0.10700000077486038, 0.3009999990463257]  entropy=1.646 (target≈1.792)  KL=0.155\n",
      "[epoch 273] train: loss=0.3511 acc=0.998 | val: loss=0.3837 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.06300000101327896, 0.10599999874830246, 0.30300000309944153]  entropy=1.645 (target≈1.792)  KL=0.156\n",
      "[epoch 274] train: loss=0.3532 acc=0.994 | val: loss=0.3864 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15299999713897705, 0.10000000149011612, 0.06199999898672104, 0.10499999672174454, 0.30399999022483826]  entropy=1.644 (target≈1.792)  KL=0.157\n",
      "[epoch 275] train: loss=0.3502 acc=0.997 | val: loss=0.3796 acc=0.896\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15299999713897705, 0.10000000149011612, 0.06300000101327896, 0.10599999874830246, 0.30300000309944153]  entropy=1.645 (target≈1.792)  KL=0.156\n",
      "[epoch 276] train: loss=0.3500 acc=0.996 | val: loss=0.3792 acc=0.898\n",
      "[epoch] avg_usage=[0.28299999237060547, 0.15199999511241913, 0.10000000149011612, 0.06300000101327896, 0.10499999672174454, 0.2980000078678131]  entropy=1.644 (target≈1.792)  KL=0.156\n",
      "[epoch 277] train: loss=0.3483 acc=0.997 | val: loss=0.3823 acc=0.894\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.1509999930858612, 0.10100000351667404, 0.06300000101327896, 0.10599999874830246, 0.3019999861717224]  entropy=1.647 (target≈1.792)  KL=0.153\n",
      "[epoch 278] train: loss=0.3489 acc=0.996 | val: loss=0.3794 acc=0.887\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.0989999994635582, 0.06300000101327896, 0.10700000077486038, 0.3009999990463257]  entropy=1.646 (target≈1.792)  KL=0.154\n",
      "[epoch 279] train: loss=0.3500 acc=0.996 | val: loss=0.3862 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.06300000101327896, 0.10700000077486038, 0.3019999861717224]  entropy=1.646 (target≈1.792)  KL=0.154\n",
      "[epoch 280] train: loss=0.3499 acc=0.998 | val: loss=0.3827 acc=0.896\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.1509999930858612, 0.10000000149011612, 0.06300000101327896, 0.10700000077486038, 0.3019999861717224]  entropy=1.646 (target≈1.792)  KL=0.154\n",
      "[epoch 281] train: loss=0.3502 acc=0.997 | val: loss=0.3852 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.0989999994635582, 0.06400000303983688, 0.10599999874830246, 0.30300000309944153]  entropy=1.645 (target≈1.792)  KL=0.155\n",
      "[epoch 282] train: loss=0.3486 acc=0.996 | val: loss=0.3833 acc=0.892\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15199999511241913, 0.0989999994635582, 0.06300000101327896, 0.1080000028014183, 0.3009999990463257]  entropy=1.647 (target≈1.792)  KL=0.153\n",
      "[epoch 283] train: loss=0.3473 acc=0.996 | val: loss=0.3774 acc=0.890\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.0989999994635582, 0.06300000101327896, 0.10599999874830246, 0.3009999990463257]  entropy=1.646 (target≈1.792)  KL=0.154\n",
      "[epoch 284] train: loss=0.3477 acc=0.996 | val: loss=0.3782 acc=0.894\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.10599999874830246, 0.2980000078678131]  entropy=1.648 (target≈1.792)  KL=0.152\n",
      "[epoch 285] train: loss=0.3470 acc=0.996 | val: loss=0.3794 acc=0.896\n",
      "[epoch] avg_usage=[0.27300000190734863, 0.15199999511241913, 0.10100000351667404, 0.06400000303983688, 0.10599999874830246, 0.30399999022483826]  entropy=1.647 (target≈1.792)  KL=0.152\n",
      "[epoch 286] train: loss=0.3481 acc=0.998 | val: loss=0.3833 acc=0.894\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.0989999994635582, 0.06300000101327896, 0.10700000077486038, 0.3009999990463257]  entropy=1.646 (target≈1.792)  KL=0.154\n",
      "[epoch 287] train: loss=0.3462 acc=0.996 | val: loss=0.3777 acc=0.896\n",
      "[epoch] avg_usage=[0.2800000011920929, 0.15299999713897705, 0.0989999994635582, 0.06400000303983688, 0.10599999874830246, 0.29899999499320984]  entropy=1.647 (target≈1.792)  KL=0.153\n",
      "[epoch 288] train: loss=0.3462 acc=0.998 | val: loss=0.3734 acc=0.898\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.10700000077486038, 0.2980000078678131]  entropy=1.648 (target≈1.792)  KL=0.151\n",
      "[epoch 289] train: loss=0.3463 acc=0.995 | val: loss=0.3785 acc=0.890\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.1509999930858612, 0.10000000149011612, 0.06400000303983688, 0.10700000077486038, 0.3009999990463257]  entropy=1.648 (target≈1.792)  KL=0.152\n",
      "[epoch 290] train: loss=0.3470 acc=0.997 | val: loss=0.3789 acc=0.892\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.10700000077486038, 0.30000001192092896]  entropy=1.648 (target≈1.792)  KL=0.152\n",
      "[epoch 291] train: loss=0.3448 acc=0.997 | val: loss=0.3745 acc=0.894\n",
      "[epoch] avg_usage=[0.2759999930858612, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.1080000028014183, 0.30000001192092896]  entropy=1.649 (target≈1.792)  KL=0.150\n",
      "[epoch 292] train: loss=0.3456 acc=0.997 | val: loss=0.3799 acc=0.883\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15199999511241913, 0.10100000351667404, 0.06400000303983688, 0.10599999874830246, 0.3009999990463257]  entropy=1.648 (target≈1.792)  KL=0.152\n",
      "[epoch 293] train: loss=0.3440 acc=0.997 | val: loss=0.3768 acc=0.900\n",
      "[epoch] avg_usage=[0.27900001406669617, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.10599999874830246, 0.29899999499320984]  entropy=1.648 (target≈1.792)  KL=0.152\n",
      "[epoch 294] train: loss=0.3447 acc=0.997 | val: loss=0.3759 acc=0.898\n",
      "[epoch] avg_usage=[0.2750000059604645, 0.15199999511241913, 0.10100000351667404, 0.06400000303983688, 0.1080000028014183, 0.30000001192092896]  entropy=1.650 (target≈1.792)  KL=0.149\n",
      "[epoch 295] train: loss=0.3442 acc=0.998 | val: loss=0.3762 acc=0.900\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.06499999761581421, 0.10700000077486038, 0.29899999499320984]  entropy=1.649 (target≈1.792)  KL=0.150\n",
      "[epoch 296] train: loss=0.3467 acc=0.997 | val: loss=0.3774 acc=0.892\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.1080000028014183, 0.2980000078678131]  entropy=1.650 (target≈1.792)  KL=0.150\n",
      "[epoch 297] train: loss=0.3472 acc=0.996 | val: loss=0.3824 acc=0.890\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15199999511241913, 0.10000000149011612, 0.06400000303983688, 0.10700000077486038, 0.2980000078678131]  entropy=1.650 (target≈1.792)  KL=0.150\n",
      "[epoch 298] train: loss=0.3417 acc=0.998 | val: loss=0.3770 acc=0.894\n",
      "[epoch] avg_usage=[0.2770000100135803, 0.15299999713897705, 0.0989999994635582, 0.06400000303983688, 0.10700000077486038, 0.29899999499320984]  entropy=1.649 (target≈1.792)  KL=0.151\n",
      "[epoch 299] train: loss=0.3436 acc=0.998 | val: loss=0.3764 acc=0.900\n",
      "[epoch] avg_usage=[0.27799999713897705, 0.15299999713897705, 0.10100000351667404, 0.06499999761581421, 0.10599999874830246, 0.2980000078678131]  entropy=1.650 (target≈1.792)  KL=0.149\n",
      "[epoch 300] train: loss=0.3426 acc=0.996 | val: loss=0.3714 acc=0.898\n",
      "[pretrain] loaded best model (val acc=0.900) from C:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\models\\MOE\\20250901_1440_MOE\n"
     ]
    }
   ],
   "source": [
    "model, logs = moe_pretrain(model, MY_CONFIG, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3be02153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss=0.3865 acc=0.900\n"
     ]
    }
   ],
   "source": [
    "# --- evaluate on pretrain val for sanity ---\n",
    "val_loss, val_acc = evaluate(model, val_loader, device=DEVICE)\n",
    "print(f\"[val] loss={val_loss:.4f} acc={val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02160fc6",
   "metadata": {},
   "source": [
    "### Transfer Learning Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b6b4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I use this directly? I don't see why not if I have the dataloaders\n",
    "## I don't have a config defined tho...\n",
    "## I ought to make one FWIW\n",
    "\n",
    "# This func is from my DNN_FT.py which is imported above\n",
    "#def fine_tune_model(finetuned_model, fine_tune_loader, config, timestamp, test_loader=None, pid=None, num_epochs=None):  #use_earlystopping=None,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823b178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[novel user P010] adapting with 10 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\utils\\gesture_dataset_classes.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  return torch.tensor(x, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT : Early stopping reached after 52 epochs\n",
      "[novel user P010] query loss=2.8377 acc=0.233\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.20         9\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.45      0.56      0.50         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         9\n",
      "           8       0.17      0.78      0.29         9\n",
      "           9       0.35      0.89      0.50         9\n",
      "\n",
      "    accuracy                           0.23        90\n",
      "   macro avg       0.20      0.23      0.15        90\n",
      "weighted avg       0.20      0.23      0.15        90\n",
      "\n",
      "\n",
      "\n",
      "[novel user P116] adapting with 10 samples\n",
      "FT : Early stopping reached after 41 epochs\n",
      "[novel user P116] query loss=3.2714 acc=0.144\n",
      "\n",
      "\n",
      "[novel user P127] adapting with 10 samples\n",
      "FT : Early stopping reached after 17 epochs\n",
      "[novel user P127] query loss=3.5795 acc=0.089\n",
      "\n",
      "\n",
      "[novel user P108] adapting with 10 samples\n",
      "FT : Early stopping reached after 20 epochs\n",
      "[novel user P108] query loss=3.3880 acc=0.089\n",
      "\n",
      "\n",
      "[novel user P128] adapting with 10 samples\n",
      "FT : Early stopping reached after 42 epochs\n",
      "[novel user P128] query loss=2.8286 acc=0.111\n",
      "\n",
      "\n",
      "[novel user P104] adapting with 10 samples\n",
      "FT : Early stopping reached after 67 epochs\n",
      "[novel user P104] query loss=2.6920 acc=0.311\n",
      "\n",
      "\n",
      "[novel user P125] adapting with 10 samples\n",
      "FT : Early stopping reached after 56 epochs\n",
      "[novel user P125] query loss=2.2836 acc=0.367\n",
      "\n",
      "\n",
      "[novel user P103] adapting with 10 samples\n",
      "FT : Early stopping reached after 32 epochs\n",
      "[novel user P103] query loss=3.2222 acc=0.156\n",
      "\n",
      "\n",
      "[summary] novel users mean acc = 0.188\n"
     ]
    }
   ],
   "source": [
    "# --- adapt/evaluate per novel user using your separate loaders ---\n",
    "results = {}\n",
    "for user_key in set(data_splits['novel_trainFT_dict']['participant_ids']):\n",
    "    user_model_inst = copy.deepcopy(model)\n",
    "\n",
    "    subj_spec_support_dict = filter_by_participant(data_splits['novel_trainFT_dict'], user_key)   # ~10 shots total\n",
    "    subj_spec_query_dict   = filter_by_participant(data_splits['novel_subject_test_dict'], user_key) # test set for that user\n",
    "\n",
    "    subj_spec_support_dataset = make_tensor_dataset(subj_spec_support_dict['feature'], subj_spec_support_dict['labels'], MY_CONFIG)\n",
    "    # TODO: Technically I should be using ft_batch_size here... idk if it matters that much...\n",
    "    subj_spec_support_loader = DataLoader(subj_spec_support_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    subj_spec_query_dataset = make_tensor_dataset(subj_spec_query_dict['feature'], subj_spec_query_dict['labels'], MY_CONFIG)\n",
    "    subj_spec_query_loader = DataLoader(subj_spec_query_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    print(f\"\\n[novel user {user_key}] adapting with {len(subj_spec_support_dataset)} samples\")\n",
    "    ft_res_dict = fine_tune_model(user_model_inst, subj_spec_support_loader, MY_CONFIG, timestamp, test_loader=subj_spec_query_loader, pid=None, num_epochs=None)\n",
    "\n",
    "    # What happens when user_embed_override is None? Does this work as expected? Review what exactly this is trying to do\n",
    "    test_loss, test_acc = evaluate(user_model_inst, subj_spec_query_loader, device=DEVICE)#, user_embed_override=u_embed)\n",
    "    print(f\"[novel user {user_key}] query loss={test_loss:.4f} acc={test_acc:.3f}\")\n",
    "    results[user_key] = {\"loss\": test_loss, \"acc\": test_acc}\n",
    "\n",
    "    # Only print this for the first user\n",
    "    if user_key == data_splits['novel_trainFT_dict']['participant_ids'][0]:\n",
    "        report_user(user_model_inst, subj_spec_query_loader, device=DEVICE, y_names=None, user_embed=None)\n",
    "    print()\n",
    "\n",
    "if results:\n",
    "    mean_acc = np.mean([v[\"acc\"] for v in results.values()])\n",
    "    print(f\"\\n[summary] novel users mean acc = {mean_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee80068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P010': {'loss': 2.8377021418677435, 'acc': 0.23333333333333334},\n",
       " 'P116': {'loss': 3.2714287175072565, 'acc': 0.14444444444444443},\n",
       " 'P127': {'loss': 3.5794785817464194, 'acc': 0.08888888888888889},\n",
       " 'P108': {'loss': 3.3879948721991644, 'acc': 0.08888888888888889},\n",
       " 'P128': {'loss': 2.8285902871025934, 'acc': 0.1111111111111111},\n",
       " 'P104': {'loss': 2.6920117139816284, 'acc': 0.3111111111111111},\n",
       " 'P125': {'loss': 2.2835858596695795, 'acc': 0.36666666666666664},\n",
       " 'P103': {'loss': 3.2221693197886148, 'acc': 0.15555555555555556}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96224df1",
   "metadata": {},
   "source": [
    "### PEFT (User Conditioning Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c267abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[novel user P010] adapting with 10 samples\n",
      "[novel user P010] query loss=3.6023 acc=0.100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.50      0.22      0.31         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         9\n",
      "           8       0.06      0.44      0.11         9\n",
      "           9       0.17      0.33      0.22         9\n",
      "\n",
      "    accuracy                           0.10        90\n",
      "   macro avg       0.07      0.10      0.06        90\n",
      "weighted avg       0.07      0.10      0.06        90\n",
      "\n",
      "\n",
      "\n",
      "[novel user P116] adapting with 10 samples\n",
      "[novel user P116] query loss=3.6094 acc=0.122\n",
      "\n",
      "\n",
      "[novel user P127] adapting with 10 samples\n",
      "[novel user P127] query loss=3.6221 acc=0.111\n",
      "\n",
      "\n",
      "[novel user P108] adapting with 10 samples\n",
      "[novel user P108] query loss=3.4443 acc=0.100\n",
      "\n",
      "\n",
      "[novel user P128] adapting with 10 samples\n",
      "[novel user P128] query loss=3.0587 acc=0.044\n",
      "\n",
      "\n",
      "[novel user P104] adapting with 10 samples\n",
      "[novel user P104] query loss=3.6154 acc=0.200\n",
      "\n",
      "\n",
      "[novel user P125] adapting with 10 samples\n",
      "[novel user P125] query loss=2.8117 acc=0.189\n",
      "\n",
      "\n",
      "[novel user P103] adapting with 10 samples\n",
      "[novel user P103] query loss=3.5091 acc=0.067\n",
      "\n",
      "\n",
      "[summary] novel users mean acc = 0.117\n"
     ]
    }
   ],
   "source": [
    "# --- adapt/evaluate per novel user using your separate loaders ---\n",
    "results = {}\n",
    "for user_key in set(data_splits['novel_trainFT_dict']['participant_ids']):\n",
    "    user_model_inst = copy.deepcopy(model)\n",
    "\n",
    "    subj_spec_support_dict = filter_by_participant(data_splits['novel_trainFT_dict'], user_key)   # ~10 shots total\n",
    "    subj_spec_query_dict   = filter_by_participant(data_splits['novel_subject_test_dict'], user_key) # test set for that user\n",
    "\n",
    "    subj_spec_support_dataset = make_tensor_dataset(subj_spec_support_dict['feature'], subj_spec_support_dict['labels'], MY_CONFIG)\n",
    "    # TODO: Technically I should be using ft_batch_size here... idk if it matters that much...\n",
    "    subj_spec_support_loader = DataLoader(subj_spec_support_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    subj_spec_query_dataset = make_tensor_dataset(subj_spec_query_dict['feature'], subj_spec_query_dict['labels'], MY_CONFIG)\n",
    "    subj_spec_query_loader = DataLoader(subj_spec_query_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    print(f\"\\n[novel user {user_key}] adapting with {len(subj_spec_support_dataset)} samples\")\n",
    "    u_embed, peft_logs = peft_user_emb_vec(user_model_inst, subj_spec_support_loader, MY_CONFIG)\n",
    "    test_loss, test_acc = evaluate(user_model_inst, subj_spec_query_loader, device=DEVICE, user_embed_override=u_embed)\n",
    "    print(f\"[novel user {user_key}] query loss={test_loss:.4f} acc={test_acc:.3f}\")\n",
    "    results[user_key] = {\"loss\": test_loss, \"acc\": test_acc}\n",
    "\n",
    "    # Only print this for the first user\n",
    "    if user_key == data_splits['novel_trainFT_dict']['participant_ids'][0]:\n",
    "        report_user(user_model_inst, subj_spec_query_loader, device=DEVICE, y_names=None, user_embed=None)\n",
    "    print()\n",
    "\n",
    "if results:\n",
    "    mean_acc = np.mean([v[\"acc\"] for v in results.values()])\n",
    "    print(f\"\\n[summary] novel users mean acc = {mean_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c7a6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P010': {'loss': 3.6022519800398083, 'acc': 0.1},\n",
       " 'P116': {'loss': 3.6093960073259144, 'acc': 0.12222222222222222},\n",
       " 'P127': {'loss': 3.622050311830309, 'acc': 0.1111111111111111},\n",
       " 'P108': {'loss': 3.4443455007341175, 'acc': 0.1},\n",
       " 'P128': {'loss': 3.0587346288892956, 'acc': 0.044444444444444446},\n",
       " 'P104': {'loss': 3.6154187785254583, 'acc': 0.2},\n",
       " 'P125': {'loss': 2.811742809083727, 'acc': 0.18888888888888888},\n",
       " 'P103': {'loss': 3.50908801290724, 'acc': 0.06666666666666667}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55af597",
   "metadata": {},
   "source": [
    "### Baseline Classifiers\n",
    "> Use just the shallow embedding network, NOT the actual MOE portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "661ab57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user P010] Proto=0.733 | Ridge(best)=0.800\n",
      "[user P103] Proto=0.722 | Ridge(best)=0.700\n",
      "[user P104] Proto=0.767 | Ridge(best)=0.767\n",
      "[user P108] Proto=0.478 | Ridge(best)=0.544\n",
      "[user P116] Proto=0.767 | Ridge(best)=0.833\n",
      "[user P125] Proto=0.778 | Ridge(best)=0.867\n",
      "[user P127] Proto=0.567 | Ridge(best)=0.567\n",
      "[user P128] Proto=0.844 | Ridge(best)=0.811\n",
      "\n",
      "[Proto] mean acc: 0.706944465637207\n",
      "[Ridge] mean acc: 0.7361111268401146\n"
     ]
    }
   ],
   "source": [
    "# ---------- Run on your novel users ----------\n",
    "proto_results = {}\n",
    "ridge_results = {}\n",
    "\n",
    "for user_key in np.unique(data_splits['novel_trainFT_dict']['participant_ids']):\n",
    "    subj_spec_support_dict = filter_by_participant(data_splits['novel_trainFT_dict'], user_key)   # ~10 shots total\n",
    "    subj_spec_query_dict   = filter_by_participant(data_splits['novel_subject_test_dict'], user_key) # test set for that user\n",
    "\n",
    "    subj_spec_support_dataset = make_tensor_dataset(subj_spec_support_dict['feature'], subj_spec_support_dict['labels'], MY_CONFIG)\n",
    "    # TODO: Technically I should be using ft_batch_size here... idk if it matters that much...\n",
    "    subj_spec_support_loader = DataLoader(subj_spec_support_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    subj_spec_query_dataset = make_tensor_dataset(subj_spec_query_dict['feature'], subj_spec_query_dict['labels'], MY_CONFIG)\n",
    "    subj_spec_query_loader = DataLoader(subj_spec_query_dataset, batch_size=MY_CONFIG[\"ft_batch_size\"], shuffle=True)\n",
    "\n",
    "    proto_acc = proto_eval_user(copy.deepcopy(model), subj_spec_support_loader, subj_spec_query_loader, MY_CONFIG, tau=0.5)\n",
    "    # try a couple of regularizations quickly\n",
    "    ridge_accs = []\n",
    "    for reg in (0.1, 1.0, 10.0):\n",
    "        ridge_accs.append(ridge_eval_user(copy.deepcopy(model), subj_spec_support_loader, subj_spec_query_loader, MY_CONFIG, reg=reg))\n",
    "    ridge_best = max(ridge_accs)\n",
    "  \n",
    "    # Only print for first user\n",
    "    if user_key == data_splits['novel_trainFT_dict']['participant_ids']:\n",
    "        report_user(copy.deepcopy(model), subj_spec_query_loader, DEVICE, y_names=None, user_embed=None)\n",
    "\n",
    "    proto_results[user_key] = proto_acc\n",
    "    ridge_results[user_key] = ridge_best\n",
    "    print(f\"[user {user_key}] Proto={proto_acc:.3f} | Ridge(best)={ridge_best:.3f}\")\n",
    "\n",
    "if proto_results:\n",
    "    print(\"\\n[Proto] mean acc:\", np.mean(list(proto_results.values())))\n",
    "if ridge_results:\n",
    "    print(\"[Ridge] mean acc:\", np.mean(list(ridge_results.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f3316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b477aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70190cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iii) FiLM adapter PEFT (tiny but powerful)\n",
    "#Adapt only a per-user affine on the backbone embedding (and optionally keep your user-embed):\n",
    "\n",
    "class UserFilm(nn.Module):\n",
    "    def __init__(self, emb_dim): super().__init__(); \n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, emb_dim))\n",
    "        self.beta  = nn.Parameter(torch.zeros(1, emb_dim))\n",
    "    def forward(self, h): return self.gamma * h + self.beta\n",
    "\n",
    "#During PEFT:\n",
    "#Freeze model.\n",
    "#Optimize {film.gamma, film.beta} (+ optional u_user) with LR 1e-2, WD 1e-3, 80–120 steps, eval() mode.\n",
    "#Replace h = model.backbone(x) by h = film(model.backbone(x)) before gating/experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3706d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5aba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915618c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26415a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7b7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
