{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200a0ae2",
   "metadata": {},
   "source": [
    "> __Purpose:__ This notebook will implement our MAML functions and test the episodic dataloader structure. Once everything is running, either this NB or a copy will do HPO for the MAML parameters. We will include the MOE layer for now (although placement is probably quite poor), and we will stick with the CNN-MLP architecture since the CNN-LSTM architecture was not performing as well (the latter architecture was probably not fit sufficiently in its HPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846d28b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_DIR: C:\\Users\\kdmen\\Repos\\pers-gest-cls\\\n",
      "DATA_DIR: C:\\Users\\kdmen\\Box\\Yamagami Lab\\Data\\Meta_Gesture_Project\\filtered_datasets\\\n",
      "RUN_DIR: C:\\Users\\kdmen\\Repos\\pers-gest-cls\\system\\results\\\n"
     ]
    }
   ],
   "source": [
    "CODE_DIR = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\\"\n",
    "DATA_DIR = \"C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\\\Meta_Gesture_Project\\\\filtered_datasets\\\\\"\n",
    "RUN_DIR  = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\system\\\\results\\\\\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "# env -> Path objects\n",
    "#CODE_DIR = Path(os.environ.get(\"CODE_DIR\", \"./\")).resolve()\n",
    "#DATA_DIR = Path(os.environ.get(\"DATA_DIR\", \"./\")).resolve()\n",
    "#RUN_DIR = Path(os.environ.get(\"RUN_DIR\", \"./\")).resolve()\n",
    "print(f\"CODE_DIR: {CODE_DIR}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"RUN_DIR: {RUN_DIR}\")\n",
    "# TODO: Update these for the appropriate saving. Don't save into the git repo on PROJECTS\n",
    "#######################################################\n",
    "# === SAVING (to SCRATCH) ===\n",
    "#results_save_dir = RUN_DIR.parent / \"runs\" / f\"{timestamp}_MOE\"        # /scratch/my13/kai/runs/<timestamp>_MOE\n",
    "#models_save_dir  = RUN_DIR.parent / \"models\" / \"MOE\" / f\"{timestamp}_MOE\"\n",
    "results_save_dir = RUN_DIR\n",
    "models_save_dir  = RUN_DIR\n",
    "# make sure they exist\n",
    "#results_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#models_save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cda459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\kdmen\\Repos\\pers-gest-cls\\system\\MAML_MOE\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf90574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Get the path to the 'pers-gest-cls' directory\n",
    "# Since you are in .../system/MAML_MOE, we go up two levels\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "#current_directory = os.getcwd()\n",
    "#print(f\"The current working directory is: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775ed30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 1\n",
    "FIXED_SEED = 42\n",
    "\n",
    "import copy, json, time#, joblib, sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "#from optuna.samplers import TPESampler\n",
    "#from optuna.pruners import MedianPruner\n",
    "#from optuna.storages import JournalStorage, JournalFileBackend\n",
    "from optuna.storages.journal import JournalStorage, JournalFileBackend\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# === LOADING (from SCRATCH data bucket) ===\n",
    "# adjust these to where you staged each file under $DATA_DIR\n",
    "## These are now inputs to the config directly... --> These are actually the NOTS specific versions...\n",
    "###########################\n",
    "user_split_json_filepath = CODE_DIR + \"system\\\\fixed_user_splits\\\\4kfcv_splits_shared_test.json\"\n",
    "def apply_fold_to_config(config, all_splits, fold_idx):\n",
    "    \"\"\"Mutates config in-place to set train/val/test PIDs for the given fold.\"\"\"\n",
    "    split = all_splits[fold_idx]\n",
    "    config[\"train_PIDs\"] = split[\"train\"]\n",
    "    config[\"val_PIDs\"]   = split[\"val\"]\n",
    "    config[\"test_PIDs\"]  = split[\"test\"]\n",
    "    config[\"num_pretrain_users\"] = len(config[\"train_PIDs\"])\n",
    "    config[\"num_testft_users\"] = len(config[\"val_PIDs\"])\n",
    "###########################\n",
    "\n",
    "from system.MAML_MOE.multimodal_data_processing import *  # Needed for load_multimodal_dataloaders()\n",
    "from system.MAML_MOE.mamlpp import *\n",
    "from system.MAML_MOE.maml_multimodal_dataloaders import *\n",
    "from system.MAML_MOE.MOE_CNN_LSTM import *\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a08af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0218a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalCNNLSTMMOE(\n",
       "  (emg_encoder): Sequential(\n",
       "    (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (imu_encoder): Sequential(\n",
       "    (0): Conv1d(72, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (demo_encoder): DemographicsEncoder(\n",
       "    (net): Sequential(\n",
       "      (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=16, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(32, 32, batch_first=True, bidirectional=True)\n",
       "  (context_projector): ContextEncoder(\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=4, bias=True)\n",
       "      (1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (moe): MOELayer(\n",
       "    (gate): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (experts): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=64, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Assuming MultimodalCNNLSTMMOE is imported from your system modules\n",
    "# from system.MAML_MOE.mamlpp import MultimodalCNNLSTMMOE \n",
    "\n",
    "config = dict()\n",
    "config[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "config[\"feature_engr\"] = \"None\"\n",
    "config[\"time_steps\"] = 1 \n",
    "config[\"sequence_length\"] = 64 \n",
    "config[\"num_train_gesture_trials\"] = 9\n",
    "config[\"num_ft_gesture_trials\"] = 1\n",
    "config[\"padding\"] = 0 \n",
    "config[\"use_batch_norm\"] = False \n",
    "config[\"timestamp\"] = timestamp\n",
    "config[\"dropout\"] = 0.0  # Lowest dropout for debugging\n",
    "config[\"num_classes\"] = 10\n",
    "config[\"use_earlystopping\"] = True\n",
    "config[\"verbose\"] = False\n",
    "config[\"num_total_users\"] = 32\n",
    "config[\"train_gesture_range\"] = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "config[\"valtest_gesture_range\"] = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "config[\"NOTS\"] = False\n",
    "if config[\"NOTS\"] == False:\n",
    "    config[\"emg_imu_pkl_full_path\"] = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\\\Meta_Gesture_Project\\\\filtered_datasets\\\\metadata_IMU_EMG_allgestures_allusers.pkl'\n",
    "    config[\"pwmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\dataset\\\\Biosignal gesture questionnaire for participants with disabilities.xlsx\"\n",
    "    config[\"pwoutmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\dataset\\\\Biosignal gesture questionnaire for participants without disabilities.xlsx\"\n",
    "    config[\"dfs_save_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\dataset\\\\meta-learning-sup-que-ds\\\\\"\n",
    "    config[\"dfs_load_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\dataset\\\\meta-learning-sup-que-ds\\\\\"\n",
    "    config[\"saved_df_timestamp\"] = '20250917_1217'\n",
    "    config[\"user_split_json_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\system\\\\fixed_user_splits\\\\4kfcv_splits_shared_test.json\"\n",
    "    config[\"results_save_dir\"] = f\"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\system\\\\results\\\\local_{timestamp}\"\n",
    "    config[\"models_save_dir\"] = f\"C:\\\\Users\\\\kdmen\\\\Repos\\\\pers-gest-cls\\\\system\\\\models\\\\local_{timestamp}\"\n",
    "elif config[\"NOTS\"] == True:\n",
    "    config[\"user_split_json_filepath\"] = user_split_json_filepath\n",
    "    config[\"results_save_dir\"] = results_save_dir\n",
    "    config[\"models_save_dir\"] = models_save_dir\n",
    "    config[\"emg_imu_pkl_full_path\"] = f\"{CODE_DIR}//dataset//filtered_datasets//metadata_IMU_EMG_allgestures_allusers.pkl\" \n",
    "    config[\"pwmd_xlsx_filepath\"] = f\"{CODE_DIR}//dataset//Biosignal gesture questionnaire for participants with disabilities.xlsx\"\n",
    "    config[\"pwoutmd_xlsx_filepath\"] = f\"{CODE_DIR}//dataset//Biosignal gesture questionnaire for participants without disabilities.xlsx\"\n",
    "    config[\"dfs_save_path\"] = f\"{CODE_DIR}/dataset//\"\n",
    "    config[\"dfs_load_path\"] = f\"{CODE_DIR}/dataset/meta-learning-sup-que-ds//\"\n",
    "\n",
    "# ----- Model layout hyperparams (Optimized for speed) -----\n",
    "config[\"num_experts\"] = 2\n",
    "config[\"use_shared_expert\"] = True\n",
    "config[\"expert_architecture\"] = \"linear\" \n",
    "config[\"top_k\"] = 1\n",
    "config[\"gate_type\"] = \"context_only\"\n",
    "config[\"mixture_mode\"] = 'logits'\n",
    "config[\"return_aux\"] = True \n",
    "\n",
    "# NEW MULTIMODAL\n",
    "config[\"groupnorm_num_groups\"] = 4\n",
    "config[\"emg_base_cnn_filters\"] = 16\n",
    "config[\"imu_base_cnn_filters\"] = 16\n",
    "config['emg_stride'] = 1 \n",
    "config['imu_stride'] = 1 \n",
    "config[\"cnn_kernel_size\"] = 3\n",
    "config[\"imu_cnn_layers\"] = 1\n",
    "config[\"emg_cnn_layers\"] = 1\n",
    "config[\"use_film_x_demo\"] = False\n",
    "config[\"use_imu\"] = True \n",
    "config[\"use_demographics\"] = True \n",
    "config[\"context_emb_dim\"] = 4\n",
    "config[\"context_pool_type\"] = 'mean'\n",
    "config[\"demo_emb_dim\"] = 4\n",
    "\n",
    "config[\"multimodal\"] = True\n",
    "config['emg_in_ch'] = 16\n",
    "config['imu_in_ch'] = 72\n",
    "config['demo_in_dim'] = 12\n",
    "config['num_epochs'] = 2  # Reduced drastically for quick debugging\n",
    "\n",
    "config[\"use_lstm\"] = True\n",
    "if config[\"use_lstm\"]:\n",
    "    config[\"lstm_hidden\"] = 32\n",
    "    config[\"lstm_layers\"] = 1\n",
    "    config[\"use_GlobalAvgPooling\"] = True\n",
    "\n",
    "# MOE Hyperparams\n",
    "config[\"label_smooth\"] = 0.1\n",
    "\n",
    "# Pretraining optim\n",
    "config[\"weight_decay\"] = 1e-4\n",
    "config[\"optimizer\"] = \"adam\"\n",
    "config[\"lr_scheduler_factor\"] = 0.1\n",
    "config[\"lr_scheduler_patience\"] = 6\n",
    "config[\"earlystopping_patience\"] = 8\n",
    "config[\"earlystopping_min_delta\"] = 0.005\n",
    "\n",
    "# MAML SPECIFIC\n",
    "config[\"meta_learning\"] = True\n",
    "config[\"n_way\"] = 10\n",
    "config[\"k_shot\"] = 1\n",
    "config[\"q_query\"] = 2  # Lowered for faster debugging\n",
    "config[\"meta_batchsize\"] = 4\n",
    "config[\"episodes_per_epoch_train\"] = 10  # Very small for rapid testing\n",
    "config[\"num_workers\"] = 0  # 0 is often safer/easier to debug on Windows locally\n",
    "config[\"maml_inner_steps\"] = 1\n",
    "\n",
    "# MAML Optimization Order\n",
    "config[\"maml_opt_order\"] = \"first\" \n",
    "config[\"maml_first_order_to_second_order_epoch\"] = 1000000 \n",
    "\n",
    "# MAML Multi-Step Loss\n",
    "config[\"use_maml_msl\"] = False\n",
    "config[\"maml_msl_num_epochs\"] = 0\n",
    "\n",
    "# LSLR Logic\n",
    "config[\"maml_use_lslr\"] = False\n",
    "config[\"learning_rate\"] = 0.001\n",
    "config[\"maml_alpha_init\"] = 0.01\n",
    "config[\"maml_alpha_init_eval\"] = 0.01\n",
    "\n",
    "config[\"enable_inner_loop_optimizable_bn_params\"] = False\n",
    "config[\"maml_inner_steps_eval\"] = 3\n",
    "config[\"use_cosine_outer_lr\"] = False\n",
    "config[\"use_lslr_at_eval\"] = False\n",
    "\n",
    "# ----- Build model -----\n",
    "model = MultimodalCNNLSTMMOE(config)\n",
    "device = config[\"device\"]\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8ae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(user_split_json_filepath, \"r\") as f:\n",
    "    ALL_SPLITS = json.load(f)\n",
    "\n",
    "apply_fold_to_config(config, ALL_SPLITS, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf30e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Pretraining data & training ----\n",
    "# train support, train query, val support (ft), val query (novel test), test support, test query\n",
    "#train_loader, val_loader, ft_loader, novel_test_loader, test_support_dl, test_query_dl = load_multimodal_data_loaders(config, load_existing_dfs=True)\n",
    "episodic_train_loader, episodic_val_loader, episodic_test_loader = load_multimodal_data_loaders(config, load_existing_dfs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9cdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_episodes = len(episodic_val_loader)\n",
    "#print(f\"Total episodes in val_loader: {num_episodes}\")\n",
    "\n",
    "# TypeError: object of type 'FixedOneShotPerUserIterable' has no len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3810c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of batch: <class 'dict'>\n",
      "Batch is a dictionary! Keys are: dict_keys(['support', 'query', 'user_id', 'label_map', 'classes_global'])\n",
      "User ID found: 0\n"
     ]
    }
   ],
   "source": [
    "unique_users = set()\n",
    "\n",
    "for batch in episodic_val_loader:\n",
    "    print(f\"Type of batch: {type(batch)}\")\n",
    "    \n",
    "    if isinstance(batch, dict):\n",
    "        print(\"Batch is a dictionary! Keys are:\", batch.keys())\n",
    "        # If user_id is in the batch directly:\n",
    "        user = batch.get('user_id')\n",
    "        print(f\"User ID found: {user}\")\n",
    "        unique_users.add(user)\n",
    "        \n",
    "    elif isinstance(batch, list):\n",
    "        print(f\"Batch is a list of length {len(batch)}\")\n",
    "        # If it's a list, the first item is likely your episode dict\n",
    "        sample = batch[0]\n",
    "        if isinstance(sample, dict):\n",
    "             print(\"First item in list is a dict. Keys:\", sample.keys())\n",
    "             unique_users.add(sample['user_id'])\n",
    "    break # Just check the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e960e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 4\n",
      "Unique users: {0, 8, 16, 24}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "unique_users = set()\n",
    "\n",
    "for batch in episodic_val_loader:\n",
    "    count += 1\n",
    "    # If the loader collates everything into one dict:\n",
    "    if 'user_id' in batch:\n",
    "        u_id = batch['user_id']\n",
    "        # If it's a single ID (string or int)\n",
    "        if isinstance(u_id, (str, int)):\n",
    "            unique_users.add(u_id)\n",
    "        # If it's a list/tensor of IDs for the whole batch\n",
    "        else:\n",
    "            for u in u_id:\n",
    "                # Convert back from tensor if necessary\n",
    "                val = u.item() if torch.is_tensor(u) else u\n",
    "                unique_users.add(val)\n",
    "\n",
    "print(f\"Total iterations: {count}\")\n",
    "print(f\"Unique users: {unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149aa06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e60404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476b79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['support', 'query', 'user_id', 'label_map', 'classes_global'])\n",
      "dict_keys(['emg', 'imu', 'demo', 'labels', 'PIDs'])\n",
      "Meta-batch size (tasks per iteration): 1\n"
     ]
    }
   ],
   "source": [
    "# Grab the first meta-batch\n",
    "first_batch = next(iter(episodic_val_loader))\n",
    "\n",
    "# If your loader returns a list of episodes (common in MAML++):\n",
    "if isinstance(first_batch, list):\n",
    "    meta_batch_size = len(first_batch)\n",
    "    # Check the first episode in that batch\n",
    "    sample_episode = first_batch[0] \n",
    "else:\n",
    "    # If it returns a single dictionary representing a batch of tasks\n",
    "    sample_episode = first_batch\n",
    "    print(sample_episode.keys())\n",
    "    print(sample_episode['support'].keys())\n",
    "    # Usually the first dimension of a tensor in the dict is the batch size\n",
    "    #meta_batch_size = sample_episode['support'].shape[0]\n",
    "    # # It's 1\n",
    "    meta_batch_size = 1\n",
    "\n",
    "print(f\"Meta-batch size (tasks per iteration): {meta_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d6a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique users in val_loader: [0, 8, 16, 24]\n"
     ]
    }
   ],
   "source": [
    "# --- To see ALL users across the entire loader ---\n",
    "all_val_users = set()\n",
    "for batch in episodic_val_loader:\n",
    "    if isinstance(batch, list):\n",
    "        for episode in batch:\n",
    "            all_val_users.add(episode['user_id'])\n",
    "    else:\n",
    "        # Handle if it's a tensor or a single ID\n",
    "        u_id = batch['user_id']\n",
    "        if torch.is_tensor(u_id):\n",
    "            all_val_users.update(u_id.tolist())\n",
    "        else:\n",
    "            all_val_users.add(u_id)\n",
    "\n",
    "print(f\"All unique users in val_loader: {sorted(list(all_val_users))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5916c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fd2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAML++ Pretraining: Epoch 1 of 2\n",
      "--- Episode 0/10 | Target Meta-Batch Size: 4 ---\n",
      "Meta batchsize hit on ep 4! Parameters updating!\n",
      "Train completed in 1.66s\n",
      "Train loss/acc: 2.3024, 10.00%\n",
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 0\n",
      "User: 0 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.3034, -0.2770, -0.2505, -0.2291, -0.1930])\n",
      "step / ep: 2 / 2\n",
      "Meta eval user_id: 8\n",
      "User: 8 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0597, -0.0578, -0.0536,  0.0230,  0.1603])\n",
      "step / ep: 3 / 3\n",
      "Meta eval user_id: 16\n",
      "User: 16 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0949, -0.0600, -0.0609, -0.0748, -0.0701])\n",
      "step / ep: 4 / 4\n",
      "Meta eval user_id: 24\n",
      "User: 24 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0002,  0.0014, -0.0015, -0.0014, -0.0005])\n",
      "Val completed in 1.11s\n",
      "Val loss/acc: 2.3036, 10.00%\n",
      "Epoch completed in 2.77s\n",
      "\n",
      "MAML++ Pretraining: Epoch 2 of 2\n",
      "--- Episode 0/10 | Target Meta-Batch Size: 4 ---\n",
      "Meta batchsize hit on ep 4! Parameters updating!\n",
      "Train completed in 0.94s\n",
      "Train loss/acc: 2.3052, 10.00%\n",
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 0\n",
      "User: 0 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.3034, -0.2770, -0.2505, -0.2291, -0.1930])\n",
      "step / ep: 2 / 2\n",
      "Meta eval user_id: 8\n",
      "User: 8 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0597, -0.0578, -0.0536,  0.0230,  0.1603])\n",
      "step / ep: 3 / 3\n",
      "Meta eval user_id: 16\n",
      "User: 16 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0949, -0.0600, -0.0609, -0.0748, -0.0701])\n",
      "step / ep: 4 / 4\n",
      "Meta eval user_id: 24\n",
      "User: 24 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0002,  0.0014, -0.0015, -0.0014, -0.0005])\n",
      "Val completed in 1.05s\n",
      "Val loss/acc: 2.3033, 10.28%\n",
      "Epoch completed in 2.00s\n",
      "\n",
      "[MAML++] loaded best model (val acc=0.103)\n"
     ]
    }
   ],
   "source": [
    "# Do the meta pretraining\n",
    "pretrained_model, pretrain_res_dict = mamlpp_pretrain(model, config, episodic_train_loader, episodic_val_loader=episodic_val_loader)\n",
    "best_val_acc = pretrain_res_dict[\"best_val_acc\"]\n",
    "best_state   = pretrain_res_dict[\"best_state\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06509cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_loaders = make_user_loaders_from_dataloaders(episodic_val_loader, episodic_test_loader, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44c749e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29cfbf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: <torch.utils.data.dataloader.DataLoader at 0x1c11693b8e0>,\n",
       "  8: <torch.utils.data.dataloader.DataLoader at 0x1c17dbab190>,\n",
       "  16: <torch.utils.data.dataloader.DataLoader at 0x1c118047eb0>,\n",
       "  24: <torch.utils.data.dataloader.DataLoader at 0x1c117fdfaf0>},\n",
       " {1: <torch.utils.data.dataloader.DataLoader at 0x1c16e8320e0>,\n",
       "  9: <torch.utils.data.dataloader.DataLoader at 0x1c16e831450>,\n",
       "  17: <torch.utils.data.dataloader.DataLoader at 0x1c16e832740>,\n",
       "  25: <torch.utils.data.dataloader.DataLoader at 0x1c16e830250>})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b2fb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 0\n",
      "User: 0 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.3034, -0.2770, -0.2505, -0.2291, -0.1930])\n",
      "Final user0 loss: 10.00%\n",
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 8\n",
      "User: 8 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0597, -0.0578, -0.0536,  0.0230,  0.1603])\n",
      "Final user8 loss: 11.11%\n",
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 16\n",
      "User: 16 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0949, -0.0600, -0.0609, -0.0748, -0.0701])\n",
      "Final user16 loss: 10.00%\n",
      "step / ep: 1 / 1\n",
      "Meta eval user_id: 24\n",
      "User: 24 | Support Labels (first 5): tensor([0, 1, 2, 3, 4])\n",
      "First 5 values in first sample: tensor([-0.0002,  0.0014, -0.0015, -0.0014, -0.0005])\n",
      "Final user24 loss: 10.00%\n"
     ]
    }
   ],
   "source": [
    "user_accs = []\n",
    "val_dls = user_loaders[0]\n",
    "#test_dls = user_loaders[1]\n",
    "#for pid, (user_val_epi_dl, user_test_epi_dl) in user_loaders.items():\n",
    "for user_id, user_val_dl in val_dls.items():\n",
    "    if user_val_dl is None:\n",
    "        raise ValueError(\"user_val_dl is None, preventing maml_finetune_and_eval...\")\n",
    "        continue\n",
    "\n",
    "    # TODO: user_val_dl here is apparently an integer...\n",
    "    # TODO: Do I need to add some code so it uses alpha_eval and such, like what happens in my maml code?\n",
    "    val_metrics = meta_evaluate(model, user_val_dl, config, mamlpp_adapt_and_eval)\n",
    "    final_user_val_loss, final_user_val_acc = val_metrics[\"loss\"], val_metrics[\"acc\"]\n",
    "    user_accs.append(final_user_val_acc)\n",
    "    print(f\"Final user{user_id} loss: {final_user_val_acc*100:.2f}%\")\n",
    "\n",
    "mean_acc = float(np.mean(user_accs)) if len(user_accs) > 0 else float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03482d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
