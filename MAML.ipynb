{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200a0ae2",
   "metadata": {},
   "source": [
    "> __Purpose:__ This notebook will implement our MAML functions and test the episodic dataloader structure. Once everything is running, either this NB or a copy will do HPO for the MAML parameters. We will include the MOE layer for now (although placement is probably quite poor), and we will stick with the CNN-MLP architecture since the CNN-LSTM architecture was not performing as well (the latter architecture was probably not fit sufficiently in its HPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d28b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0218a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "CWD after sys path append: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "DNN_FT_funcs.py: The current working directory is: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "Global seed set to 17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys, copy, json, time, joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#code_dir.April_25.MOE. --> Said not to use this... code_dir isn't an actual package...\n",
    "# Make sure I don't have files named the same thing...\n",
    "from MOE_multimodal_model_classes import *\n",
    "from MOE_quick_cls_heads import *\n",
    "from MOE_training import *\n",
    "from MOE_configs import *\n",
    "from multimodal_data_processing import *  # Needed for load_multimodal_dataloaders()\n",
    "from mamlpp import *\n",
    "from maml_multimodal_dataloaders import *\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# Add the parent directory folder to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "print(f\"CWD after sys path append: {os.getcwd()}\")\n",
    "\n",
    "from configs.hyperparam_tuned_configs import *\n",
    "from utils.DNN_FT_funcs import *\n",
    "from utils.gesture_dataset_classes import *\n",
    "\n",
    "from utils.global_seed import set_seed\n",
    "set_seed()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# ===================== OPTUNA TUNING SCRIPT =====================\n",
    "\n",
    "# ====== (3) --- Objective: build model, pretrain, finetune, return metric ======\n",
    "def build_maml_model(base_config):\n",
    "    config = copy.deepcopy(base_config)\n",
    "\n",
    "    # From base_config, not sure if they are being used...\n",
    "    config[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    config[\"cluster_iter_str\"] = None\n",
    "    config[\"feature_engr\"] = \"None\"\n",
    "    config[\"time_steps\"] = 1\n",
    "    config[\"sequence_length\"] = 64\n",
    "    config[\"num_train_gesture_trials\"] = 9\n",
    "    config[\"num_ft_gesture_trials\"] = 1\n",
    "    config[\"num_pretrain_users\"] = 24\n",
    "    config[\"num_testft_users\"] = 4\n",
    "    config[\"padding\"] = 0 \n",
    "    config[\"use_batch_norm\"] = False\n",
    "    config[\"timestamp\"] = timestamp\n",
    "    config[\"fc_dropout\"] = 0.0\n",
    "    config[\"num_classes\"] = 10\n",
    "    config[\"use_earlystopping\"] = True\n",
    "    config[\"reset_ft_layers\"] = False \n",
    "    config[\"verbose\"] = False\n",
    "    config[\"num_total_users\"] = 32\n",
    "\n",
    "    # ----- Model layout hyperparams -----\n",
    "    config[\"user_emb_dim\"]  = 16\n",
    "    config[\"num_experts\"]   = 8\n",
    "    config[\"top_k\"]         = 3\n",
    "\n",
    "    # TODO: Figure out a better user+demo+etc gating mechanism...\n",
    "    # MOE + User + Gate choice\n",
    "    config[\"gate_type\"]     = \"feature_only\"  # What are all the options here? ...\n",
    "    config[\"gate_requires_u_user\"] =  True\n",
    "    config[\"use_u_init_warm_start\"] = True\n",
    "    # User table usage (important for novel users) --> I think this needs to stay True, if False there's no backup method to learn user embeddings rn...\n",
    "    config[\"use_user_table\"] = True  # TODO: I think this won't even run when False? Passing in None for users...\n",
    "    config[\"demo_conditioning\"] = 'concat'\n",
    "    config[\"u_user_and_demos\"] = \"mix\"\n",
    "    config[\"use_u_init_warm_start\"] = True\n",
    "    config[\"gate_dense_before_topk\"] = True \n",
    "    config[\"gate_requires_u_user\"] = False if config[\"gate_type\"] == \"feature_only\" else True\n",
    "    config[\"mix_demo_u_alpha\"] = 0.5\n",
    "    # TODO: novel user emb definitely isn't getting trained anywhere in my current code...\n",
    "    ## This was just a finetuning thing... idk how we should do it in meta learning...\n",
    "    ## Why cant we just train a NN to do this? Maintain the NN instead of the table? I mean we still have the NN right in nn.embedding()\n",
    "    #config[\"alt_or_seq_MOE_user_emb_ft\"]= \"alternating\"\n",
    "\n",
    "    # Head choice\n",
    "    config[\"head_type\"]     = \"cosine\"\n",
    "    config[\"init_tau\"] = 8.0  \n",
    "    # Dropout / regularizers\n",
    "    config[\"expert_dropout\"]     = 0.0  #0.25 \n",
    "    config[\"label_smooth\"]       = 0.05  \n",
    "    config[\"gate_balance_coef\"]  = 0.05  \n",
    "\n",
    "    # Pretraining optim\n",
    "    config[\"learning_rate\"]      = 1E-3  # This is used by the meta-optimizer... idk if it should be or how/where it is getting used, given LSLR and such...\n",
    "    config[\"weight_decay\"]       = 0.0  #2.7E-6\n",
    "    config[\"optimizer\"]          = \"adamw\" \n",
    "    config[\"lr_scheduler_factor\"]= 0.1  \n",
    "    config[\"lr_scheduler_patience\"]= 4  \n",
    "    config[\"earlystopping_patience\"]= 8 \n",
    "    config[\"earlystopping_min_delta\"]= 0.005 \n",
    "    config['num_epochs'] = 35\n",
    "\n",
    "    # NEW MULTIMODAL\n",
    "    # NOTE: GroupNorm uses 8 groups currently, could raise/lower that, but emb_dim must be divisible by num_groups or it will break!!\n",
    "    config[\"groupnorm_num_groups\"] = 8  # TODO: Is this groupnorm even used...\n",
    "    config[\"emb_dim\"]       = 64  # TODO: Is this strictly related to multimodal? This is the modality embedding dim??\n",
    "    # It is probably only worth trying strides of 221 and 211. My data is already downsampled to 64 so no reason to use higher stride idt\n",
    "    ## Hmm I wonder if the strides need to be the same actually so the feature maps have the same seq lens... not sure...\n",
    "    config['emg_stride2'] = 1\n",
    "    config['imu_stride2'] = 2\n",
    "    # Eh I'll just scale by 2 for now...\n",
    "    config['emg_CNN_capacity_scaling'] = 1\n",
    "    config['imu_CNN_capacity_scaling'] = 2\n",
    "    config[\"multimodal\"] = True\n",
    "    config['emg_in_ch'] = 16\n",
    "    config['imu_in_ch'] = 72\n",
    "    config['demo_in_dim'] = 12\n",
    "\n",
    "    # NEW(-ish) FIELDS!\n",
    "    config[\"pool_mode\"] = 'max'\n",
    "    config[\"pdrop\"] = 0.0  # TODO: This is dropout (not weight decay?) and idk where exactly this is applied... \n",
    "    config[\"mixture_mode\"] = 'logits'  # 'logits' | 'probs' | 'logprobs' --> I don't think this is implemented AFAIK\n",
    "    config[\"num_pretrain_users\"] = 24  \n",
    "    config[\"moddrop_p\"] = 0.0  # TODO: No idea what this is --? \"(probability to drop IMU at train time)\"\n",
    "    config[\"demo_emb_dim\"] = 16\n",
    "    config[\"expert_bigger\"] = False  # (if True, widen Expert hidden)\n",
    "    config[\"expert_bigger_mult\"] = 2\n",
    "    config['log_each_pid_results'] = False\n",
    "\n",
    "    # ADDING MAML SPECIFIC\n",
    "    config[\"meta_learning\"] = True\n",
    "    config[\"n_way\"] = 10\n",
    "    config[\"k_shot\"] = 1\n",
    "    config[\"q_query\"] = 9  # TODO: Does this need to be 9? If it set it lower does that just make it faster? Does that impact the model? Slightly noiser eval??\n",
    "    # TODO: Do the below eps/batch and eps/epoch need to be multiple of each other?\n",
    "    config[\"episodes_per_batch_train\"] = 12  # Meta learning batch size\n",
    "    config[\"episodes_per_epoch_train\"] = 1000  # TODO: I have no idea what this should be... this is the max on the number of tasks per EPOCH. So this limits training, if the iterable is way too big. Idk if this is necessary for us or not\n",
    "    config[\"num_workers\"] = 0  # TODO: Idk what this does\n",
    "    # Core MAML++\n",
    "    config[\"maml_inner_steps\"] = 3\n",
    "    # TODO: Are the first and second order plus MSL not... like almost the same thing? I guess with no MSL there is literally no inner loop??\n",
    "    config[\"maml_second_order\"] = True                         # enables second-order when DOA switches on\n",
    "    config[\"maml_first_order_to_second_order_epoch\"] = 50      # DOA threshold (epochs <= this are first-order)\n",
    "    config[\"maml_use_msl\"] = True                              # MSL (multi-step loss) on\n",
    "    config[\"maml_msl_num_epochs\"] = 80                         # use MSL during first N epochs; after that, final-step loss only\n",
    "    config[\"maml_use_lslr\"] = False                             # learn per-parameter, per-step inner LRs\n",
    "    # TODO: Is this maml_alpha_init being used as a learning rate?\n",
    "    ## I remember that in PerFedAvg they said beta was around 0.5 or something (IIRC)\n",
    "    ## Yes this is being used as a learning rate\n",
    "    ## Gotta sort this out with the other one, idek if the other one is being used anymore...\n",
    "    config[\"maml_alpha_init\"] = 1E-3                            # fallback α (also eval α if LSLR not used at eval)\n",
    "    config[\"enable_inner_loop_optimizable_bn_params\"] = False  # by default, do NOT adapt BN in inner loop\n",
    "    # Eval\n",
    "    config[\"maml_inner_steps_eval\"] = 3\n",
    "    config[\"maml_alpha_init_eval\"] = 1E-3\n",
    "    config[\"use_cosine_outer_lr\"] = False                       # This is cosine-based lr annealing... is this in addition to my lr scheduler....\n",
    "    config[\"use_lslr_at_eval\"] = False                         # set True if you want to use learned per-parameter step sizes at eval\n",
    "\n",
    "    # NOTE: Only the non-commented ones here are actually used... (and finetune_strategy might not actually be used yet)\n",
    "    ### Finetuning regime ############################################\n",
    "    config[\"finetune_strategy\"]  = \"full\"  # TODO: I think this doesn't get used with MAML? Could repurpose it to be ANIL or something?\n",
    "    config['num_ft_epochs'] = 15  # TODO: Pretty sure this isn't doing anything with MAML (currently at least)\n",
    "    #config[\"use_dropout_during_peft\"] = False  \n",
    "    #config[\"ft_learning_rate\"]   = 0.00125 # TODO: Not sure if this is used? Regardless would like this to be lower than the pre LR...\n",
    "    #config[\"ft_weight_decay\"]    = 1E-5\n",
    "    #config[\"ft_lr_scheduler_factor\"]= 0.1 \n",
    "    #config[\"ft_lr_scheduler_patience\"]= 4 \n",
    "    #config[\"ft_earlystopping_patience\"]= 10 \n",
    "    #config[\"ft_earlystopping_min_delta\"]= 0.008 \n",
    "    ##############################################################\n",
    "    # Batch sizes (keep pretrain stable; you can expose if needed)\n",
    "    #config[\"batch_size\"] = 64  # TODO: Is this used with MAML??\n",
    "    #config[\"ft_batch_size\"] = 1  # TODO: Is this used with MAML??\n",
    "    ##############################################################\n",
    "    # LSTM RELATED (not used here but need to pass in \"none\")\n",
    "    # ---- backbone toggle ----\n",
    "    config[\"temporal_backbone\"] = \"none\"     # \"none\" (current TCN-only) | \"lstm\"\n",
    "    # ---- LSTM settings (used when temporal_backbone == \"lstm\") ----\n",
    "    #config[\"lstm_hidden\"] = 128\n",
    "    #config[\"lstm_layers\"] = 2\n",
    "    #config[\"lstm_bidirectional\"] = False\n",
    "    #config[\"temporal_pool_mode\"] = \"last\"    # \"last\" | \"mean\"   (pool *after* LSTM)\n",
    "    # ---- MoE placement (you can leave this as-is; we keep MoE at the head) ----\n",
    "    #config[\"moe_placement\"] = \"head\"        # (\"head\" recommended; others optional/unused here)\n",
    "\n",
    "    config[\"use_supportquery_for_ft\"] = True\n",
    "    config[\"emg_imu_pkl_full_path\"] = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\\\Meta_Gesture_Project\\\\filtered_datasets\\\\metadata_IMU_EMG_allgestures_allusers.pkl'\n",
    "    config[\"pwmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\Biosignal gesture questionnaire for participants with disabilities.xlsx\"\n",
    "    config[\"pwoutmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\Biosignal gesture questionnaire for participants without disabilities.xlsx\"\n",
    "    config[\"dfs_save_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\MOE\\\\full_datasplit_dfs\\\\\"\n",
    "    config[\"dfs_load_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\MOE\\\\full_datasplit_dfs\\\\Initial_Multimodal\\\\\"\n",
    "    config[\"saved_df_timestamp\"] = '20250917_1217'\n",
    "    # These need to get set by the json split right... are those set somewhere... --> THESE ARE HARDCODED IN load_multimodal_data_loaders right now!!\n",
    "    #train_PIDs=['P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P004', 'P005', 'P006', 'P008', 'P010', 'P011'], \n",
    "    #val_PIDs=['P102', 'P110', 'P121', 'P131'], \n",
    "    #test_PIDs=['P103', 'P111', 'P122', 'P132'], \n",
    "\n",
    "    # ----- Build model -----\n",
    "    model = MultiModalMoEClassifier(config)\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    # Tweak Expert’s dropout inline (uses Expert.drop)\n",
    "    for exp in model.experts:\n",
    "        if isinstance(exp.drop, nn.Dropout):\n",
    "            exp.drop.p = config[\"expert_dropout\"]\n",
    "\n",
    "    # Swap head if cosine\n",
    "    if config[\"head_type\"] == \"cosine\":\n",
    "        # This is the non-mutlimodal version:\n",
    "        #swap_expert_head_to_cosine(model, emb_dim=config[\"emb_dim\"], num_classes=config[\"num_classes\"], init_tau=config[\"init_tau\"])\n",
    "        # This is the mutlimodal version:\n",
    "        model.swap_expert_head_to_cosine(init_tau=config[\"init_tau\"])\n",
    "\n",
    "    model.to(device)\n",
    "    return model, config\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Load in / create the dataloaders\n",
    "# --------- base config ----------\n",
    "base_config = {}  #emg_moments_only_MOE_config\n",
    "# ---- Build model + config for this trial ----\n",
    "model, config = build_maml_model(base_config)\n",
    "if config[\"device\"]==\"cpu\":\n",
    "    print(\"HPO is happening on the CPU! Probably ought to switch to GPU!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf30e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Pretraining data & training ----\n",
    "# train support, train query, val support (ft), val query (novel test), test support, test query\n",
    "#train_loader, val_loader, ft_loader, novel_test_loader, test_support_dl, test_query_dl = load_multimodal_data_loaders(config, load_existing_dfs=True)\n",
    "episodic_train_loader, episodic_val_loader, episodic_test_loader = load_multimodal_data_loaders(config, load_existing_dfs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be46bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the meta pretraining\n",
    "pretrained_model, pretrain_res_dict = MAMLpp_pretrain(model, config, episodic_train_loader, episodic_val_loader=episodic_val_loader)\n",
    "best_val_acc = pretrain_res_dict[\"best_val_acc\"]\n",
    "best_state   = pretrain_res_dict[\"best_state\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef64d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06509cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_loaders = make_user_loaders_from_dataloaders(episodic_val_loader, episodic_test_loader, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b330d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, (user_val_epi_dl, user_test_epi_dl) in user_loaders.items():\n",
    "    if user_val_epi_dl is None:\n",
    "        ft = 0\n",
    "    else:\n",
    "        ft = 1\n",
    "    if user_test_epi_dl is None:\n",
    "        tst = 0\n",
    "    else:\n",
    "        tst = 1\n",
    "\n",
    "    print(f\"{ft}, {tst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22caec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_accs = []\n",
    "\n",
    "for pid, (user_val_epi_dl, user_test_epi_dl) in user_loaders.items():\n",
    "    # For FixedOneShotPerUserIterable, each dl will usually yield exactly 1 episode\n",
    "\n",
    "    if user_test_epi_dl is None:\n",
    "            continue\n",
    "    \n",
    "    for episode in user_test_epi_dl:  \n",
    "        support_batch = episode[\"support\"]\n",
    "        query_batch   = episode[\"query\"]\n",
    "\n",
    "        result = mamlpp_finetune_and_eval(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            support_batch=support_batch,\n",
    "            query_batch=query_batch,\n",
    "            #use_lslr_at_eval=False,\n",
    "        )\n",
    "\n",
    "        user_accs.append(result[\"acc\"])\n",
    "        # optionally store result[\"adapted_params\"] per pid\n",
    "\n",
    "mean_acc = float(np.mean(user_accs))\n",
    "print(user_accs)\n",
    "print(f\"Mean acc: {mean_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28920c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3aaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a76ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6667822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "CWD after sys path append: c:\\Users\\kdmen\\Repos\\fl-gestures\\April_25\\MOE\n",
      "Global seed set to 17\n"
     ]
    }
   ],
   "source": [
    "import sys, copy, json, time, joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from MOE_multimodal_model_classes import *\n",
    "from MOE_quick_cls_heads import *\n",
    "from MOE_training import *\n",
    "from MOE_configs import *\n",
    "from multimodal_data_processing import *  # Needed for load_multimodal_dataloaders()\n",
    "from mamlpp import *\n",
    "from maml_multimodal_dataloaders import *\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# Add the parent directory folder to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "print(f\"CWD after sys path append: {os.getcwd()}\")\n",
    "\n",
    "from configs.hyperparam_tuned_configs import *\n",
    "from utils.DNN_FT_funcs import *\n",
    "from utils.gesture_dataset_classes import *\n",
    "\n",
    "from utils.global_seed import set_seed\n",
    "set_seed()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fa34a",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimization Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb84e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== OPTUNA TUNING SCRIPT =====================\n",
    "\n",
    "# ====== (3) --- Objective: build model, pretrain, finetune, return metric ======\n",
    "def build_model_from_trial(trial, base_config=None):\n",
    "    if base_config is None:\n",
    "        config = dict()\n",
    "    else:   \n",
    "        config = copy.deepcopy(base_config) \n",
    "    config[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    #config[\"cluster_iter_str\"] = None\n",
    "    config[\"feature_engr\"] = \"None\"\n",
    "    config[\"time_steps\"] = 1\n",
    "    config[\"sequence_length\"] = 64\n",
    "    config[\"num_train_gesture_trials\"] = 9\n",
    "    config[\"num_ft_gesture_trials\"] = 1\n",
    "    config[\"num_pretrain_users\"] = 24\n",
    "    config[\"num_testft_users\"] = 4\n",
    "    config[\"padding\"] = 0 \n",
    "    config[\"use_batch_norm\"] = False\n",
    "    config[\"timestamp\"] = timestamp\n",
    "    config[\"fc_dropout\"] = 0.0\n",
    "    config[\"num_classes\"] = 10\n",
    "    config[\"use_earlystopping\"] = True\n",
    "    config[\"reset_ft_layers\"] = False \n",
    "    config[\"verbose\"] = False\n",
    "    config[\"num_total_users\"] = 32\n",
    "\n",
    "    # ----- Model layout hyperparams -----\n",
    "    config[\"user_emb_dim\"]  = trial.suggest_int(\"user_emb_dim\", 12, 48)\n",
    "    config[\"num_experts\"]   = trial.suggest_int(\"num_experts\", 2, 10)\n",
    "    config[\"top_k\"]         = trial.suggest_categorical(\"top_k\", [None, 1, 2, 3])\n",
    "\n",
    "    # Gate choice\n",
    "    config[\"gate_type\"]     = trial.suggest_categorical(\"gate_type\", [\"user_aware\", \"feature_only\", \"user_only\", \"film\"])  #, \"bilinear\"\n",
    "    config[\"gate_requires_u_user\"] = False if config[\"gate_type\"] == \"feature_only\" else True\n",
    "    config[\"use_u_init_warm_start\"] = True #trial.suggest_categorical(\"use_u_init_warm_start\", [True, False])\n",
    "    # ^ False is broken right now because WithUserOverride doesn't accept None as the init vector\n",
    "    #if config[\"gate_type\"]==\"bilinear\":\n",
    "    #    # Using min here will def break this in Optuna right? ...\n",
    "    #    config[\"rank\"] = trial.suggest_int(\"bilinear_rank\", 4, min(config[\"emb_dim\"], 16))\n",
    "\n",
    "    # Head choice\n",
    "    config[\"head_type\"]     = trial.suggest_categorical(\"head_type\", [\"linear\", \"cosine\"])\n",
    "    if config[\"head_type\"] == \"cosine\":\n",
    "        config[\"init_tau\"] = 5.0  #trial.suggest_float(\"init_tau\", 5.0, 30.0)\n",
    "\n",
    "    # Dropout / regularizers\n",
    "    config[\"expert_dropout\"]     = 0.25  #trial.suggest_float(\"expert_dropout\", 0.0, 0.40)\n",
    "    config[\"label_smooth\"]       = 0.1  #trial.suggest_float(\"label_smooth\", 0.0, 0.15)\n",
    "    config[\"gate_balance_coef\"]  = 0.1  #trial.suggest_float(\"gate_balance_coef\", 0.0, 0.15)\n",
    "\n",
    "    # Pretraining optim\n",
    "    config[\"learning_rate\"]      = trial.suggest_float(\"pre_lr\", 5e-6, 5e-4, log=True)\n",
    "    config[\"weight_decay\"]       = trial.suggest_float(\"pre_wd\", 1e-6, 3e-3, log=True)\n",
    "    config[\"optimizer\"]          = \"adamw\"  #trial.suggest_categorical(\"pre_opt\", [\"adamw\", \"adam\", \"sgd\"])\n",
    "    config[\"lr_scheduler_factor\"]= 0.1  #trial.suggest_categorical(\"pre_sched_factor\", [0.1, 0.2])\n",
    "    config[\"lr_scheduler_patience\"]= 6  #trial.suggest_int(\"pre_sched_pat\", 4, 10)\n",
    "    config[\"earlystopping_patience\"]= 8 #trial.suggest_int(\"pre_es_pat\", 6, 14)\n",
    "    config[\"earlystopping_min_delta\"]= 0.005 #trial.suggest_float(\"pre_es_delta\", 0.001, 0.01)\n",
    "\n",
    "    # Finetuning regime\n",
    "    config[\"finetune_strategy\"]  = 'adaptation' #trial.suggest_categorical(\"finetune_strategy\", [\"experts_only\", \"experts_plus_gate\", \"full\"])  #\"linear_probing\", \n",
    "    config[\"use_dropout_during_peft\"] = False  #trial.suggest_categorical(\"use_dropout_during_peft\", [False, True])\n",
    "    config[\"ft_learning_rate\"]   = trial.suggest_float(\"ft_lr\", 1e-4, 5e-2, log=True)\n",
    "    config[\"ft_weight_decay\"]    = trial.suggest_float(\"ft_wd\", 1e-6, 5e-3, log=True)\n",
    "    config[\"ft_lr_scheduler_factor\"]= 0.1  #trial.suggest_categorical(\"ft_sched_factor\", [0.1, 0.25, 0.5])\n",
    "    config[\"ft_lr_scheduler_patience\"]= 4  #trial.suggest_int(\"ft_sched_pat\", 4, 10)\n",
    "    config[\"ft_earlystopping_patience\"]= 10  #trial.suggest_int(\"ft_es_pat\", 6, 14)\n",
    "    config[\"ft_earlystopping_min_delta\"]= 0.008  #trial.suggest_float(\"ft_es_delta\", 0.0005, 0.01)\n",
    "\n",
    "    # TODO: Surely this isn't used with MAML? We don't do PEFT... so wtf is the user table doing then.......\n",
    "    #config[\"alt_or_seq_MOE_user_emb_ft\"]= trial.suggest_categorical(\"alt_or_seq_MOE_user_emb_ft\", [\"sequential\", \"alternating\"])\n",
    "\n",
    "    # Batch sizes (keep pretrain stable; you can expose if needed)\n",
    "    ## TODO: Confirm this has no effect... for MAML it should be fully controlled by num episodes or something??\n",
    "    config[\"batch_size\"] = 128  #trial.suggest_categorical(\"pre_bs\", [32, 64, 128, 256, 512, 1024])\n",
    "    config[\"ft_batch_size\"] = 10  #trial.suggest_categorical(\"ft_bs\", [1, 2, 8, 10])\n",
    "\n",
    "    # User table usage (important for novel users) --> I think this needs to stay True, if False there's no backup method to learn user embeddings rn...\n",
    "    config[\"use_user_table\"]     = True  #trial.suggest_categorical(\"use_user_table\", [True, False])\n",
    "\n",
    "    # NEW MULTIMODAL\n",
    "    # NOTE: GroupNorm uses 8 groups currently, could raise/lower that, but emb_dim must be divisible by num_groups or it will break!!\n",
    "    config[\"groupnorm_num_groups\"] = trial.suggest_categorical(\"groupnorm_num_groups\", [4, 6, 8, 12])\n",
    "    #config[\"emg_emb_dim\"]       = trial.suggest_categorical(\"emg_emb_dim\", [72, 96, 120, 192, 216, 288, 360])\n",
    "    #config[\"imu_emb_dim\"]       = trial.suggest_categorical(\"imu_emb_dim\", [72, 96, 120, 192, 216, 288, 360])\n",
    "    # Actually I'm gonna keep these the same. Simplifies the network. If IMU >> EMG in the emb dim, it might just overfit to IMU\n",
    "    config[\"emb_dim\"]       = trial.suggest_categorical(\"emb_dim\", [72, 96, 120, 192, 216, 288, 360])\n",
    "\n",
    "    # It is probably only worth trying strides of 221 and 211. My data is already downsampled to 64 so no reason to use higher stride idt\n",
    "    ## Hmm I wonder if the strides need to be the same actually so the feature maps have the same seq lens... not sure...\n",
    "    config['emg_stride2'] = trial.suggest_int(\"emg_stride2\", 1, 2)\n",
    "    config['imu_stride2'] = trial.suggest_int(\"imu_stride2\", 1, 2)\n",
    "\n",
    "    # Eh I'll just scale by 2...\n",
    "    #config['emg_CNN_capacity'] = trial.suggest_categorical(\"emg_CNN_capacity\", [72, 96, 120, 192, 216, 288, 360])\n",
    "    #config['imu_CNN_capacity'] = trial.suggest_categorical(\"imu_CNN_capacity\", [72, 96, 120, 192, 216, 288, 360])\n",
    "    config['emg_CNN_capacity_scaling'] = trial.suggest_categorical(\"emg_CNN_capacity_scaling\", [1, 2, 3])\n",
    "    config['imu_CNN_capacity_scaling'] = trial.suggest_categorical(\"imu_CNN_capacity_scaling\", [1, 2, 3, 4, 5])\n",
    "\n",
    "    config[\"multimodal\"] = True\n",
    "    config['emg_in_ch'] = 16\n",
    "    config['imu_in_ch'] = 72\n",
    "    config['demo_in_dim'] = 12\n",
    "    config['num_epochs'] = 35\n",
    "    config['num_ft_epochs'] = 15\n",
    "\n",
    "    # NEW FIELDS!\n",
    "    config[\"mix_demo_u_alpha\"] = 0.5\n",
    "\n",
    "    # ADDING MAML SPECIFIC\n",
    "    # TODO: How do all of these interact???\n",
    "    config[\"meta_learning\"] = True\n",
    "    config[\"n_way\"] = 10\n",
    "    config[\"k_shot\"] = 1\n",
    "    config[\"q_query\"] = 9  # TODO: Does this need to be 9? If it set it lower does that just make it faster? Does that impact the model? Slightly noiser eval??\n",
    "    # TODO: Do the below eps/batch and eps/epoch need to be multiple of each other?\n",
    "    config[\"episodes_per_batch_train\"] = trial.suggest_categorical(\"episodes_per_batch_train\", [10, 100, 500])  # Meta learning batch size\n",
    "    config[\"episodes_per_epoch_train\"] = trial.suggest_categorical(\"episodes_per_epoch_train\", [100, 1000, 5000])  # TODO: I have no idea what this should be... this is the max on the number of tasks per EPOCH. So this limits training, if the iterable is way too big. Idk if this is necessary for us or not\n",
    "    config[\"num_workers\"] = 0  # TODO: Idk what this does\n",
    "    # Core MAML++\n",
    "    config[\"maml_inner_steps\"] = trial.suggest_int(\"maml_inner_steps\", 1, 5)\n",
    "    # TODO: Are the first and second order plus MSL not... like almost the same thing? I guess with no MSL there is literally no inner loop??\n",
    "    config[\"maml_second_order\"] = trial.suggest_categorical(\"maml_second_order\", [True, False])                         # enables second-order when DOA switches on\n",
    "    config[\"maml_first_order_to_second_order_epoch\"] = trial.suggest_categorical(\"maml_first_order_to_second_order_epoch\", [10, 30, 60, 100])      # DOA threshold (epochs <= this are first-order)\n",
    "    config[\"maml_use_msl\"] = trial.suggest_categorical(\"maml_use_msl\", [True, False])                              # MSL (multi-step loss) on\n",
    "    config[\"maml_msl_num_epochs\"] = trial.suggest_categorical(\"maml_msl_num_epochs\", [10, 30, 60, 100])                         # use MSL during first N epochs; after that, final-step loss only\n",
    "    config[\"maml_use_lslr\"] = trial.suggest_categorical(\"maml_use_lslr\", [True, False])                             # learn per-parameter, per-step inner LRs\n",
    "    # TODO: Is this maml_alpha_init being used as a learning rate?\n",
    "    ## I remember that in PerFedAvg they said beta was around 0.5 or something (IIRC)\n",
    "    ## Yes this is being used as a learning rate\n",
    "    ## Gotta sort this out with the other one, idek if the other one is being used anymore...\n",
    "    config[\"maml_alpha_init\"] = 1E-3                            # fallback α (also eval α if LSLR not used at eval)\n",
    "    config[\"enable_inner_loop_optimizable_bn_params\"] = False  # by default, do NOT adapt BN in inner loop\n",
    "    # Eval\n",
    "    config[\"maml_inner_steps_eval\"] = trial.suggest_int(\"maml_inner_steps_eval\", 1, 5)\n",
    "    config[\"maml_alpha_init_eval\"] = 1E-3\n",
    "    config[\"use_cosine_outer_lr\"] = False                       # This is cosine-based lr annealing... is this in addition to my lr scheduler....\n",
    "    config[\"use_lslr_at_eval\"] = False                         # set True if you want to use learned per-parameter step sizes at eval\n",
    "\n",
    "    # OPTUNA\n",
    "    config[\"pool_mode\"] = trial.suggest_categorical(\"pool_mode\", ['avg', 'max', 'avgmax']) \n",
    "    config[\"pdrop\"] = 0.1  # TODO: No idea what this is...\n",
    "    config[\"mixture_mode\"] = 'logits'  # 'logits' | 'probs' | 'logprobs' --> I don't think this is implemented AFAIK\n",
    "    config[\"use_user_table\"] = True  # TODO: I think this won't even run when False? Passing in None for users...\n",
    "    config[\"num_pretrain_users\"] = 24  \n",
    "    config[\"moddrop_p\"] = 0.15  # TODO: No idea what this is --? \"(probability to drop IMU at train time)\"\n",
    "    config[\"demo_emb_dim\"] = 16\n",
    "    config[\"demo_conditioning\"] = trial.suggest_categorical(\"demo_conditioning\", ['concat', 'film'])\n",
    "    config[\"expert_bigger\"] = False  # (if True, widen Expert hidden)\n",
    "    config[\"expert_bigger_mult\"] = 2\n",
    "    config[\"u_user_and_demos\"] = trial.suggest_categorical(\"u_user_and_demos\", [\"demo\", \"mix\", \"u_user\"])  # (ie table and u_user_overwriting, ie the default version) \n",
    "\n",
    "    # Thse should be in there but don't seem to be printed? Idk...\n",
    "    config[\"use_u_init_warm_start\"] = True\n",
    "    config[\"gate_dense_before_topk\"] = True \n",
    "    config[\"gate_requires_u_user\"] = False if config[\"gate_type\"] == \"feature_only\" else True\n",
    "    config['log_each_pid_results'] = False\n",
    "    #config['saved_df_timestamp'] = '20250917_1217'  # Apparently this is already input/default somewhere\n",
    "\n",
    "    # NEW FOR LSTM VERSION! I AM NOT USING THE LSTM VERSION IN THIS NB!\n",
    "    # ---- backbone toggle ----\n",
    "    config[\"temporal_backbone\"] = \"none\"     # \"none\" (current TCN-only) | \"lstm\"\n",
    "    # ---- LSTM settings (used when temporal_backbone == \"lstm\") ----\n",
    "    #config[\"lstm_hidden\"] = 128\n",
    "    #config[\"lstm_layers\"] = 2\n",
    "    #config[\"lstm_bidirectional\"] = False\n",
    "    #config[\"temporal_pool_mode\"] = \"last\"    # \"last\" | \"mean\"   (pool *after* LSTM)\n",
    "    # ---- MoE placement (you can leave this as-is; we keep MoE at the head) ----\n",
    "    #config[\"moe_placement\"] = \"head\"        # (\"head\" recommended; others optional/unused here)\n",
    "\n",
    "    config[\"use_supportquery_for_ft\"] = True\n",
    "    config[\"emg_imu_pkl_full_path\"] = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Yamagami Lab\\\\Data\\\\Meta_Gesture_Project\\\\filtered_datasets\\\\metadata_IMU_EMG_allgestures_allusers.pkl'\n",
    "    config[\"pwmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\Biosignal gesture questionnaire for participants with disabilities.xlsx\"\n",
    "    config[\"pwoutmd_xlsx_filepath\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\Biosignal gesture questionnaire for participants without disabilities.xlsx\"\n",
    "    config[\"dfs_save_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\MOE\\\\full_datasplit_dfs\\\\\"\n",
    "    config[\"dfs_load_path\"] = \"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\MOE\\\\full_datasplit_dfs\\\\Initial_Multimodal\\\\\"\n",
    "    config[\"saved_df_timestamp\"] = '20250917_1217'\n",
    "\n",
    "    # ----- Build model -----\n",
    "    model = MultiModalMoEClassifier(config)\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    # Tweak Expert’s dropout inline (uses Expert.drop)\n",
    "    for exp in model.experts:\n",
    "        if isinstance(exp.drop, nn.Dropout):\n",
    "            exp.drop.p = config[\"expert_dropout\"]\n",
    "\n",
    "    # Swap head if cosine\n",
    "    if config[\"head_type\"] == \"cosine\":\n",
    "        swap_expert_head_to_cosine(model, emb_dim=config[\"emb_dim\"], num_classes=config[\"num_classes\"], init_tau=config[\"init_tau\"])\n",
    "\n",
    "    model.to(device)\n",
    "    return model, config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120a4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --------- base config ----------\n",
    "    #base_config = emg_moments_only_MOE_config\n",
    "\n",
    "    # ---- Build model + config for this trial ----\n",
    "    model, config = build_model_from_trial(trial)#, base_config)\n",
    "\n",
    "    print(f\"CONFIG[GATE_TYPE]: {config['gate_type']}\")\n",
    "\n",
    "    if config[\"device\"]==\"cpu\":\n",
    "        print(\"HPO is happening on the CPU! Probably ought to switch to GPU!\")\n",
    "\n",
    "    # ---- Pretraining data & training ----\n",
    "    episodic_train_loader, episodic_val_loader, episodic_test_loader = load_multimodal_data_loaders(config, load_existing_dfs=True)\n",
    "\n",
    "    # Do the meta pretraining\n",
    "    pretrained_model, pretrain_res_dict = MAMLpp_pretrain(model, config, episodic_train_loader, episodic_val_loader=episodic_val_loader)\n",
    "    best_val_acc = pretrain_res_dict[\"best_val_acc\"]\n",
    "    best_state   = pretrain_res_dict[\"best_state\"]\n",
    "\n",
    "    # TODO: Idk if I want to do this... I don't really care about the pretrained network...\n",
    "    # Report intermediate score for pruning\n",
    "    #trial.report(best_val_acc, step=0)\n",
    "    #if trial.should_prune():\n",
    "    #    raise optuna.TrialPruned()\n",
    "\n",
    "    user_loaders = make_user_loaders_from_dataloaders(episodic_val_loader, episodic_test_loader, config)\n",
    "\n",
    "    user_accs = []\n",
    "\n",
    "    for pid, (user_val_epi_dl, user_test_epi_dl) in user_loaders.items():\n",
    "        # For FixedOneShotPerUserIterable, each dl will usually yield exactly 1 episode\n",
    "\n",
    "        if user_test_epi_dl is None:\n",
    "                continue\n",
    "        \n",
    "        for episode in user_test_epi_dl:  \n",
    "            support_batch = episode[\"support\"]\n",
    "            query_batch   = episode[\"query\"]\n",
    "\n",
    "            result = mamlpp_finetune_and_eval(\n",
    "                model=model,\n",
    "                config=config,\n",
    "                support_batch=support_batch,\n",
    "                query_batch=query_batch,\n",
    "                #use_lslr_at_eval=False,\n",
    "            )\n",
    "\n",
    "            user_accs.append(result[\"acc\"])\n",
    "            # optionally store result[\"adapted_params\"] per pid\n",
    "\n",
    "    mean_acc = float(np.mean(user_accs))\n",
    "    print(user_accs)\n",
    "    print(f\"Mean acc: {mean_acc*100:.2f}%\")\n",
    "\n",
    "    # ---- Log ancillary info for analysis ----\n",
    "    trial.set_user_attr(\"pretrain_val_acc\", float(best_val_acc))\n",
    "    trial.set_user_attr(\"per_user_accs\", user_accs)\n",
    "\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89617fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== (4) --- Main: study, sampler, pruner & run ======\n",
    "def run_study(study_name=\"multimodal_moe_ft_HPO\", storage=None, n_trials=2):\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=12, multivariate=True, group=True)\n",
    "    pruner  = MedianPruner(n_startup_trials=8, n_warmup_steps=0)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        storage=storage,          # e.g., \"sqlite:///optuna_moe.db\"\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, gc_after_trial=True)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    bt = study.best_trial\n",
    "    print(\"  value (finetune acc):\", bt.value)\n",
    "    print(\"  params:\")\n",
    "    for k, v in bt.params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "    print(\"  pretrain_val_acc:\", bt.user_attrs.get(\"pretrain_val_acc\"))\n",
    "\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636dcfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-20 20:51:40,710] A new study created in memory with name: multimodal_moe_ft_HPO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG[GATE_TYPE]: user_aware\n",
      "MAML Pretraining: Epoch 1 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1638.28s\n",
      "Train loss/acc: 2.4111, 12.22%\n",
      "Val completed in 1.36s\n",
      "Val loss/acc: 2.3488, 12.78%\n",
      "Epoch completed in 1639.66s\n",
      "\n",
      "MAML Pretraining: Epoch 2 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1643.55s\n",
      "Train loss/acc: 2.3810, 12.86%\n",
      "Val completed in 1.14s\n",
      "Val loss/acc: 2.3504, 13.06%\n",
      "Epoch completed in 1644.69s\n",
      "\n",
      "MAML Pretraining: Epoch 3 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1635.13s\n",
      "Train loss/acc: 2.3813, 12.89%\n",
      "Val completed in 1.40s\n",
      "Val loss/acc: 2.3327, 14.44%\n",
      "Epoch completed in 1636.54s\n",
      "\n",
      "MAML Pretraining: Epoch 4 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1637.06s\n",
      "Train loss/acc: 2.3814, 13.19%\n",
      "Val completed in 1.64s\n",
      "Val loss/acc: 2.3383, 13.06%\n",
      "Epoch completed in 1638.71s\n",
      "\n",
      "MAML Pretraining: Epoch 5 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1630.24s\n",
      "Train loss/acc: 2.3872, 13.24%\n",
      "Val completed in 1.06s\n",
      "Val loss/acc: 2.3540, 16.11%\n",
      "Epoch completed in 1631.31s\n",
      "\n",
      "MAML Pretraining: Epoch 6 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1634.92s\n",
      "Train loss/acc: 2.3890, 13.40%\n",
      "Val completed in 1.04s\n",
      "Val loss/acc: 2.3226, 16.67%\n",
      "Epoch completed in 1635.98s\n",
      "\n",
      "MAML Pretraining: Epoch 7 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1649.17s\n",
      "Train loss/acc: 2.3894, 13.34%\n",
      "Val completed in 1.00s\n",
      "Val loss/acc: 2.3545, 15.83%\n",
      "Epoch completed in 1650.17s\n",
      "\n",
      "MAML Pretraining: Epoch 8 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1651.69s\n",
      "Train loss/acc: 2.3830, 13.66%\n",
      "Val completed in 1.11s\n",
      "Val loss/acc: 2.2978, 18.61%\n",
      "Epoch completed in 1652.81s\n",
      "\n",
      "MAML Pretraining: Epoch 9 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1658.69s\n",
      "Train loss/acc: 2.3928, 13.36%\n",
      "Val completed in 1.01s\n",
      "Val loss/acc: 2.3101, 18.61%\n",
      "Epoch completed in 1659.69s\n",
      "\n",
      "MAML Pretraining: Epoch 10 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 1636.08s\n",
      "Train loss/acc: 2.3860, 13.57%\n",
      "Val completed in 1.03s\n",
      "Val loss/acc: 2.2976, 19.72%\n",
      "Epoch completed in 1637.12s\n",
      "\n",
      "MAML Pretraining: Epoch 11 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 2642.45s\n",
      "Train loss/acc: 2.3054, 15.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val completed in 1.02s\n",
      "Val loss/acc: 2.3226, 20.28%\n",
      "Epoch completed in 2643.48s\n",
      "\n",
      "MAML Pretraining: Epoch 12 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 2644.58s\n",
      "Train loss/acc: 2.2428, 19.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val completed in 1.70s\n",
      "Val loss/acc: 2.3266, 19.44%\n",
      "Epoch completed in 2646.28s\n",
      "\n",
      "MAML Pretraining: Epoch 13 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n",
      "Train completed in 2623.21s\n",
      "Train loss/acc: 2.2097, 22.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "c:\\Users\\kdmen\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:950: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val completed in 1.08s\n",
      "Val loss/acc: 2.3208, 23.33%\n",
      "Epoch completed in 2624.30s\n",
      "\n",
      "MAML Pretraining: Epoch 14 of 35\n",
      "Checking params to see if they are causing blow up:\n",
      "meta_opt.param_groups: 1\n",
      "lslr_params: 0\n"
     ]
    }
   ],
   "source": [
    "# X trials took Y minutes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study_res = run_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0afb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study_res, f\"C:\\\\Users\\\\kdmen\\\\Repos\\\\fl-gestures\\\\April_25\\\\results\\\\MOE\\\\{timestamp}_MultiModalMOE_optuna_study.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679dd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04da28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\") \n",
    "trial = study_res.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study_res).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study_res).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55045111",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study_res, params=['emb_dim', 'user_emb_dim', 'emg_CNN_capacity_scaling', 'imu_CNN_capacity_scaling', 'pool_mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study_res, params=['num_experts', 'top_k', 'gate_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb777a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study_res, params=['alt_or_seq_MOE_user_emb_ft', 'finetune_strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1fdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study_res, params=['pre_lr', 'pre_wd', 'ft_lr', 'ft_wd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbce39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_slice(study_res, params=['pre_sched_factor', 'pre_sched_pat', 'pre_es_pat', 'pre_es_delta'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_slice(study_res, params=['ft_sched_factor', 'ft_sched_pat', 'ft_es_pat', 'ft_es_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c785bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from optuna.trial import TrialState\n",
    "from optuna.study import StudyDirection\n",
    "\n",
    "def top_k_trials(study: optuna.Study, k: int = 5, target_attr: str | None = None):\n",
    "    trials = [t for t in study.get_trials(deepcopy=False) if t.state == TrialState.COMPLETE]\n",
    "\n",
    "    def key_fn(t):\n",
    "        if target_attr is None:\n",
    "            return t.value\n",
    "        return t.user_attrs.get(target_attr, float(\"-inf\"))\n",
    "\n",
    "    reverse = (target_attr is not None) or (study.direction == StudyDirection.MAXIMIZE)\n",
    "    trials_sorted = sorted(trials, key=key_fn, reverse=reverse)\n",
    "    return trials_sorted[:k]\n",
    "\n",
    "def summarize_top_k(study: optuna.Study, k: int = 5, target_attr: str | None = None):\n",
    "    top = top_k_trials(study, k, target_attr=target_attr)\n",
    "\n",
    "    rows = []\n",
    "    for t in top:\n",
    "        row = {\n",
    "            \"trial\": t.number,\n",
    "            \"objective_value\": t.value,  # whatever objective() returned\n",
    "            \"duration_s\": (t.datetime_complete - t.datetime_start).total_seconds(),\n",
    "        }\n",
    "        if target_attr is not None:\n",
    "            row[f\"target:{target_attr}\"] = t.user_attrs.get(target_attr)\n",
    "        row.update({f\"param.{k}\": v for k, v in t.params.items()})\n",
    "        row.update({f\"attr.{k}\": v for k, v in t.user_attrs.items()})\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"trial\")\n",
    "\n",
    "    # Sort display\n",
    "    if target_attr is None:\n",
    "        ascending = (study.direction == StudyDirection.MINIMIZE)\n",
    "        df = df.sort_values(\"objective_value\", ascending=ascending)\n",
    "    else:\n",
    "        df = df.sort_values(f\"target:{target_attr}\", ascending=False)\n",
    "\n",
    "    # Union of all params across top-k\n",
    "    all_params = set()\n",
    "    for t in top:\n",
    "        all_params.update(t.params.keys())\n",
    "\n",
    "    # Medians/modes across top-k (skip params missing in some trials)\n",
    "    numeric, categorical = {}, {}\n",
    "    for p in all_params:\n",
    "        vals = [t.params[p] for t in top if p in t.params]\n",
    "        if not vals:\n",
    "            continue\n",
    "        if all(isinstance(v, (int, float)) for v in vals):\n",
    "            numeric[p] = pd.Series(vals).median()\n",
    "        else:\n",
    "            categorical[p] = pd.Series(vals).mode().iloc[0]\n",
    "\n",
    "    # Exact agreement across trials that *have* the param\n",
    "    exact_agreement = {}\n",
    "    for p in all_params:\n",
    "        vals = {t.params[p] for t in top if p in t.params}\n",
    "        exact_agreement[p] = len(vals) == 1\n",
    "\n",
    "    return df, numeric, categorical, exact_agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top5, medians, modes, exact = summarize_top_k(study_res, k=5)\n",
    "\n",
    "print(\"Top-5 trials:\")\n",
    "df_top5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNumeric parameter medians (top-5):\", medians)\n",
    "print(\"Categorical parameter modes (top-5):\", modes)\n",
    "print(\"Exact agreement across all top-5:\", {p: a for p, a in exact.items() if a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd59ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = study_res.best_trial\n",
    "print(\"direction:\", study_res.direction)\n",
    "print(\"value:\", t.value)                  # <- this is the metric you returned\n",
    "print(\"params:\", t.params)\n",
    "print(\"user_attrs:\", t.user_attrs)        # <- anything you stored manually\n",
    "print(\"intermediate values:\", t.intermediate_values)  # pruning steps, if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ddb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "\n",
    "groups = {\n",
    "    \"rep\": [\"alt_or_seq_MOE_user_emb_ft\", \"finetune_strategy\", \"num_experts\", \"top_k\"],\n",
    "}\n",
    "for name, params in groups.items():\n",
    "    print(f\"{name}: {params}\")\n",
    "    fig = plot_contour(study_res, params=params)\n",
    "fig.update_layout(width=1200, height=1400)  # 2–3× taller/wider\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f321d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
